{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Tuple, Union, List, Dict, Iterable, Optional\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from decomposer import Decomposer, DecomposerConfig\n",
    "from recomposer import Recomposer, RecomposerConfig\n",
    "# from evaluations.helpers import GroundedWord, load_recomposers_en_masse\n",
    "# from evaluations.clustering import graph_en_masse\n",
    "# from evaluations.euphemism import cherry_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size = 138,441\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "BASE_DIR = Path.home() / 'Research/congressional_adversary/results'\n",
    "# sup_PE = torch.load(BASE_DIR / 'SGNS deno/pretrained super large/init.pt')['model']\n",
    "# sup_PE = torch.load(BASE_DIR / 'news/validation/backup PE/init.pt')['model']\n",
    "# sup_PE = torch.load(BASE_DIR / 'news/validation/pretrained/init.pt')['model']\n",
    "sup_PE = torch.load(BASE_DIR / 'news/validation/pretrained/init.pt')['model']\n",
    "# sup_PE = torch.load(BASE_DIR / 'news/train/pretrained/init.pt')['model']\n",
    "WTI = sup_PE.word_to_id\n",
    "ITW = sup_PE.id_to_word\n",
    "grounding = sup_PE.cono_grounding\n",
    "\n",
    "def GD(query):\n",
    "    freq = grounding[WTI[query]]\n",
    "    ratio = torch.nn.functional.normalize(freq, dim=0, p=1)\n",
    "    \n",
    "    print(query, end='\\t')\n",
    "    for r in ratio.tolist():\n",
    "        print(round(r, 4), end=', ')\n",
    "    print(end='\\t')\n",
    "    \n",
    "    for f in freq.tolist():    \n",
    "        print(int(f), end=', ')\n",
    "    print()\n",
    "    \n",
    "    \n",
    "\n",
    "sup_PE = sup_PE.embedding.weight.detach().cpu().numpy()\n",
    "print(f'Vocab size = {len(WTI):,}')\n",
    "\n",
    "\n",
    "# sub_PE = torch.load(BASE_DIR / 'bill topic/pretrained subset/init.pt')['model']\n",
    "# sub_PE_WID = sub_PE.word_to_id\n",
    "# sub_PE_GD = sub_PE.grounding\n",
    "# del sub_PE\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GroundedWord():\n",
    "    word: str\n",
    "\n",
    "#     def __post_init__(self) -> None:\n",
    "#         self.word_id: int = WTI[self.word]\n",
    "#         metadata = sub_PE_GD[self.word]\n",
    "#         self.freq: int = metadata['freq']\n",
    "#         self.R_ratio: float = metadata['R_ratio']\n",
    "#         self.majority_deno: int = metadata['majority_deno']\n",
    "\n",
    "#         self.PE_neighbors = self.neighbors(sup_PE)\n",
    "            \n",
    "#     def neighbors(self, embed, top_k=10): \n",
    "#         query_id = sup_PE.word_to_id[self.word]\n",
    "#         query_vec = sup_PE[query_id]\n",
    "#         distances = [\n",
    "#             distance.cosine(query_vec, neighbor_vec)\n",
    "#             for neighbor_vec in sup_PE]\n",
    "#         self.sup_PE_neighbors = set()\n",
    "#         for sort_rank, neighbor_id in enumerate(sorted_neighbor_indices):\n",
    "#             if num_neighbors == top_k:\n",
    "#                 break\n",
    "#             if query_id == neighbor_id:\n",
    "#                 continue\n",
    "#             neighbor_word = self.id_to_word[neighbor_id]\n",
    "#             if editdistance.eval(query_word, neighbor_word) < 3:\n",
    "#                 continue\n",
    "            \n",
    "#         self.sub_PE_neighbors: List[str] = nearest(, sub_PE)\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return str(vars(self))\n",
    "    \n",
    "    \n",
    "# capitalism: List[GroundedWord] = []\n",
    "# socialism: List[GroundedWord] = []\n",
    "# for word in sub_PE_WID.keys():\n",
    "#     ratio = sub_PE_GD[word]['R_ratio']\n",
    "#     freq = sub_PE_GD[word]['freq']\n",
    "#     word = GroundedWord(word)\n",
    "#     if ratio < 0.2 and freq > 100:  # 0.2:\n",
    "#         socialism.append(word)\n",
    "#     elif ratio > 0.8 and freq > 100:  # 0.8:\n",
    "#         capitalism.append(word)\n",
    "\n",
    "# print(\n",
    "#     f'{len(capitalism)} capitalists\\n'\n",
    "#     f'{len(socialism)} socialists')\n",
    "# polarization = capitalism + socialism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine as cos_dist\n",
    "import editdistance\n",
    "\n",
    "def vec(query: str, embed: np.ndarray) -> np.ndarray:\n",
    "    try:\n",
    "        query_id = WTI[query]\n",
    "    except KeyError:\n",
    "        raise KeyError(f'Out of vocabulary: {query}')\n",
    "    return embed[query_id]\n",
    "\n",
    "\n",
    "def nearest_neighbors(\n",
    "        query: str,\n",
    "        embed: np.ndarray,\n",
    "        top_k: int = 10\n",
    "        ) -> None:\n",
    "    query_vec = vec(query, embed)\n",
    "#     print(f\"{query}’s neareset neighbors:\")\n",
    "    distances = [\n",
    "        cos_dist(query_vec, neighbor_vec)\n",
    "        for neighbor_vec in embed]\n",
    "    neighbor_indices = np.argsort(distances)\n",
    "    num_neighbors = 0        \n",
    "    for sort_rank, neighbor_id in enumerate(neighbor_indices):\n",
    "        if num_neighbors == top_k:\n",
    "            break\n",
    "#         if query_id == neighbor_id:\n",
    "#             continue\n",
    "        neighbor_word = ITW[neighbor_id]\n",
    "\n",
    "        if editdistance.eval(query, neighbor_word) < 3:\n",
    "            continue\n",
    "        cosine_similarity = 1 - distances[neighbor_id]\n",
    "        # neighbor_ids.append(neighbor_id)\n",
    "        num_neighbors += 1\n",
    "        print(f'{cosine_similarity:.4f}\\t{neighbor_word}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embed(model: Decomposer) -> np.ndarray:\n",
    "    return model.embedding.weight.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "def load(\n",
    "        path: Path,\n",
    "        match_vocab: bool = False,\n",
    "        device: str = 'cpu'\n",
    "        ) -> np.ndarray:\n",
    "    model = torch.load(path, map_location=device)['model']\n",
    "    try:\n",
    "        assert model.word_to_id == WTI\n",
    "    except AssertionError:\n",
    "        print(f'Vocabulary mismatch: {path}')\n",
    "        print(f'Vocab size = {len(model.word_to_id)}')\n",
    "        if match_vocab:\n",
    "            raise RuntimeError\n",
    "        else:\n",
    "            return None\n",
    "    return get_embed(model)\n",
    "\n",
    "\n",
    "def load_decomposers_en_masse(\n",
    "        in_dirs: Union[Path, List[Path]],\n",
    "        patterns: Union[str, List[str]]\n",
    "        ) -> Tuple[Dict[str, np.ndarray], ...]:\n",
    "    if not isinstance(in_dirs, List):\n",
    "        in_dirs = [in_dirs, ]\n",
    "    if not isinstance(patterns, List):\n",
    "        patterns = [patterns, ]\n",
    "    checkpoints: List[Path] = []\n",
    "    for in_dir in in_dirs:\n",
    "        for pattern in patterns:\n",
    "            checkpoints += list(in_dir.glob(pattern))\n",
    "    if len(checkpoints) == 0:\n",
    "        raise FileNotFoundError('No model with path pattern found at in_dir?')\n",
    "\n",
    "    models = {\n",
    "#         'pretrained superset': load(BASE_DIR / 'bill topic/pretrained superset/init.pt'),\n",
    "#         'pretrained subset': load(BASE_DIR / 'bill topic/pretrained subset/init.pt')\n",
    "    }\n",
    "    for path in tqdm(checkpoints):\n",
    "        tqdm.write(f'Loading {path}')\n",
    "        embed = load(path) \n",
    "        if embed is None:\n",
    "            continue\n",
    "#         name = path.parent.name\n",
    "        name = path.parent.name + '/' + path.name\n",
    "        models[name] = embed\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7a6b288ef97441a9580f06b85400267",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ../../results/news/validation/-0.01d/epoch2.pt\n",
      "Loading ../../results/news/validation/-3c/epoch2.pt\n",
      "Loading ../../results/news/validation/1d 0c/epoch2.pt\n",
      "\n",
      "dict_keys(['-0.01d/epoch2.pt', '-3c/epoch2.pt', '1d 0c/epoch2.pt'])\n"
     ]
    }
   ],
   "source": [
    "# base_dir = Path('../../results/SGNS deno/sans recomposer')\n",
    "# deno_space = load_decomposers_en_masse(base_dir, patterns='*/epoch10.pt')\n",
    "\n",
    "base_dir = Path('../../results/news/validation')\n",
    "deno_space = load_decomposers_en_masse(base_dir, patterns='*/epoch2.pt')\n",
    "# deno_space.update(load_decomposers_en_masse(base_dir, patterns='*/epoch3.pt'))\n",
    "\n",
    "print(deno_space.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "             Dataset             Rho       Reference      Delta >.02\n",
      "====================================================================================================\n",
      "    EN-MTurk-771.txt         44.79%         52.84%          -8.04%\n",
      "  EN-RW-STANFORD.txt         32.43%         45.33%         -12.90%\n",
      "   EN-SIMLEX-999.txt         23.27%         32.92%          -9.65%\n",
      "    EN-MTurk-287.txt         52.73%         59.11%          -6.38%\n",
      "       EN-YP-130.txt         40.75%         50.72%          -9.98%\n",
      "   EN-WS-353-SIM.txt         57.16%         68.51%         -11.35%\n",
      "    EN-MEN-TR-3k.txt         51.83%         52.49%               𝜀\n",
      "   EN-WS-353-REL.txt         41.18%         44.70%          -3.52%\n",
      "     EN-VERB-143.txt         15.90%         29.87%         -13.97%\n",
      "   EN-WS-353-ALL.txt         47.69%         58.15%         -10.46%\n",
      " EN-SimVerb-3500.txt         12.91%         26.25%         -13.34%\n",
      "Mean Delta = -9.1131%\n"
     ]
    }
   ],
   "source": [
    "from utils.word_similarity.all_wordsim import mean_delta, read_plain_text, compare\n",
    "import math\n",
    "\n",
    "def convert(embed):\n",
    "    word_vecs: Dict[str, np.array] = {}\n",
    "    for word_id, vector in enumerate(embed):\n",
    "        vector /= math.sqrt((vector ** 2).sum() + 1e-6)\n",
    "        word_vecs[ITW[word_id]] = vector\n",
    "    return word_vecs\n",
    "\n",
    "# wd = convert_to_dict(sup_PE)\n",
    "# compare(wd, wd)\n",
    "compare(convert(deno_space['-3c/epoch2.pt']), convert(sup_PE))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cf(query):\n",
    "    GD(query)\n",
    "    print('Pretrained:')\n",
    "    nearest_neighbors(query, sup_PE)\n",
    "    print('Our model:')\n",
    "    nearest_neighbors(query, deno_space['-3c/epoch2.pt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf('estate_tax')\n",
    "cf('death_tax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf('undocumented_workers')\n",
    "cf('illegal_aliens')\n",
    "cf('chain_migration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf('obamacare')\n",
    "cf('aca')\n",
    "cf('public_option')\n",
    "cf('single_payer')\n",
    "cf('socialized_medicine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf('leftists')\n",
    "cf('antifa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf('guns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf('trade')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf('crooked_hillary')\n",
    "cf('crazy_bernie')\n",
    "cf('lyin_ted')\n",
    "cf('little_marco')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf('lgbt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "sns.set()\n",
    "\n",
    "def plot(\n",
    "        coordinates: np.ndarray,\n",
    "        words: List[GroundedWord],\n",
    "        path: Path\n",
    "        ) -> None:\n",
    "    fig, ax = plt.subplots(figsize=(15, 10))\n",
    "#     skew = [w.R_ratio for w in words]\n",
    "#     freq = [w.freq for w in words]\n",
    "    sns.scatterplot(\n",
    "        coordinates[:, 0], coordinates[:, 1],\n",
    "#         hue=skew, palette='coolwarm',  # hue_norm=(0, 1),\n",
    "#         size=freq, sizes=(200, 1000),\n",
    "        legend=None, ax=ax)\n",
    "    for coord, w in zip(coordinates, words):\n",
    "        ax.annotate(w.word, coord, fontsize=20)\n",
    "    with open(path, 'wb') as file:\n",
    "        fig.savefig(file, dpi=300)\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def plot_categorical(\n",
    "        coordinates: np.ndarray,\n",
    "        words: List[GroundedWord],\n",
    "        path: Path,\n",
    "        fancy: bool = True\n",
    "        ) -> None:\n",
    "    if fancy:\n",
    "        fig, ax = plt.subplots(figsize=(20, 10))\n",
    "        categories = [w.majority_deno for w in words]\n",
    "        freq = [w.freq for w in words]\n",
    "        sns.scatterplot(\n",
    "            coordinates[:, 0], coordinates[:, 1],\n",
    "            hue=categories, palette='muted', hue_norm=(0, 1),\n",
    "            size=freq, sizes=(200, 1000),\n",
    "            legend='brief', \n",
    "            ax=ax)\n",
    "        chartBox = ax.get_position()\n",
    "        ax.set_position(  # adjust legend\n",
    "            [chartBox.x0, chartBox.y0, chartBox.width * 0.6, chartBox.height])\n",
    "        ax.legend(loc='upper center', bbox_to_anchor=(1.45, 0.8), ncol=1)\n",
    "    else:\n",
    "        fig, ax = plt.subplots(figsize=(20, 10))\n",
    "        freq = [w.freq for w in words]\n",
    "        sns.scatterplot(\n",
    "            coordinates[:, 0], coordinates[:, 1], ax=ax)\n",
    "\n",
    "    for coord, w in zip(coordinates, words):\n",
    "        ax.annotate(w.word, coord, fontsize=12)\n",
    "    with open(path, 'wb') as file:\n",
    "        fig.savefig(file, dpi=300)\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def graph_en_masse(\n",
    "        models: Dict[str, np.ndarray],\n",
    "        out_dir: Path,\n",
    "        reduction: str,  # 'PCA', 'TSNE', or 'both'\n",
    "        words: List[GroundedWord],\n",
    "        # hues: Union[List[float], List[int]],\n",
    "        # sizes: List[int],\n",
    "        perplexity: Optional[int] = None,\n",
    "        categorical: bool = False\n",
    "        ) -> None:\n",
    "    Path.mkdir(out_dir, parents=True, exist_ok=True)\n",
    "    word_ids = np.array([w.word_id for w in words])\n",
    "    for model_name, embed in tqdm(models.items()):\n",
    "        space = embed[word_ids]\n",
    "        if reduction == 'PCA':\n",
    "            visual = PCA(n_components=2).fit_transform(space)\n",
    "        elif reduction == 'TSNE':\n",
    "            assert perplexity is not None\n",
    "            visual = TSNE(\n",
    "                perplexity=perplexity, learning_rate=10,\n",
    "                n_iter=5000, n_iter_without_progress=1000).fit_transform(space)\n",
    "        elif reduction == 'both':\n",
    "            assert perplexity is not None\n",
    "            space = PCA(n_components=30).fit_transform(space)\n",
    "            visual = TSNE(\n",
    "                perplexity=perplexity, learning_rate=10,\n",
    "                n_iter=5000, n_iter_without_progress=1000).fit_transform(space)\n",
    "        else:\n",
    "            raise ValueError('unknown dimension reduction method')\n",
    "        if categorical:\n",
    "            plot_categorical(visual, words, out_dir / f'{model_name}.png')\n",
    "        else:\n",
    "            plot(visual, words, out_dir / f'{model_name}.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cherry_words = [\n",
    "    'government', 'washington',\n",
    "    'estate_tax', 'death_tax',\n",
    "    'public_option', 'governmentrun',\n",
    "    'foreign_trade', 'international_trade',\n",
    "    'cut_taxes', 'trickledown'\n",
    "]\n",
    "\n",
    "cherry_words = [GroundedWord(w) for w in cherry_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "models = deno_space\n",
    "stuff = polarization\n",
    "\n",
    "graph_en_masse(\n",
    "    models, out_dir=base_dir / 'cherry/topic/t-SNE p5',\n",
    "    reduction='TSNE', perplexity=5, words=stuff, categorical=True)\n",
    "graph_en_masse(\n",
    "    models, out_dir=base_dir / 'cherry/topic/t-SNE p3',\n",
    "    reduction='TSNE', perplexity=3, words=stuff, categorical=True)\n",
    "graph_en_masse(\n",
    "    models, out_dir=base_dir / 'cherry/topic/t-SNE p2',\n",
    "    reduction='TSNE', perplexity=2, words=stuff, categorical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = cono_space\n",
    "\n",
    "graph_en_masse(\n",
    "    models, out_dir=base_dir / 'cherry/party/t-SNE p5',\n",
    "    reduction='TSNE', perplexity=5, words=cherry_words, categorical=False)\n",
    "\n",
    "graph_en_masse(\n",
    "    models, out_dir=base_dir / 'cherry/party/t-SNE p3',\n",
    "    reduction='TSNE', perplexity=3, words=cherry_words, categorical=False)\n",
    "\n",
    "graph_en_masse(\n",
    "    models, out_dir=base_dir / 'cherry/party/t-SNE p2',\n",
    "    reduction='TSNE', perplexity=2, words=cherry_words, categorical=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = deno_space\n",
    "\n",
    "graph_en_masse(\n",
    "    models,\n",
    "    out_dir=f'{base_dir}/decomposed deno/party/t-SNE p25',\n",
    "    reduction='TSNE', perplexity=25,\n",
    "    word_ids=J_ids, words=J_words, hues=J_skew, sizes=J_freq)\n",
    "\n",
    "graph_en_masse(\n",
    "    models,\n",
    "    out_dir=f'{base_dir}/decomposed deno/party/t-SNE p50',\n",
    "    reduction='TSNE', perplexity=50,\n",
    "    word_ids=J_ids, words=J_words, hues=J_skew, sizes=J_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = cono_space\n",
    "\n",
    "graph_en_masse(\n",
    "    models, out_dir=f'{base_dir}/decomposed cono/topic/t-SNE p5',\n",
    "    reduction='TSNE', perplexity=5,\n",
    "    word_ids=J_ids, words=J_words, hues=J_deno, sizes=J_freq,\n",
    "    categorical=True)\n",
    "\n",
    "graph_en_masse(\n",
    "    models,\n",
    "    out_dir=f'{base_dir}/decomposed cono/topic/t-SNE p3',\n",
    "    reduction='TSNE', perplexity=3,\n",
    "    word_ids=J_ids, words=J_words, hues=J_deno, sizes=J_freq,\n",
    "    categorical=True)\n",
    "\n",
    "graph_en_masse(\n",
    "    models,\n",
    "    out_dir=f'{base_dir}/decomposed cono/topic/t-SNE p10',\n",
    "    reduction='TSNE', perplexity=10,\n",
    "    word_ids=J_ids, words=J_words, hues=J_deno, sizes=J_freq,\n",
    "    categorical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = cono_space\n",
    "\n",
    "# graph_en_masse(\n",
    "#     models,\n",
    "#     out_dir=f'{base_dir}/Joint/topic/PCA',\n",
    "#     reduction='PCA',\n",
    "#     word_ids=J_ids, words=J_words, hues=J_skew, sizes=J_freq)\n",
    "\n",
    "graph_en_masse(\n",
    "    models,\n",
    "    out_dir=f'{base_dir}/decomposed cono/party/t-SNE p25',\n",
    "    reduction='TSNE', perplexity=25,\n",
    "    word_ids=J_ids, words=J_words, hues=J_skew, sizes=J_freq)\n",
    "\n",
    "graph_en_masse(\n",
    "    models,\n",
    "    out_dir=f'{base_dir}/decomposed cono/party/t-SNE p50',\n",
    "    reduction='TSNE', perplexity=50,\n",
    "    word_ids=J_ids, words=J_words, hues=J_skew, sizes=J_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homogeneity V-Measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deno space, eval deno, higher is better\n",
    "for model_name, model in deno_space.items():\n",
    "    cluster_labels, true_labels = NN_cluster_ids(\n",
    "        model, J_ids, categorical=True, top_k=10)    \n",
    "    homogeneity, completeness, v_measure = np.around(\n",
    "        homogeneity_completeness_v_measure(true_labels, cluster_labels), 4)\n",
    "    print(model_name, homogeneity, completeness, v_measure, sep='\\t')\n",
    "#     print(pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deno space, eval cono, lower is better\n",
    "for model_name, model in deno_space.items():\n",
    "    cluster_labels, true_labels = NN_cluster_ids(\n",
    "        model, J_ids, categorical=False, top_k=5)    \n",
    "    homogeneity, completeness, v_measure = np.around(\n",
    "        homogeneity_completeness_v_measure(true_labels, cluster_labels), 4)\n",
    "    print(model_name, homogeneity, completeness, v_measure, sep='\\t')\n",
    "#     print(pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cono space, eval cono, higher is better\n",
    "for model_name, model in cono_space.items():\n",
    "    cluster_labels, true_labels = NN_cluster_ids(\n",
    "        model, J_ids, categorical=False, top_k=5)    \n",
    "    homogeneity, completeness, v_measure = np.around(\n",
    "        homogeneity_completeness_v_measure(true_labels, cluster_labels), 4)\n",
    "    print(model_name, homogeneity, completeness, v_measure, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cono space, eval deno, lower is better\n",
    "for model_name, model in cono_space.items():\n",
    "    cluster_labels, true_labels = NN_cluster_ids(\n",
    "        model, J_ids, categorical=True, top_k=5)    \n",
    "    homogeneity, completeness, v_measure = np.around(\n",
    "        homogeneity_completeness_v_measure(true_labels, cluster_labels), 4)\n",
    "    print(model_name, homogeneity, completeness, v_measure, sep='\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
