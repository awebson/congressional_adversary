{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Tuple, Union, List, Dict, Iterable, Optional\n",
    "\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from decomposer import Decomposer, DecomposerConfig\n",
    "from recomposer import Recomposer, RecomposerConfig\n",
    "# from evaluations.helpers import GroundedWord, load_recomposers_en_masse\n",
    "# from evaluations.clustering import graph_en_masse\n",
    "# from evaluations.euphemism import cherry_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size = 111,386\n",
      "29 capitalists\n",
      "64 socialists\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "BASE_DIR = Path.home() / 'Research/congressional_adversary/results'\n",
    "sup_PE = torch.load(BASE_DIR / 'SGNS deno/pretrained super large/init.pt')['model']\n",
    "WTI = sup_PE.word_to_id\n",
    "ITW = sup_PE.id_to_word\n",
    "sup_PE = sup_PE.embedding.weight.detach().cpu().numpy()\n",
    "print(f'Vocab size = {len(WTI):,}')\n",
    "\n",
    "\n",
    "sub_PE = torch.load(BASE_DIR / 'bill topic/pretrained subset/init.pt')['model']\n",
    "sub_PE_WID = sub_PE.word_to_id\n",
    "sub_PE_GD = sub_PE.grounding\n",
    "del sub_PE\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GroundedWord():\n",
    "    word: str\n",
    "\n",
    "#     def __post_init__(self) -> None:\n",
    "#         self.word_id: int = WTI[self.word]\n",
    "#         metadata = sub_PE_GD[self.word]\n",
    "#         self.freq: int = metadata['freq']\n",
    "#         self.R_ratio: float = metadata['R_ratio']\n",
    "#         self.majority_deno: int = metadata['majority_deno']\n",
    "\n",
    "#         self.PE_neighbors = self.neighbors(sup_PE)\n",
    "            \n",
    "#     def neighbors(self, embed, top_k=10): \n",
    "#         query_id = sup_PE.word_to_id[self.word]\n",
    "#         query_vec = sup_PE[query_id]\n",
    "#         distances = [\n",
    "#             distance.cosine(query_vec, neighbor_vec)\n",
    "#             for neighbor_vec in sup_PE]\n",
    "#         self.sup_PE_neighbors = set()\n",
    "#         for sort_rank, neighbor_id in enumerate(sorted_neighbor_indices):\n",
    "#             if num_neighbors == top_k:\n",
    "#                 break\n",
    "#             if query_id == neighbor_id:\n",
    "#                 continue\n",
    "#             neighbor_word = self.id_to_word[neighbor_id]\n",
    "#             if editdistance.eval(query_word, neighbor_word) < 3:\n",
    "#                 continue\n",
    "            \n",
    "#         self.sub_PE_neighbors: List[str] = nearest(, sub_PE)\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return str(vars(self))\n",
    "    \n",
    "    \n",
    "capitalism: List[GroundedWord] = []\n",
    "socialism: List[GroundedWord] = []\n",
    "for word in sub_PE_WID.keys():\n",
    "    ratio = sub_PE_GD[word]['R_ratio']\n",
    "    freq = sub_PE_GD[word]['freq']\n",
    "    word = GroundedWord(word)\n",
    "    if ratio < 0.2 and freq > 100:  # 0.2:\n",
    "        socialism.append(word)\n",
    "    elif ratio > 0.8 and freq > 100:  # 0.8:\n",
    "        capitalism.append(word)\n",
    "\n",
    "print(\n",
    "    f'{len(capitalism)} capitalists\\n'\n",
    "    f'{len(socialism)} socialists')\n",
    "polarization = capitalism + socialism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_embed(model: Decomposer) -> np.ndarray:\n",
    "    return model.embedding.weight.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "def load(\n",
    "        path: Path,\n",
    "        match_vocab: bool = False,\n",
    "        device: str = 'cpu'\n",
    "        ) -> np.ndarray:\n",
    "    model = torch.load(path, map_location=device)['model']\n",
    "    try:\n",
    "        assert model.word_to_id == WTI\n",
    "    except AssertionError:\n",
    "        print(f'Vocabulary mismatch: {path}')\n",
    "        print(f'Vocab size = {len(model.word_to_id)}')\n",
    "        if match_vocab:\n",
    "            raise RuntimeError\n",
    "        else:\n",
    "            return None\n",
    "    return get_embed(model)\n",
    "\n",
    "\n",
    "def load_decomposers_en_masse(\n",
    "        in_dirs: Union[Path, List[Path]],\n",
    "        patterns: Union[str, List[str]]\n",
    "        ) -> Tuple[Dict[str, np.ndarray], ...]:\n",
    "    if not isinstance(in_dirs, List):\n",
    "        in_dirs = [in_dirs, ]\n",
    "    if not isinstance(patterns, List):\n",
    "        patterns = [patterns, ]\n",
    "    checkpoints: List[Path] = []\n",
    "    for in_dir in in_dirs:\n",
    "        for pattern in patterns:\n",
    "            checkpoints += list(in_dir.glob(pattern))\n",
    "    if len(checkpoints) == 0:\n",
    "        raise FileNotFoundError('No model with path pattern found at in_dir?')\n",
    "\n",
    "    models = {\n",
    "#         'pretrained superset': load(BASE_DIR / 'bill topic/pretrained superset/init.pt'),\n",
    "#         'pretrained subset': load(BASE_DIR / 'bill topic/pretrained subset/init.pt')\n",
    "    }\n",
    "    for path in tqdm(checkpoints):\n",
    "        tqdm.write(f'Loading {path}')\n",
    "        embed = load(path) \n",
    "        if embed is None:\n",
    "            continue\n",
    "#         name = path.parent.name + path.name \n",
    "        name = path.parent.name \n",
    "        models[name] = embed\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "sns.set()\n",
    "\n",
    "def plot(\n",
    "        coordinates: np.ndarray,\n",
    "        words: List[GroundedWord],\n",
    "        path: Path\n",
    "        ) -> None:\n",
    "    fig, ax = plt.subplots(figsize=(15, 10))\n",
    "#     skew = [w.R_ratio for w in words]\n",
    "#     freq = [w.freq for w in words]\n",
    "    sns.scatterplot(\n",
    "        coordinates[:, 0], coordinates[:, 1],\n",
    "#         hue=skew, palette='coolwarm',  # hue_norm=(0, 1),\n",
    "#         size=freq, sizes=(200, 1000),\n",
    "        legend=None, ax=ax)\n",
    "    for coord, w in zip(coordinates, words):\n",
    "        ax.annotate(w.word, coord, fontsize=20)\n",
    "    with open(path, 'wb') as file:\n",
    "        fig.savefig(file, dpi=300)\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def plot_categorical(\n",
    "        coordinates: np.ndarray,\n",
    "        words: List[GroundedWord],\n",
    "        path: Path,\n",
    "        fancy: bool = True\n",
    "        ) -> None:\n",
    "    if fancy:\n",
    "        fig, ax = plt.subplots(figsize=(20, 10))\n",
    "        categories = [w.majority_deno for w in words]\n",
    "        freq = [w.freq for w in words]\n",
    "        sns.scatterplot(\n",
    "            coordinates[:, 0], coordinates[:, 1],\n",
    "            hue=categories, palette='muted', hue_norm=(0, 1),\n",
    "            size=freq, sizes=(200, 1000),\n",
    "            legend='brief', \n",
    "            ax=ax)\n",
    "        chartBox = ax.get_position()\n",
    "        ax.set_position(  # adjust legend\n",
    "            [chartBox.x0, chartBox.y0, chartBox.width * 0.6, chartBox.height])\n",
    "        ax.legend(loc='upper center', bbox_to_anchor=(1.45, 0.8), ncol=1)\n",
    "    else:\n",
    "        fig, ax = plt.subplots(figsize=(20, 10))\n",
    "        freq = [w.freq for w in words]\n",
    "        sns.scatterplot(\n",
    "            coordinates[:, 0], coordinates[:, 1], ax=ax)\n",
    "\n",
    "    for coord, w in zip(coordinates, words):\n",
    "        ax.annotate(w.word, coord, fontsize=12)\n",
    "    with open(path, 'wb') as file:\n",
    "        fig.savefig(file, dpi=300)\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def graph_en_masse(\n",
    "        models: Dict[str, np.ndarray],\n",
    "        out_dir: Path,\n",
    "        reduction: str,  # 'PCA', 'TSNE', or 'both'\n",
    "        words: List[GroundedWord],\n",
    "        # hues: Union[List[float], List[int]],\n",
    "        # sizes: List[int],\n",
    "        perplexity: Optional[int] = None,\n",
    "        categorical: bool = False\n",
    "        ) -> None:\n",
    "    Path.mkdir(out_dir, parents=True, exist_ok=True)\n",
    "    word_ids = np.array([w.word_id for w in words])\n",
    "    for model_name, embed in tqdm(models.items()):\n",
    "        space = embed[word_ids]\n",
    "        if reduction == 'PCA':\n",
    "            visual = PCA(n_components=2).fit_transform(space)\n",
    "        elif reduction == 'TSNE':\n",
    "            assert perplexity is not None\n",
    "            visual = TSNE(\n",
    "                perplexity=perplexity, learning_rate=10,\n",
    "                n_iter=5000, n_iter_without_progress=1000).fit_transform(space)\n",
    "        elif reduction == 'both':\n",
    "            assert perplexity is not None\n",
    "            space = PCA(n_components=30).fit_transform(space)\n",
    "            visual = TSNE(\n",
    "                perplexity=perplexity, learning_rate=10,\n",
    "                n_iter=5000, n_iter_without_progress=1000).fit_transform(space)\n",
    "        else:\n",
    "            raise ValueError('unknown dimension reduction method')\n",
    "        if categorical:\n",
    "            plot_categorical(visual, words, out_dir / f'{model_name}.png')\n",
    "        else:\n",
    "            plot(visual, words, out_dir / f'{model_name}.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cherry_words = [\n",
    "    'government', 'washington',\n",
    "    'estate_tax', 'death_tax',\n",
    "    'public_option', 'governmentrun',\n",
    "    'foreign_trade', 'international_trade',\n",
    "    'cut_taxes', 'trickledown'\n",
    "]\n",
    "\n",
    "cherry_words = [GroundedWord(w) for w in cherry_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d93eedb32f9e428bb4e631bbf136c99a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading ../../results/SGNS deno/sans recomposer/L4 BS4/epoch10.pt\n",
      "\r",
      "Loading ../../results/SGNS deno/sans recomposer/L4/epoch10.pt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base_dir = Path('../../results/SGNS deno/sans recomposer')\n",
    "# deno_space = load_decomposers_en_masse(base_dir, patterns='*/epoch10.pt')\n",
    "deno_space = load_decomposers_en_masse(base_dir, patterns='*/epoch*.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "models = deno_space\n",
    "stuff = polarization\n",
    "\n",
    "graph_en_masse(\n",
    "    models, out_dir=base_dir / 'cherry/topic/t-SNE p5',\n",
    "    reduction='TSNE', perplexity=5, words=stuff, categorical=True)\n",
    "graph_en_masse(\n",
    "    models, out_dir=base_dir / 'cherry/topic/t-SNE p3',\n",
    "    reduction='TSNE', perplexity=3, words=stuff, categorical=True)\n",
    "graph_en_masse(\n",
    "    models, out_dir=base_dir / 'cherry/topic/t-SNE p2',\n",
    "    reduction='TSNE', perplexity=2, words=stuff, categorical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine as cos_dist\n",
    "import editdistance\n",
    "\n",
    "def vec(query: str, embed: np.ndarray) -> np.ndarray:\n",
    "    try:\n",
    "        query_id = WTI[query]\n",
    "    except KeyError:\n",
    "        raise KeyError(f'Out of vocabulary: {query}')\n",
    "    return embed[query_id]\n",
    "\n",
    "\n",
    "def nearest_neighbors(\n",
    "        query: str,\n",
    "        embed: np.ndarray,\n",
    "        top_k: int = 10\n",
    "        ) -> None:\n",
    "    query_vec = vec(query, embed)\n",
    "    print(f\"{query}'s neareset neighbors:\")\n",
    "    distances = [\n",
    "        cos_dist(query_vec, neighbor_vec)\n",
    "        for neighbor_vec in embed]\n",
    "    neighbor_indices = np.argsort(distances)\n",
    "    num_neighbors = 0        \n",
    "    for sort_rank, neighbor_id in enumerate(neighbor_indices):\n",
    "        if num_neighbors == top_k:\n",
    "            break\n",
    "#         if query_id == neighbor_id:\n",
    "#             continue\n",
    "        neighbor_word = ITW[neighbor_id]\n",
    "\n",
    "        if editdistance.eval(query, neighbor_word) < 3:\n",
    "            continue\n",
    "        cosine_similarity = 1 - distances[neighbor_id]\n",
    "        # neighbor_ids.append(neighbor_id)\n",
    "        num_neighbors += 1\n",
    "        print(f'{cosine_similarity:.4f}\\t{neighbor_word}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deno_space['bill topic'] = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['L4 BS4', 'L4'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deno_space.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "death_tax's neareset neighbors:\n",
      "0.8923\testate_tax\n",
      "0.8097\testate_taxes\n",
      "0.7626\trepeal_the_death\n",
      "0.7265\tinheritance\n",
      "0.7078\testates\n",
      "0.7058\tmarriage_penalty\n",
      "0.7054\testate_tax_relief\n",
      "0.7026\tconfiscatory\n",
      "0.6846\ttaxable_event\n",
      "0.6762\tmarriage_penalty_tax\n",
      "\n",
      "death_tax's neareset neighbors:\n",
      "0.5959\tfamilyowned\n",
      "0.5730\tinheritance\n",
      "0.5555\testate_tax\n",
      "0.5261\testate_taxes\n",
      "0.5225\tfamily_farms\n",
      "0.5057\tfamily_farm\n",
      "0.5025\testate\n",
      "0.4966\theirs\n",
      "0.4695\tmarriage_penalty\n",
      "0.4639\tundertaker\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = 'death_tax'\n",
    "nn = nearest_neighbors\n",
    "our_model = deno_space['L4']\n",
    "nn(query, sup_PE)\n",
    "nn(query, our_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estate_tax's neareset neighbors:\n",
      "0.8923\tdeath_tax\n",
      "0.8697\testates\n",
      "0.8360\tdeath_taxes\n",
      "0.8125\testates_would\n",
      "0.7943\testate_tax_relief\n",
      "0.7392\tinheritance\n",
      "0.7075\trepeal_the_death\n",
      "0.7045\testate\n",
      "0.6890\tfamily_farms\n",
      "0.6844\tmarriage_penalty\n",
      "\n",
      "estate_tax's neareset neighbors:\n",
      "0.8851\testates\n",
      "0.7540\testate_tax_relief\n",
      "0.6820\tfamily_farms\n",
      "0.6472\tfamilyowned\n",
      "0.6428\tdemocratic_alternative\n",
      "0.6308\testate\n",
      "0.6284\tinheritance\n",
      "0.6275\testates_would\n",
      "0.6066\twealthiest\n",
      "0.5702\tdeath_taxes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = 'estate_tax'\n",
    "nn(query, sup_PE)\n",
    "nn(query, our_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "public_option's neareset neighbors:\n",
      "0.8769\tpublic_plan\n",
      "0.8208\tgovernment_option\n",
      "0.8003\tgovernmentrun_plan\n",
      "0.7969\tgovernment_plan\n",
      "0.7803\tinsurance_plan\n",
      "0.7760\tgovernmentrun_health\n",
      "0.7517\tpublic_health_insurance\n",
      "0.7455\tprivate_health_insurance\n",
      "0.7432\tlower_costs\n",
      "0.7348\tprivate_market\n",
      "\n",
      "public_option's neareset neighbors:\n",
      "0.7612\tpublic_plan\n",
      "0.6937\tinsurance_plan\n",
      "0.6787\tprivate_health_insurance\n",
      "0.6599\tprivate_insurance\n",
      "0.6548\tlower_costs\n",
      "0.6499\thealth_insurance_companies\n",
      "0.6398\tinsurance_market\n",
      "0.6372\tsinglepayer\n",
      "0.6334\thealth_insurance_industry\n",
      "0.6280\thealth_reform\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = 'public_option'\n",
    "nn(query, sup_PE)\n",
    "nn(query, deno_space['L4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foreign_trade's neareset neighbors:\n",
      "0.7665\tunfair_trade_practices\n",
      "0.7424\tinternational_trade\n",
      "0.7331\tunfair_trade\n",
      "0.7287\tus_trade\n",
      "0.7261\ttrade_practices\n",
      "0.7219\tunfair_trading_practices\n",
      "0.7146\ttrade\n",
      "0.7077\tus_exports\n",
      "0.6895\tforeign_competition\n",
      "0.6880\ttrading_partners\n",
      "\n",
      "foreign_trade's neareset neighbors:\n",
      "0.7244\tunfair_trade_practices\n",
      "0.6509\ttrade_laws\n",
      "0.6469\tunfair_trade\n",
      "0.6448\ttrade_practices\n",
      "0.6325\ttrading_partners\n",
      "0.6280\ttrade_problems\n",
      "0.6245\tus_exports\n",
      "0.6224\tus_trade\n",
      "0.6209\tforeign_competition\n",
      "0.6158\tforeign_markets\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = 'foreign_trade'\n",
    "nn(query, sup_PE)\n",
    "nn(query, deno_space['L4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "political_speech's neareset neighbors:\n",
      "0.8270\tfree_speech\n",
      "0.7660\tthe_first_amendment\n",
      "0.7543\tfirst_amendment_rights\n",
      "0.7400\tfirst_amendment\n",
      "0.7334\tfreedom_of_speech\n",
      "0.7286\tissue_advocacy\n",
      "0.7039\tbuckley_versus_valeo\n",
      "0.6988\tbuckley_v_valeo\n",
      "0.6820\tcampaign_spending\n",
      "0.6659\tthe_first_amendments\n",
      "\n",
      "political_speech's neareset neighbors:\n",
      "0.6674\tbuckley_v_valeo\n",
      "0.6672\tfree_speech\n",
      "0.6569\tissue_advocacy\n",
      "0.6482\tthe_first_amendment\n",
      "0.6435\tfirst_amendment_rights\n",
      "0.6271\tbuckley_versus_valeo\n",
      "0.6209\tcampaign_spending\n",
      "0.6170\tmccainfeingold\n",
      "0.6061\tfirst_amendment\n",
      "0.5947\tbuckley\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = 'political_speech'\n",
    "nn(query, sup_PE)\n",
    "nn(query, deno_space['L4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = cono_space\n",
    "\n",
    "graph_en_masse(\n",
    "    models, out_dir=base_dir / 'cherry/party/t-SNE p5',\n",
    "    reduction='TSNE', perplexity=5, words=cherry_words, categorical=False)\n",
    "\n",
    "graph_en_masse(\n",
    "    models, out_dir=base_dir / 'cherry/party/t-SNE p3',\n",
    "    reduction='TSNE', perplexity=3, words=cherry_words, categorical=False)\n",
    "\n",
    "graph_en_masse(\n",
    "    models, out_dir=base_dir / 'cherry/party/t-SNE p2',\n",
    "    reduction='TSNE', perplexity=2, words=cherry_words, categorical=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = deno_space\n",
    "\n",
    "graph_en_masse(\n",
    "    models,\n",
    "    out_dir=f'{base_dir}/decomposed deno/party/t-SNE p25',\n",
    "    reduction='TSNE', perplexity=25,\n",
    "    word_ids=J_ids, words=J_words, hues=J_skew, sizes=J_freq)\n",
    "\n",
    "graph_en_masse(\n",
    "    models,\n",
    "    out_dir=f'{base_dir}/decomposed deno/party/t-SNE p50',\n",
    "    reduction='TSNE', perplexity=50,\n",
    "    word_ids=J_ids, words=J_words, hues=J_skew, sizes=J_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = cono_space\n",
    "\n",
    "graph_en_masse(\n",
    "    models, out_dir=f'{base_dir}/decomposed cono/topic/t-SNE p5',\n",
    "    reduction='TSNE', perplexity=5,\n",
    "    word_ids=J_ids, words=J_words, hues=J_deno, sizes=J_freq,\n",
    "    categorical=True)\n",
    "\n",
    "graph_en_masse(\n",
    "    models,\n",
    "    out_dir=f'{base_dir}/decomposed cono/topic/t-SNE p3',\n",
    "    reduction='TSNE', perplexity=3,\n",
    "    word_ids=J_ids, words=J_words, hues=J_deno, sizes=J_freq,\n",
    "    categorical=True)\n",
    "\n",
    "graph_en_masse(\n",
    "    models,\n",
    "    out_dir=f'{base_dir}/decomposed cono/topic/t-SNE p10',\n",
    "    reduction='TSNE', perplexity=10,\n",
    "    word_ids=J_ids, words=J_words, hues=J_deno, sizes=J_freq,\n",
    "    categorical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = cono_space\n",
    "\n",
    "# graph_en_masse(\n",
    "#     models,\n",
    "#     out_dir=f'{base_dir}/Joint/topic/PCA',\n",
    "#     reduction='PCA',\n",
    "#     word_ids=J_ids, words=J_words, hues=J_skew, sizes=J_freq)\n",
    "\n",
    "graph_en_masse(\n",
    "    models,\n",
    "    out_dir=f'{base_dir}/decomposed cono/party/t-SNE p25',\n",
    "    reduction='TSNE', perplexity=25,\n",
    "    word_ids=J_ids, words=J_words, hues=J_skew, sizes=J_freq)\n",
    "\n",
    "graph_en_masse(\n",
    "    models,\n",
    "    out_dir=f'{base_dir}/decomposed cono/party/t-SNE p50',\n",
    "    reduction='TSNE', perplexity=50,\n",
    "    word_ids=J_ids, words=J_words, hues=J_skew, sizes=J_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homogeneity V-Measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deno space, eval deno, higher is better\n",
    "for model_name, model in deno_space.items():\n",
    "    cluster_labels, true_labels = NN_cluster_ids(\n",
    "        model, J_ids, categorical=True, top_k=10)    \n",
    "    homogeneity, completeness, v_measure = np.around(\n",
    "        homogeneity_completeness_v_measure(true_labels, cluster_labels), 4)\n",
    "    print(model_name, homogeneity, completeness, v_measure, sep='\\t')\n",
    "#     print(pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deno space, eval cono, lower is better\n",
    "for model_name, model in deno_space.items():\n",
    "    cluster_labels, true_labels = NN_cluster_ids(\n",
    "        model, J_ids, categorical=False, top_k=5)    \n",
    "    homogeneity, completeness, v_measure = np.around(\n",
    "        homogeneity_completeness_v_measure(true_labels, cluster_labels), 4)\n",
    "    print(model_name, homogeneity, completeness, v_measure, sep='\\t')\n",
    "#     print(pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cono space, eval cono, higher is better\n",
    "for model_name, model in cono_space.items():\n",
    "    cluster_labels, true_labels = NN_cluster_ids(\n",
    "        model, J_ids, categorical=False, top_k=5)    \n",
    "    homogeneity, completeness, v_measure = np.around(\n",
    "        homogeneity_completeness_v_measure(true_labels, cluster_labels), 4)\n",
    "    print(model_name, homogeneity, completeness, v_measure, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cono space, eval deno, lower is better\n",
    "for model_name, model in cono_space.items():\n",
    "    cluster_labels, true_labels = NN_cluster_ids(\n",
    "        model, J_ids, categorical=True, top_k=5)    \n",
    "    homogeneity, completeness, v_measure = np.around(\n",
    "        homogeneity_completeness_v_measure(true_labels, cluster_labels), 4)\n",
    "    print(model_name, homogeneity, completeness, v_measure, sep='\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
