{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Tuple, Union, List, Dict, Iterable, Optional\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from decomposer import Decomposer, DecomposerConfig\n",
    "from recomposer import Recomposer, RecomposerConfig\n",
    "# from evaluations.helpers import GroundedWord, load_recomposers_en_masse\n",
    "# from evaluations.clustering import graph_en_masse\n",
    "# from evaluations.euphemism import cherry_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size = 138,441\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "BASE_DIR = Path.home() / 'Research/congressional_adversary/results'\n",
    "# sup_PE = torch.load(BASE_DIR / 'SGNS deno/pretrained super large/init.pt')['model']\n",
    "# sup_PE = torch.load(BASE_DIR / 'news/validation/backup PE/init.pt')['model']\n",
    "# sup_PE = torch.load(BASE_DIR / 'news/validation/pretrained/init.pt')['model']\n",
    "sup_PE = torch.load(BASE_DIR / 'news/validation/pretrained/init.pt')['model']\n",
    "# sup_PE = torch.load(BASE_DIR / 'news/train/pretrained/init.pt')['model']\n",
    "WTI = sup_PE.word_to_id\n",
    "ITW = sup_PE.id_to_word\n",
    "grounding = sup_PE.cono_grounding\n",
    "\n",
    "def GD(query):\n",
    "    freq = grounding[WTI[query]]\n",
    "    ratio = torch.nn.functional.normalize(freq, dim=0, p=1)\n",
    "    \n",
    "    print(query, end='\\t')\n",
    "    for r in ratio.tolist():\n",
    "        print(round(r, 4), end=', ')\n",
    "    print(end='\\t')\n",
    "    \n",
    "    for f in freq.tolist():    \n",
    "        print(int(f), end=', ')\n",
    "    print()\n",
    "    \n",
    "    \n",
    "\n",
    "sup_PE = sup_PE.embedding.weight.detach().cpu().numpy()\n",
    "print(f'Vocab size = {len(WTI):,}')\n",
    "\n",
    "\n",
    "# sub_PE = torch.load(BASE_DIR / 'bill topic/pretrained subset/init.pt')['model']\n",
    "# sub_PE_WID = sub_PE.word_to_id\n",
    "# sub_PE_GD = sub_PE.grounding\n",
    "# del sub_PE\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GroundedWord():\n",
    "    word: str\n",
    "\n",
    "#     def __post_init__(self) -> None:\n",
    "#         self.word_id: int = WTI[self.word]\n",
    "#         metadata = sub_PE_GD[self.word]\n",
    "#         self.freq: int = metadata['freq']\n",
    "#         self.R_ratio: float = metadata['R_ratio']\n",
    "#         self.majority_deno: int = metadata['majority_deno']\n",
    "\n",
    "#         self.PE_neighbors = self.neighbors(sup_PE)\n",
    "            \n",
    "#     def neighbors(self, embed, top_k=10): \n",
    "#         query_id = sup_PE.word_to_id[self.word]\n",
    "#         query_vec = sup_PE[query_id]\n",
    "#         distances = [\n",
    "#             distance.cosine(query_vec, neighbor_vec)\n",
    "#             for neighbor_vec in sup_PE]\n",
    "#         self.sup_PE_neighbors = set()\n",
    "#         for sort_rank, neighbor_id in enumerate(sorted_neighbor_indices):\n",
    "#             if num_neighbors == top_k:\n",
    "#                 break\n",
    "#             if query_id == neighbor_id:\n",
    "#                 continue\n",
    "#             neighbor_word = self.id_to_word[neighbor_id]\n",
    "#             if editdistance.eval(query_word, neighbor_word) < 3:\n",
    "#                 continue\n",
    "            \n",
    "#         self.sub_PE_neighbors: List[str] = nearest(, sub_PE)\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return str(vars(self))\n",
    "    \n",
    "    \n",
    "# capitalism: List[GroundedWord] = []\n",
    "# socialism: List[GroundedWord] = []\n",
    "# for word in sub_PE_WID.keys():\n",
    "#     ratio = sub_PE_GD[word]['R_ratio']\n",
    "#     freq = sub_PE_GD[word]['freq']\n",
    "#     word = GroundedWord(word)\n",
    "#     if ratio < 0.2 and freq > 100:  # 0.2:\n",
    "#         socialism.append(word)\n",
    "#     elif ratio > 0.8 and freq > 100:  # 0.8:\n",
    "#         capitalism.append(word)\n",
    "\n",
    "# print(\n",
    "#     f'{len(capitalism)} capitalists\\n'\n",
    "#     f'{len(socialism)} socialists')\n",
    "# polarization = capitalism + socialism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine as cos_dist\n",
    "import editdistance\n",
    "\n",
    "def vec(query: str, embed: np.ndarray) -> np.ndarray:\n",
    "    try:\n",
    "        query_id = WTI[query]\n",
    "    except KeyError:\n",
    "        raise KeyError(f'Out of vocabulary: {query}')\n",
    "    return embed[query_id]\n",
    "\n",
    "\n",
    "def nearest_neighbors(\n",
    "        query: str,\n",
    "        embed: np.ndarray,\n",
    "        top_k: int = 10\n",
    "        ) -> None:\n",
    "    query_vec = vec(query, embed)\n",
    "#     print(f\"{query}â€™s neareset neighbors:\")\n",
    "    distances = [\n",
    "        cos_dist(query_vec, neighbor_vec)\n",
    "        for neighbor_vec in embed]\n",
    "    neighbor_indices = np.argsort(distances)\n",
    "    num_neighbors = 0        \n",
    "    for sort_rank, neighbor_id in enumerate(neighbor_indices):\n",
    "        if num_neighbors == top_k:\n",
    "            break\n",
    "#         if query_id == neighbor_id:\n",
    "#             continue\n",
    "        neighbor_word = ITW[neighbor_id]\n",
    "\n",
    "        if editdistance.eval(query, neighbor_word) < 3:\n",
    "            continue\n",
    "        cosine_similarity = 1 - distances[neighbor_id]\n",
    "        # neighbor_ids.append(neighbor_id)\n",
    "        num_neighbors += 1\n",
    "        print(f'{cosine_similarity:.4f}\\t{neighbor_word}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embed(model: Decomposer) -> np.ndarray:\n",
    "    return model.embedding.weight.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "def load(\n",
    "        path: Path,\n",
    "        match_vocab: bool = False,\n",
    "        device: str = 'cpu'\n",
    "        ) -> np.ndarray:\n",
    "    model = torch.load(path, map_location=device)['model']\n",
    "    try:\n",
    "        assert model.word_to_id == WTI\n",
    "    except AssertionError:\n",
    "        print(f'Vocabulary mismatch: {path}')\n",
    "        print(f'Vocab size = {len(model.word_to_id)}')\n",
    "        if match_vocab:\n",
    "            raise RuntimeError\n",
    "        else:\n",
    "            return None\n",
    "    return get_embed(model)\n",
    "\n",
    "\n",
    "def load_decomposers_en_masse(\n",
    "        in_dirs: Union[Path, List[Path]],\n",
    "        patterns: Union[str, List[str]]\n",
    "        ) -> Tuple[Dict[str, np.ndarray], ...]:\n",
    "    if not isinstance(in_dirs, List):\n",
    "        in_dirs = [in_dirs, ]\n",
    "    if not isinstance(patterns, List):\n",
    "        patterns = [patterns, ]\n",
    "    checkpoints: List[Path] = []\n",
    "    for in_dir in in_dirs:\n",
    "        for pattern in patterns:\n",
    "            checkpoints += list(in_dir.glob(pattern))\n",
    "    if len(checkpoints) == 0:\n",
    "        raise FileNotFoundError('No model with path pattern found at in_dir?')\n",
    "\n",
    "    models = {\n",
    "#         'pretrained superset': load(BASE_DIR / 'bill topic/pretrained superset/init.pt'),\n",
    "#         'pretrained subset': load(BASE_DIR / 'bill topic/pretrained subset/init.pt')\n",
    "    }\n",
    "    for path in tqdm(checkpoints):\n",
    "        tqdm.write(f'Loading {path}')\n",
    "        embed = load(path) \n",
    "        if embed is None:\n",
    "            continue\n",
    "#         name = path.parent.name\n",
    "        name = path.parent.name + '/' + path.name\n",
    "        models[name] = embed\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7a6b288ef97441a9580f06b85400267",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ../../results/news/validation/-0.01d/epoch2.pt\n",
      "Loading ../../results/news/validation/-3c/epoch2.pt\n",
      "Loading ../../results/news/validation/1d 0c/epoch2.pt\n",
      "\n",
      "dict_keys(['-0.01d/epoch2.pt', '-3c/epoch2.pt', '1d 0c/epoch2.pt'])\n"
     ]
    }
   ],
   "source": [
    "# base_dir = Path('../../results/SGNS deno/sans recomposer')\n",
    "# deno_space = load_decomposers_en_masse(base_dir, patterns='*/epoch10.pt')\n",
    "\n",
    "base_dir = Path('../../results/news/validation')\n",
    "deno_space = load_decomposers_en_masse(base_dir, patterns='*/epoch2.pt')\n",
    "# deno_space.update(load_decomposers_en_masse(base_dir, patterns='*/epoch3.pt'))\n",
    "\n",
    "print(deno_space.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "sns.set()\n",
    "\n",
    "def plot(\n",
    "        coordinates: np.ndarray,\n",
    "        words: List[GroundedWord],\n",
    "        path: Path\n",
    "        ) -> None:\n",
    "    fig, ax = plt.subplots(figsize=(15, 10))\n",
    "#     skew = [w.R_ratio for w in words]\n",
    "#     freq = [w.freq for w in words]\n",
    "    sns.scatterplot(\n",
    "        coordinates[:, 0], coordinates[:, 1],\n",
    "#         hue=skew, palette='coolwarm',  # hue_norm=(0, 1),\n",
    "#         size=freq, sizes=(200, 1000),\n",
    "        legend=None, ax=ax)\n",
    "    for coord, w in zip(coordinates, words):\n",
    "        ax.annotate(w.word, coord, fontsize=20)\n",
    "    with open(path, 'wb') as file:\n",
    "        fig.savefig(file, dpi=300)\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def plot_categorical(\n",
    "        coordinates: np.ndarray,\n",
    "        words: List[GroundedWord],\n",
    "        path: Path,\n",
    "        fancy: bool = True\n",
    "        ) -> None:\n",
    "    if fancy:\n",
    "        fig, ax = plt.subplots(figsize=(20, 10))\n",
    "        categories = [w.majority_deno for w in words]\n",
    "        freq = [w.freq for w in words]\n",
    "        sns.scatterplot(\n",
    "            coordinates[:, 0], coordinates[:, 1],\n",
    "            hue=categories, palette='muted', hue_norm=(0, 1),\n",
    "            size=freq, sizes=(200, 1000),\n",
    "            legend='brief', \n",
    "            ax=ax)\n",
    "        chartBox = ax.get_position()\n",
    "        ax.set_position(  # adjust legend\n",
    "            [chartBox.x0, chartBox.y0, chartBox.width * 0.6, chartBox.height])\n",
    "        ax.legend(loc='upper center', bbox_to_anchor=(1.45, 0.8), ncol=1)\n",
    "    else:\n",
    "        fig, ax = plt.subplots(figsize=(20, 10))\n",
    "        freq = [w.freq for w in words]\n",
    "        sns.scatterplot(\n",
    "            coordinates[:, 0], coordinates[:, 1], ax=ax)\n",
    "\n",
    "    for coord, w in zip(coordinates, words):\n",
    "        ax.annotate(w.word, coord, fontsize=12)\n",
    "    with open(path, 'wb') as file:\n",
    "        fig.savefig(file, dpi=300)\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def graph_en_masse(\n",
    "        models: Dict[str, np.ndarray],\n",
    "        out_dir: Path,\n",
    "        reduction: str,  # 'PCA', 'TSNE', or 'both'\n",
    "        words: List[GroundedWord],\n",
    "        # hues: Union[List[float], List[int]],\n",
    "        # sizes: List[int],\n",
    "        perplexity: Optional[int] = None,\n",
    "        categorical: bool = False\n",
    "        ) -> None:\n",
    "    Path.mkdir(out_dir, parents=True, exist_ok=True)\n",
    "    word_ids = np.array([w.word_id for w in words])\n",
    "    for model_name, embed in tqdm(models.items()):\n",
    "        space = embed[word_ids]\n",
    "        if reduction == 'PCA':\n",
    "            visual = PCA(n_components=2).fit_transform(space)\n",
    "        elif reduction == 'TSNE':\n",
    "            assert perplexity is not None\n",
    "            visual = TSNE(\n",
    "                perplexity=perplexity, learning_rate=10,\n",
    "                n_iter=5000, n_iter_without_progress=1000).fit_transform(space)\n",
    "        elif reduction == 'both':\n",
    "            assert perplexity is not None\n",
    "            space = PCA(n_components=30).fit_transform(space)\n",
    "            visual = TSNE(\n",
    "                perplexity=perplexity, learning_rate=10,\n",
    "                n_iter=5000, n_iter_without_progress=1000).fit_transform(space)\n",
    "        else:\n",
    "            raise ValueError('unknown dimension reduction method')\n",
    "        if categorical:\n",
    "            plot_categorical(visual, words, out_dir / f'{model_name}.png')\n",
    "        else:\n",
    "            plot(visual, words, out_dir / f'{model_name}.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cherry_words = [\n",
    "    'government', 'washington',\n",
    "    'estate_tax', 'death_tax',\n",
    "    'public_option', 'governmentrun',\n",
    "    'foreign_trade', 'international_trade',\n",
    "    'cut_taxes', 'trickledown'\n",
    "]\n",
    "\n",
    "cherry_words = [GroundedWord(w) for w in cherry_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "models = deno_space\n",
    "stuff = polarization\n",
    "\n",
    "graph_en_masse(\n",
    "    models, out_dir=base_dir / 'cherry/topic/t-SNE p5',\n",
    "    reduction='TSNE', perplexity=5, words=stuff, categorical=True)\n",
    "graph_en_masse(\n",
    "    models, out_dir=base_dir / 'cherry/topic/t-SNE p3',\n",
    "    reduction='TSNE', perplexity=3, words=stuff, categorical=True)\n",
    "graph_en_masse(\n",
    "    models, out_dir=base_dir / 'cherry/topic/t-SNE p2',\n",
    "    reduction='TSNE', perplexity=2, words=stuff, categorical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = cono_space\n",
    "\n",
    "graph_en_masse(\n",
    "    models, out_dir=base_dir / 'cherry/party/t-SNE p5',\n",
    "    reduction='TSNE', perplexity=5, words=cherry_words, categorical=False)\n",
    "\n",
    "graph_en_masse(\n",
    "    models, out_dir=base_dir / 'cherry/party/t-SNE p3',\n",
    "    reduction='TSNE', perplexity=3, words=cherry_words, categorical=False)\n",
    "\n",
    "graph_en_masse(\n",
    "    models, out_dir=base_dir / 'cherry/party/t-SNE p2',\n",
    "    reduction='TSNE', perplexity=2, words=cherry_words, categorical=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = deno_space\n",
    "\n",
    "graph_en_masse(\n",
    "    models,\n",
    "    out_dir=f'{base_dir}/decomposed deno/party/t-SNE p25',\n",
    "    reduction='TSNE', perplexity=25,\n",
    "    word_ids=J_ids, words=J_words, hues=J_skew, sizes=J_freq)\n",
    "\n",
    "graph_en_masse(\n",
    "    models,\n",
    "    out_dir=f'{base_dir}/decomposed deno/party/t-SNE p50',\n",
    "    reduction='TSNE', perplexity=50,\n",
    "    word_ids=J_ids, words=J_words, hues=J_skew, sizes=J_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = cono_space\n",
    "\n",
    "graph_en_masse(\n",
    "    models, out_dir=f'{base_dir}/decomposed cono/topic/t-SNE p5',\n",
    "    reduction='TSNE', perplexity=5,\n",
    "    word_ids=J_ids, words=J_words, hues=J_deno, sizes=J_freq,\n",
    "    categorical=True)\n",
    "\n",
    "graph_en_masse(\n",
    "    models,\n",
    "    out_dir=f'{base_dir}/decomposed cono/topic/t-SNE p3',\n",
    "    reduction='TSNE', perplexity=3,\n",
    "    word_ids=J_ids, words=J_words, hues=J_deno, sizes=J_freq,\n",
    "    categorical=True)\n",
    "\n",
    "graph_en_masse(\n",
    "    models,\n",
    "    out_dir=f'{base_dir}/decomposed cono/topic/t-SNE p10',\n",
    "    reduction='TSNE', perplexity=10,\n",
    "    word_ids=J_ids, words=J_words, hues=J_deno, sizes=J_freq,\n",
    "    categorical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = cono_space\n",
    "\n",
    "# graph_en_masse(\n",
    "#     models,\n",
    "#     out_dir=f'{base_dir}/Joint/topic/PCA',\n",
    "#     reduction='PCA',\n",
    "#     word_ids=J_ids, words=J_words, hues=J_skew, sizes=J_freq)\n",
    "\n",
    "graph_en_masse(\n",
    "    models,\n",
    "    out_dir=f'{base_dir}/decomposed cono/party/t-SNE p25',\n",
    "    reduction='TSNE', perplexity=25,\n",
    "    word_ids=J_ids, words=J_words, hues=J_skew, sizes=J_freq)\n",
    "\n",
    "graph_en_masse(\n",
    "    models,\n",
    "    out_dir=f'{base_dir}/decomposed cono/party/t-SNE p50',\n",
    "    reduction='TSNE', perplexity=50,\n",
    "    word_ids=J_ids, words=J_words, hues=J_skew, sizes=J_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homogeneity V-Measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deno space, eval deno, higher is better\n",
    "for model_name, model in deno_space.items():\n",
    "    cluster_labels, true_labels = NN_cluster_ids(\n",
    "        model, J_ids, categorical=True, top_k=10)    \n",
    "    homogeneity, completeness, v_measure = np.around(\n",
    "        homogeneity_completeness_v_measure(true_labels, cluster_labels), 4)\n",
    "    print(model_name, homogeneity, completeness, v_measure, sep='\\t')\n",
    "#     print(pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deno space, eval cono, lower is better\n",
    "for model_name, model in deno_space.items():\n",
    "    cluster_labels, true_labels = NN_cluster_ids(\n",
    "        model, J_ids, categorical=False, top_k=5)    \n",
    "    homogeneity, completeness, v_measure = np.around(\n",
    "        homogeneity_completeness_v_measure(true_labels, cluster_labels), 4)\n",
    "    print(model_name, homogeneity, completeness, v_measure, sep='\\t')\n",
    "#     print(pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cono space, eval cono, higher is better\n",
    "for model_name, model in cono_space.items():\n",
    "    cluster_labels, true_labels = NN_cluster_ids(\n",
    "        model, J_ids, categorical=False, top_k=5)    \n",
    "    homogeneity, completeness, v_measure = np.around(\n",
    "        homogeneity_completeness_v_measure(true_labels, cluster_labels), 4)\n",
    "    print(model_name, homogeneity, completeness, v_measure, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cono space, eval deno, lower is better\n",
    "for model_name, model in cono_space.items():\n",
    "    cluster_labels, true_labels = NN_cluster_ids(\n",
    "        model, J_ids, categorical=True, top_k=5)    \n",
    "    homogeneity, completeness, v_measure = np.around(\n",
    "        homogeneity_completeness_v_measure(true_labels, cluster_labels), 4)\n",
    "    print(model_name, homogeneity, completeness, v_measure, sep='\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
