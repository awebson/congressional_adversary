{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "from typing import Set, Tuple, List, Dict, Counter\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import scipy\n",
    "from sklearn.metrics import pairwise\n",
    "from tqdm import tqdm\n",
    "\n",
    "from adversarial import AdversarialDecomposer, AdversarialConfig\n",
    "# from preprocessing.S4_export_training_corpus import Document\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "class Embedding():\n",
    "    \n",
    "    def __init__(self, source: str, path: str):\n",
    "        if source == 'adversarial':\n",
    "            self.init_from_adversarial(path)\n",
    "        elif source == 'skip_gram':\n",
    "            self.init_from_skip_gram(path)\n",
    "        elif source == 'plain_text':\n",
    "            self.init_from_plain_text(path)\n",
    "        else:\n",
    "            raise ValueError('Unknown embedding source.')\n",
    "            \n",
    "    def init_from_adversarial(self, path: str, device=torch.device('cpu')):\n",
    "        payload = torch.load(path, map_location=device)\n",
    "        model = payload['model']\n",
    "        self.word_to_id = model.word_to_id\n",
    "        self.id_to_word = model.id_to_word \n",
    "        self.Dem_frequency: Counter[str] = model.Dem_frequency\n",
    "        self.GOP_frequency: Counter[str] = model.GOP_frequency\n",
    "        \n",
    "        # encoded layer\n",
    "        self.embedding = model.export_decomposed_embedding(device=device)\n",
    "\n",
    "#         # manually choose which layer to export\n",
    "#         all_vocab_ids = torch.arange(\n",
    "#             len(self.word_to_id), dtype=torch.long, device=device)\n",
    "#         with torch.no_grad():\n",
    "#             embed = model.embedding(all_vocab_ids)\n",
    "#             encoded = model.encoder(embed)\n",
    "#             self.cono_logits = model.cono_decoder(encoded)\n",
    "            \n",
    "#     def init_from_adversarial(self, path: str):        \n",
    "#         config = DenotationEncoderConfig()\n",
    "#         config.input_dir = '../../data/processed/adversarial/44_Obama_1e-5'\n",
    "#         data = AdversarialDataset(config)\n",
    "#         model = DenotationEncoder(config, data)\n",
    "#         model.load_state_dict(torch.load(path))\n",
    "#         self.embedding = model.export_decomposed_embedding().to('cpu')\n",
    "#         self.word_to_id = model.word_to_id\n",
    "#         self.id_to_word = model.id_to_word\n",
    "\n",
    "    def init_from_skip_gram(self, paths: Tuple[str, str]) -> None:\n",
    "        \"\"\"Directly extract the weights of a single layer.\"\"\"\n",
    "        model_path, vocab_path = paths\n",
    "        with open(model_path, 'rb') as model_file:\n",
    "            state_dict = torch.load(model_file, map_location='cpu')\n",
    "    #     print(state_dict.keys())\n",
    "        self.embedding = state_dict['center_embedding.weight'].numpy()\n",
    "        with open(vocab_path, 'rb') as vocab_file:\n",
    "            self.word_to_id, self.id_to_word, _ = pickle.load(vocab_file)\n",
    "\n",
    "    def init_from_plain_text(self, path: str) -> Tuple[np.array, Dict[str, int]]:\n",
    "        id_generator = 0\n",
    "        word_to_id: Dict[str, int] = {}\n",
    "        embeddings: List[float] = []\n",
    "        embedding_file = open(path)\n",
    "        vocab_size, num_dimensions = map(int, embedding_file.readline().split())\n",
    "        print(f'vocab_size = {vocab_size:,}, num_dimensions = {num_dimensions}')\n",
    "        print(f'Loading embeddings from {path}', flush=True)\n",
    "        for line in embedding_file:\n",
    "            line: List[str] = line.split()  # type: ignore\n",
    "            word = line[0]\n",
    "            vector = np.array(line[-num_dimensions:], dtype=np.float64)\n",
    "            embeddings.append(vector)\n",
    "            word_to_id[word] = id_generator\n",
    "            id_generator += 1\n",
    "        embedding_file.close()\n",
    "        print('Done')\n",
    "        self.id_to_word = {val: key for key, val in word_to_id.items()}\n",
    "        self.word_to_id = word_to_id\n",
    "        self.embedding = np.array(embeddings)\n",
    "        \n",
    "    def write_to_tensorboard_projector(self, tb_dir: str) -> None:\n",
    "        from torch.utils import tensorboard\n",
    "        tb = tensorboard.SummaryWriter(log_dir=tb_dir)\n",
    "        all_vocab_ids = range(len(self.word_to_id))\n",
    "        embedding_labels = [\n",
    "            self.id_to_word[word_id]\n",
    "            for word_id in all_vocab_ids]\n",
    "        tb.add_embedding(\n",
    "            self.embedding[:9999], \n",
    "            embedding_labels[:9999], \n",
    "            global_step=0)\n",
    "        \n",
    "    def export_web_projector(self, out_dir: str) -> None:\n",
    "        random_indices = np.random.randint(len(self.embedding), size=10000)\n",
    "        subset_embedding = self.embedding[random_indices].tolist()\n",
    "        \n",
    "        vector_path = os.path.join(out_dir, 'tensorboard.tsv')\n",
    "        with open(vector_path, 'w') as vector_file:\n",
    "            for vector in subset_embedding:\n",
    "                vector_file.write('\\t'.join(map(str, vector)) + '\\n')\n",
    "\n",
    "        label_path = os.path.join(out_dir, 'tensorboard_labels.tsv')\n",
    "        with open(label_path, 'w') as label_file:\n",
    "            for index in random_indices:\n",
    "                label_file.write(self.id_to_word[index] + '\\n')\n",
    "\n",
    "    def cosine_similarity(self, query1: str, query2: str) -> float:\n",
    "        try:\n",
    "            query1_id = self.word_to_id[query1]\n",
    "        except KeyError as error:\n",
    "            print(f'Out of vocabulary: {query1}')\n",
    "            raise error\n",
    "        try:\n",
    "            query2_id = self.word_to_id[query2]\n",
    "        except KeyError as error:\n",
    "            print(f'Out of vocabulary: {query2}')\n",
    "            raise error\n",
    "        vectors = self.embedding[(query1_id, query2_id), :]\n",
    "        similarity = 1 - scipy.spatial.distance.cosine(vectors[0], vectors[1])\n",
    "        return similarity\n",
    "\n",
    "    def nearest_neighbor(self, query: str, top_k: int = 10):\n",
    "        try:\n",
    "            query_id = self.word_to_id[query]\n",
    "        except KeyError:\n",
    "            raise KeyError(f'{query} is out of vocabulary. Sorry!')    \n",
    "        query_vec = self.embedding[query_id]\n",
    "        \n",
    "        distances = [scipy.spatial.distance.cosine(query_vec, vec) \n",
    "                     for vec in self.embedding]\n",
    "        neighbors = np.argsort(distances)\n",
    "        print(f\"{query}'s neareset neighbors:\")\n",
    "        for ranking in range(1, top_k + 1):\n",
    "            word_id = neighbors[ranking]\n",
    "            word = self.id_to_word[word_id]\n",
    "            cosine_similarity = 1 - distances[word_id]\n",
    "            print(f'{cosine_similarity:.4f}\\t{word}')\n",
    "        print()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size = 28,177, num_dimensions = 300\n",
      "Loading embeddings from ../../data/processed/pretrained_word2vec/Obama_SGNS.txt\n",
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/webson/Research/congressional_adversary/congressional_env/lib/python3.7/site-packages/torch/serialization.py:454: SourceChangeWarning: source code of class 'adversarial.AdversarialDecomposer' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    }
   ],
   "source": [
    "base_dir = '../../results/Obama/'\n",
    "models = {}\n",
    "models['w2v'] = Embedding('plain_text', '../../data/processed/pretrained_word2vec/Obama_SGNS.txt')\n",
    "models['0c'] = Embedding('adversarial', base_dir + 'remove cono/1d0c/epoch30.pt')\n",
    "models['-8c'] = Embedding('adversarial', base_dir + 'remove cono/1d-8c/epoch30.pt')\n",
    "models['-10c'] = Embedding('adversarial', base_dir + 'remove cono/1d-10c/epoch30.pt')\n",
    "models['-12c'] = Embedding('adversarial', base_dir + 'remove cono/1d-12c/epoch30.pt')\n",
    "models['-12.75c'] = Embedding('adversarial', base_dir + 'remove cono/1d-12.75c/epoch30.pt')\n",
    "models['-12c NTi'] = Embedding('adversarial', base_dir + 'remove cono/1d-12c NTi/epoch30.pt')\n",
    "\n",
    "# models['vanilla cono'] = Embedding('adversarial', base_dir + '0d1c/epoch30.pt')\n",
    "# models['cono minus deno'] = Embedding('adversarial', base_dir + '-0.05d1c/epoch12.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size = 53,887, num_dimensions = 300\n",
      "Loading embeddings from ../../data/processed/pretrained_word2vec/W_Bush.txt\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "base_dir = '../../results/W_Bush/'\n",
    "models = {}\n",
    "models['w2v'] = Embedding('plain_text', '../../data/processed/pretrained_word2vec/W_Bush.txt')\n",
    "models['0c'] = Embedding('adversarial', base_dir + '1d 0c/epoch27.pt')\n",
    "# models['-10c'] = Embedding('adversarial', base_dir + '1d -10c/epoch21.pt')\n",
    "models['-12c'] = Embedding('adversarial', base_dir + '1d -12c/epoch27.pt')\n",
    "\n",
    "models['1c'] = Embedding('adversarial', base_dir + '0d 1c/epoch27.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models['cono'].write_to_tensorboard_projector(\n",
    "#     '../../results/adversarial/Obama/p8_.55to.75/d0_c1/embedding_projector')\n",
    "# models['cono'].export_web_tensorboard('../../results/adversarial/Obama/p8_.55to.75/d0_c1/web_projector')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models['deno minus cono']\n",
    "cherries = [\n",
    "    'estate_tax', 'death_tax', \n",
    "    'undocumented', 'illegal_aliens', \n",
    "    'music', 'language']\n",
    "for cherry in cherries:\n",
    "    model.nearest_neighbor(cherry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cherry_pick(model1, model2):\n",
    "    \"\"\"prints difference := model2 - model1\"\"\"   \n",
    "#     def print_similarities(pairs):\n",
    "#         for word1, word2 in pairs:\n",
    "#             try: \n",
    "#                 print(f'{model.cosine_similarity(word1, word2):.4f}  '\n",
    "#                       f'{word1:<30}{word2:<30}')\n",
    "#             except KeyError:\n",
    "#                 pass\n",
    "    \n",
    "    def compare_similarities(pairs):\n",
    "        for word1, word2 in pairs:\n",
    "            try:\n",
    "                sim1 = model1.cosine_similarity(word1, word2)\n",
    "                sim2 = model2.cosine_similarity(word1, word2)\n",
    "            except KeyError:\n",
    "                continue\n",
    "            delta = sim2 - sim1\n",
    "            print(f'{sim1:.1%}\\t{sim2:.1%}\\t{delta:+.1%}  '\n",
    "                  f'{word1:<30}{word2:<30}')\n",
    "        \n",
    "    print('Same entity, different parties.\\n'\n",
    "          'Removing connotation should increase similarity.\\n'\n",
    "          'Removing denotation should decrease similarity.')\n",
    "    cherries = [\n",
    "        ('estate_tax', 'death_tax'), \n",
    "        ('undocumented', 'illegal_aliens'),\n",
    "        ('american_clean_energy', 'national_energy_tax'),\n",
    "        ('capandtax', 'capandtrade_legislation'),\n",
    "        ('protection_and_affordable', 'obamacare'),\n",
    "        ('the_health_care_reform', 'obamacare'),\n",
    "        ('public_option', 'socialized_medicine'),\n",
    "        ('guns', 'second_amendment_rights'),\n",
    "        ('washington_spending', 'federeal_budget'),\n",
    "        ('icbms', 'star_wars_program'),\n",
    "        ('excessive_speculation', 'highfrequency'),\n",
    "        ('corporate_profit', 'earnings'), \n",
    "        ('megabanks', 'aig'),\n",
    "        ('unemployment_insurance_benefits', 'stimulus'),\n",
    "        ('retroactive_immunity', 'the_fisa_bill'),\n",
    "        ('give_tax_breaks', 'cut_taxes')\n",
    "    # capandtax, wall_street_reform, the_recovery_act, lesbian, inequality\n",
    "    #     'health_care_bill', ',\n",
    "    #     'the_wall_street_reform_legislation', 'financial_stability', 'capital_gains_tax',\n",
    "    #     'deficit_spending', 'bush_tax_cuts'\n",
    "    ]    \n",
    "    compare_similarities(cherries)\n",
    "     \n",
    "    print('\\n\\nDifferent entities, same party.\\n'\n",
    "          'Removing connotation should decresase similarity.\\n'\n",
    "          'Removing denotation should increase similarity.')\n",
    "    ideologies = [\n",
    "        ('tax_cuts', 'right_to_life'),\n",
    "        ('new_entitlements', 'religious_freedom'),\n",
    "        ('illegals', 'unborn'),\n",
    "        ('antitrust_laws', 'lesbian'),\n",
    "        ('wall_street_reform', 'the_recovery_act'),\n",
    "        ('nuclear_option', 'clean_energy_jobs'), \n",
    "        ('record_deficits', 'living_wage'),\n",
    "        ('manmade_global_warming', 'radical_jihadists')\n",
    "    ]\n",
    "#     ideologies = [  # difficult to distinguish\n",
    "#         ('tax_cuts', 'new_entitlements'),\n",
    "#         ('religious_freedom', 'right_to_life')\n",
    "#     ]\n",
    "    compare_similarities(ideologies)\n",
    "    \n",
    "    print('\\n\\nDifferent entities, different parties. Control group.')\n",
    "    controls = [\n",
    "        ('taxes', 'antitrust_laws'),\n",
    "        ('carbon', 'guns'),\n",
    "        ('abortion', 'minimum_wage'),\n",
    "        ('apple', 'piano'),\n",
    "        ('beef', 'computer')\n",
    "    ]\n",
    "    compare_similarities(controls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Same entity, different parties.\n",
      "Removing connotation should increase similarity.\n",
      "Removing denotation should decrease similarity.\n",
      "95.3%\t88.3%\t-7.0%  estate_tax                    death_tax                     \n",
      "89.9%\t74.3%\t-15.6%  undocumented                  illegal_aliens                \n",
      "68.0%\t48.8%\t-19.2%  american_clean_energy         national_energy_tax           \n",
      "90.9%\t77.6%\t-13.3%  capandtax                     capandtrade_legislation       \n",
      "74.5%\t46.4%\t-28.0%  protection_and_affordable     obamacare                     \n",
      "93.6%\t77.1%\t-16.5%  the_health_care_reform        obamacare                     \n",
      "83.2%\t63.2%\t-20.0%  public_option                 socialized_medicine           \n",
      "68.6%\t16.2%\t-52.4%  guns                          second_amendment_rights       \n",
      "Out of vocabulary: washington_spending\n",
      "Out of vocabulary: star_wars_program\n",
      "Out of vocabulary: excessive_speculation\n",
      "Out of vocabulary: corporate_profit\n",
      "82.5%\t55.2%\t-27.4%  megabanks                     aig                           \n",
      "85.4%\t56.5%\t-28.9%  unemployment_insurance_benefitsstimulus                      \n",
      "Out of vocabulary: retroactive_immunity\n",
      "Out of vocabulary: give_tax_breaks\n",
      "\n",
      "\n",
      "Different entities, same party.\n",
      "Removing connotation should decresase similarity.\n",
      "Removing denotation should increase similarity.\n",
      "48.3%\t-15.8%\t-64.2%  tax_cuts                      right_to_life                 \n",
      "56.6%\t-43.8%\t-100.4%  new_entitlements              religious_freedom             \n",
      "79.4%\t44.7%\t-34.7%  illegals                      unborn                        \n",
      "43.1%\t-8.7%\t-51.8%  antitrust_laws                lesbian                       \n",
      "63.6%\t24.5%\t-39.1%  wall_street_reform            the_recovery_act              \n",
      "\n",
      "\n",
      "Different entities, different parties. Control group.\n",
      "75.3%\t37.7%\t-37.6%  taxes                         antitrust_laws                \n",
      "48.2%\t5.3%\t-42.9%  carbon                        guns                          \n",
      "63.9%\t17.5%\t-46.4%  abortion                      minimum_wage                  \n",
      "89.6%\t48.0%\t-41.6%  apple                         piano                         \n",
      "81.8%\t51.5%\t-30.4%  beef                          computer                      \n"
     ]
    }
   ],
   "source": [
    "cherry_pick(models['w2v'], models['0c'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Same entity, different parties.\n",
      "Removing connotation should increase similarity.\n",
      "Removing denotation should decrease similarity.\n",
      "88.3%\t92.7%\t+4.4%  estate_tax                    death_tax                     \n",
      "74.3%\t82.1%\t+7.8%  undocumented                  illegal_aliens                \n",
      "48.8%\t68.3%\t+19.5%  american_clean_energy         national_energy_tax           \n",
      "77.6%\t78.3%\t+0.7%  capandtax                     capandtrade_legislation       \n",
      "46.4%\t68.5%\t+22.1%  protection_and_affordable     obamacare                     \n",
      "77.1%\t83.7%\t+6.6%  the_health_care_reform        obamacare                     \n",
      "63.2%\t63.3%\t+0.0%  public_option                 socialized_medicine           \n",
      "16.2%\t41.4%\t+25.2%  guns                          second_amendment_rights       \n",
      "Out of vocabulary: washington_spending\n",
      "Out of vocabulary: star_wars_program\n",
      "Out of vocabulary: excessive_speculation\n",
      "Out of vocabulary: corporate_profit\n",
      "55.2%\t62.8%\t+7.6%  megabanks                     aig                           \n",
      "56.5%\t60.3%\t+3.8%  unemployment_insurance_benefitsstimulus                      \n",
      "Out of vocabulary: retroactive_immunity\n",
      "Out of vocabulary: give_tax_breaks\n",
      "\n",
      "\n",
      "Different entities, same party.\n",
      "Removing connotation should decresase similarity.\n",
      "Removing denotation should increase similarity.\n",
      "-15.8%\t9.2%\t+25.1%  tax_cuts                      right_to_life                 \n",
      "-43.8%\t10.8%\t+54.6%  new_entitlements              religious_freedom             \n",
      "44.7%\t63.6%\t+18.9%  illegals                      unborn                        \n",
      "-8.7%\t1.2%\t+9.9%  antitrust_laws                lesbian                       \n",
      "24.5%\t31.4%\t+6.9%  wall_street_reform            the_recovery_act              \n",
      "Out of vocabulary: nuclear_option\n",
      "Out of vocabulary: record_deficits\n",
      "Out of vocabulary: radical_jihadists\n",
      "\n",
      "\n",
      "Different entities, different parties. Control group.\n",
      "37.7%\t30.3%\t-7.4%  taxes                         antitrust_laws                \n",
      "5.3%\t7.9%\t+2.6%  carbon                        guns                          \n",
      "17.5%\t32.6%\t+15.1%  abortion                      minimum_wage                  \n",
      "48.0%\t78.4%\t+30.5%  apple                         piano                         \n",
      "51.5%\t50.7%\t-0.8%  beef                          computer                      \n"
     ]
    }
   ],
   "source": [
    "cherry_pick(models['0c'], models['-12c'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cherry_pick(models['-10c'], models['-12c'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# does not work\n",
    "cherry_pick(models['vanilla cono'], models['cono minus deno'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top Changes in Vector Spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_all_vocab(model1, model2, top_k=100, min_freq=100, max_freq=300):  \n",
    "    assert model1.id_to_word == model2.id_to_word\n",
    "    assert model1.Dem_frequency == model2.Dem_frequency\n",
    "    assert model1.GOP_frequency == model2.GOP_frequency\n",
    "    id_to_word = model1.id_to_word\n",
    "    freq: Counter[str] = model1.Dem_frequency + model1.GOP_frequency\n",
    "\n",
    "    # TODO select by indices\n",
    "    embed1 = np.array(\n",
    "        [vec.numpy() for word_id, vec in enumerate(model1.embedding) \n",
    "         if max_freq >= freq[id_to_word[word_id]] >= min_freq])\n",
    "    embed2 = np.array(\n",
    "        [vec.numpy() for word_id, vec in enumerate(model2.embedding) \n",
    "         if max_freq >= freq[id_to_word[word_id]] >= min_freq])    \n",
    "    print(f'min_freq = {min_freq}, filtered vocab size = {len(embed1):,}')\n",
    "    \n",
    "    sim1 = pairwise.cosine_similarity(embed1)\n",
    "    sim2 = pairwise.cosine_similarity(embed2)\n",
    "    sim1 = np.triu(sim1)\n",
    "    sim2 = np.triu(sim2)\n",
    "    sim_diff = sim1 - sim2\n",
    "    top_changes = np.argsort(sim_diff, axis=None)\n",
    "    \n",
    "    top_changed = []\n",
    "    for i, row in enumerate(sim_diff):\n",
    "        sorted_indices = np.argsort(row)\n",
    "        x = sorted_indices[:top_k]\n",
    "        y = sorted_indices[-top_k:]\n",
    "        both_extremes = np.hstack((x, y))\n",
    "        top_changed += [(sim_diff[i, j], id_to_word[i], id_to_word[j]) \n",
    "                        for j in both_extremes]\n",
    "            \n",
    "    top_changed.sort(key=lambda tup: tup[0], reverse=True)\n",
    "    return top_changed\n",
    "\n",
    "#     top_changed[:100]\n",
    "    \n",
    "#     top_changed.sort(key=lambda tup: tup[0], reverse=True)\n",
    "#     for sim_delta, x, y in top_changed:\n",
    "#         if 0.3 < sim_delta < 0.4:\n",
    "#             print(f'{sim_delta:.4f}  {x:<25}{y:<25}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stuff = compare_all_vocab(models['vanilla deno'], models['-10c'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stuff[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim1 = pairwise.cosine_similarity(models['deno minus cono'].embedding)\n",
    "sim2 = pairwise.cosine_similarity(models['vanilla deno'].embedding)\n",
    "# Filter > 100 freq\n",
    "sim1 = np.triu(sim1)\n",
    "sim2 = np.triu(sim2)\n",
    "sim_diff = sim1 - sim2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = models['vanilla deno'].id_to_word\n",
    "top_k = 5\n",
    "top_changed = []\n",
    "for i, row in enumerate(sim_diff):\n",
    "    sorted_indices = np.argsort(row)\n",
    "    x = sorted_indices[:top_k]\n",
    "    y = sorted_indices[-top_k:]\n",
    "    both_extremes = np.hstack((x, y))\n",
    "    top_changed += [(sim_diff[i, j], vocab[i], vocab[j]) \n",
    "                    for j in both_extremes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_changed.sort(key=lambda tup: tup[0], reverse=True)\n",
    "top_changed[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_changed.sort(key=lambda tup: tup[0], reverse=True)\n",
    "for sim_delta, x, y in top_changed:\n",
    "    if 0.3 < sim_delta < 0.4:\n",
    "        print(f'{sim_delta:.4f}  {x:<25}{y:<25}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
