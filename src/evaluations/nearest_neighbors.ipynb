{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "from typing import Set, Tuple, List, Dict\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import scipy\n",
    "from sklearn.metrics import pairwise\n",
    "from tqdm import tqdm\n",
    "\n",
    "from adversarial import AdversarialDecomposer, AdversarialConfig\n",
    "from preprocessing.S4_export_training_corpus import Document\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "class Embedding():\n",
    "    \n",
    "    def __init__(self, source: str, path: str):\n",
    "        if source == 'adversarial':\n",
    "            self.init_from_adversarial(path)\n",
    "        elif source == 'skip_gram':\n",
    "            self.init_from_skip_gram(path)\n",
    "        elif source == 'plain_text':\n",
    "            self.init_from_plain_text(path)\n",
    "        else:\n",
    "            raise ValueError('Unknown embedding source.')\n",
    "            \n",
    "    def init_from_adversarial(self, path: str, device=torch.device('cpu')):\n",
    "        payload = torch.load(path, map_location=device)\n",
    "        model = payload['model']\n",
    "        self.word_to_id = model.word_to_id\n",
    "        self.id_to_word = model.id_to_word \n",
    "        # encoded layer\n",
    "        self.embedding = model.export_decomposed_embedding(device=device)\n",
    "        \n",
    "        # manually choose which layer to export\n",
    "#         all_vocab_ids = torch.arange(\n",
    "#             len(self.word_to_id), dtype=torch.long, device=device)\n",
    "#         with torch.no_grad():\n",
    "#             self.embedding = model.center_decoder(model.encoder_forward(all_vocab_ids))\n",
    "            \n",
    "#             self.embedding = self.encoder_forward(all_vocab_ids)\n",
    "#             self.embedding = model.denotation_decoder(model.encoder_forward(all_vocab_ids))\n",
    "#             self.embeddings = model.context_embedding(all_vocab_ids)  # galaxy brain\n",
    "            \n",
    "#     def init_from_adversarial(self, path: str):        \n",
    "#         config = DenotationEncoderConfig()\n",
    "#         config.input_dir = '../../data/processed/adversarial/44_Obama_1e-5'\n",
    "#         data = AdversarialDataset(config)\n",
    "#         model = DenotationEncoder(config, data)\n",
    "#         model.load_state_dict(torch.load(path))\n",
    "#         self.embedding = model.export_decomposed_embedding().to('cpu')\n",
    "#         self.word_to_id = model.word_to_id\n",
    "#         self.id_to_word = model.id_to_word\n",
    "\n",
    "    def init_from_skip_gram(self, paths: Tuple[str, str]) -> None:\n",
    "        \"\"\"Directly extract the weights of a single layer.\"\"\"\n",
    "        model_path, vocab_path = paths\n",
    "        with open(model_path, 'rb') as model_file:\n",
    "            state_dict = torch.load(model_file, map_location='cpu')\n",
    "    #     print(state_dict.keys())\n",
    "        self.embedding = state_dict['center_embedding.weight'].numpy()\n",
    "        with open(vocab_path, 'rb') as vocab_file:\n",
    "            self.word_to_id, self.id_to_word, _ = pickle.load(vocab_file)\n",
    "\n",
    "    def init_from_plain_text(self, path: str) -> Tuple[np.array, Dict[str, int]]:\n",
    "        id_generator = 0\n",
    "        word_to_id: Dict[str, int] = {}\n",
    "        embeddings: List[float] = []\n",
    "        embedding_file = open(path)\n",
    "        vocab_size, num_dimensions = map(int, embedding_file.readline().split())\n",
    "        print(f'vocab_size = {vocab_size:,}, num_dimensions = {num_dimensions}')\n",
    "        print(f'Loading embeddings from {path}', flush=True)\n",
    "        for line in embedding_file:\n",
    "            line: List[str] = line.split()  # type: ignore\n",
    "            word = line[0]\n",
    "            vector = np.array(line[-num_dimensions:], dtype=np.float64)\n",
    "            embeddings.append(vector)\n",
    "            word_to_id[word] = id_generator\n",
    "            id_generator += 1\n",
    "        embedding_file.close()\n",
    "        print('Done')\n",
    "        self.id_to_word = {val: key for key, val in word_to_id.items()}\n",
    "        self.word_to_id = word_to_id\n",
    "        self.embedding = np.array(embeddings)\n",
    "        \n",
    "    def write_to_tensorboard_projector(self, tb_dir: str) -> None:\n",
    "        from torch.utils import tensorboard\n",
    "        tb = tensorboard.SummaryWriter(log_dir=tb_dir)\n",
    "        all_vocab_ids = range(len(self.word_to_id))\n",
    "        embedding_labels = [\n",
    "            self.id_to_word[word_id]\n",
    "            for word_id in all_vocab_ids]\n",
    "        tb.add_embedding(\n",
    "            self.embedding[:9999], \n",
    "            embedding_labels[:9999], \n",
    "            global_step=0)\n",
    "        \n",
    "    def export_web_projector(self, out_dir: str) -> None:\n",
    "        random_indices = np.random.randint(len(self.embedding), size=10000)\n",
    "        subset_embedding = self.embedding[random_indices].tolist()\n",
    "        \n",
    "        vector_path = os.path.join(out_dir, 'tensorboard.tsv')\n",
    "        with open(vector_path, 'w') as vector_file:\n",
    "            for vector in subset_embedding:\n",
    "                vector_file.write('\\t'.join(map(str, vector)) + '\\n')\n",
    "\n",
    "        label_path = os.path.join(out_dir, 'tensorboard_labels.tsv')\n",
    "        with open(label_path, 'w') as label_file:\n",
    "            for index in random_indices:\n",
    "                label_file.write(self.id_to_word[index] + '\\n')\n",
    "\n",
    "    def cosine_similarity(self, query1: str, query2: str) -> float:\n",
    "        try:\n",
    "            query1_id = self.word_to_id[query1]\n",
    "        except KeyError as error:\n",
    "            print(f'{query1} is out of vocabulary. Sorry!')\n",
    "            raise error\n",
    "        try:\n",
    "            query2_id = self.word_to_id[query2]\n",
    "        except KeyError as error:\n",
    "            print(f'{query2} is out of vocabulary. Sorry!')\n",
    "            raise error\n",
    "        vectors = self.embedding[(query1_id, query2_id), :]\n",
    "        similarity = 1 - scipy.spatial.distance.cosine(vectors[0], vectors[1])\n",
    "        return similarity\n",
    "\n",
    "    def nearest_neighbor(self, query: str, top_k: int = 10):\n",
    "        try:\n",
    "            query_id = self.word_to_id[query]\n",
    "        except KeyError:\n",
    "            raise KeyError(f'{query} is out of vocabulary. Sorry!')    \n",
    "        query_vec = self.embedding[query_id]\n",
    "        \n",
    "        distances = [scipy.spatial.distance.cosine(query_vec, vec) \n",
    "                     for vec in self.embedding]\n",
    "        neighbors = np.argsort(distances)\n",
    "        print(f\"{query}'s neareset neighbors:\")\n",
    "        for ranking in range(1, top_k + 1):\n",
    "            word_id = neighbors[ranking]\n",
    "            word = self.id_to_word[word_id]\n",
    "            cosine_similarity = 1 - distances[word_id]\n",
    "            print(f'{cosine_similarity:.4f}\\t{word}')\n",
    "        print()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size = 34,100, num_dimensions = 300\n",
      "Loading embeddings from ../../results/baseline/word2vec_Obama.txt\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "models['w2v'] = Embedding(  # comparable to d1c0\n",
    "    'plain_text', \n",
    "    '../../results/baseline/word2vec_Obama.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PReLU\n",
    "base_dir = '../../results/adversarial/PReLU/'\n",
    "\n",
    "models['vanilla deno'] = Embedding(\n",
    "    'adversarial', base_dir + '1d0c w2vTi SE/epoch50.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear-ReLU-linear encoder, frozen uniform init\n",
    "base_dir = '../../results/adversarial/MLP encoder/'\n",
    "\n",
    "models['vanilla deno'] = Embedding(\n",
    "    'adversarial', base_dir + '1d0c/epoch50.pt')\n",
    "\n",
    "models['deno minus cono'] = Embedding(\n",
    "    'adversarial', base_dir + '1d-1c/epoch15.pt')  # need more epochs\n",
    "\n",
    "models['vanilla cono'] = Embedding(\n",
    "    'adversarial', base_dir + '0d1c/epoch50.pt')\n",
    "\n",
    "# models['cono minus deno'] = Embedding(\n",
    "#     'adversarial', "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear-ReLU-linear encoder, frozen uniform init, adversary-free context, no gradient clipping\n",
    "base_dir = '../../results/adversarial/context sans adversary/'\n",
    "\n",
    "models['vanilla deno'] = Embedding(\n",
    "    'adversarial', base_dir + '1d0c bs1/epoch100.pt')\n",
    "\n",
    "# models['deno minus cono'] = Embedding(\n",
    "#     'adversarial', \n",
    "\n",
    "models['vanilla cono'] = Embedding(\n",
    "    'adversarial', base_dir + '0d1c/epoch30.pt')\n",
    "\n",
    "# models['cono minus deno'] = Embedding(\n",
    "#     'adversarial', "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear-ReLU encoder, frozen uniform init\n",
    "\n",
    "models['vanilla deno'] = Embedding(\n",
    "    'adversarial', \n",
    "    '../../results/adversarial/nonlinear encoder/d1c0/epoch45.pt')\n",
    "\n",
    "models['deno minus cono'] = Embedding(\n",
    "    'adversarial', \n",
    "    '../../results/adversarial/nonlinear encoder/d1c-1_w2v/epoch50.pt')  # inconsistent init!\n",
    "\n",
    "models['vanilla cono'] = Embedding(\n",
    "    'adversarial', \n",
    "    '../../results/adversarial/nonlinear encoder/d0c1/epoch25.pt')\n",
    "\n",
    "models['cono mius deno'] = Embedding(\n",
    "    'adversarial', \n",
    "    '../../results/adversarial/nonlinear encoder/d-0.1c1/epoch50.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models['cono'].write_to_tensorboard_projector(\n",
    "#     '../../results/adversarial/Obama/p8_.55to.75/d0_c1/embedding_projector')\n",
    "# models['cono'].export_web_tensorboard('../../results/adversarial/Obama/p8_.55to.75/d0_c1/web_projector')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estate_tax's neareset neighbors:\n",
      "0.8805\testates\n",
      "0.7583\tdeath_tax\n",
      "0.7486\testate_taxes\n",
      "0.7481\testate\n",
      "0.7041\tamt\n",
      "0.6804\theirs\n",
      "0.6655\talternative_minimum_tax\n",
      "0.6571\tbush_tax\n",
      "0.6538\tbush_tax_cuts\n",
      "0.6530\tcapital_gains\n",
      "\n",
      "death_tax's neareset neighbors:\n",
      "0.7616\ttax_rates\n",
      "0.7583\testate_tax\n",
      "0.7534\testates\n",
      "0.7532\tcapital_gains_tax\n",
      "0.7467\tcapital_gains\n",
      "0.7407\testate_taxes\n",
      "0.7394\ttax_rate\n",
      "0.7002\tpercent_tax\n",
      "0.6898\tbracket\n",
      "0.6872\tdeath_taxes\n",
      "\n",
      "undocumented's neareset neighbors:\n",
      "0.6868\tillegal_aliens\n",
      "0.6839\timmigrants\n",
      "0.6820\tillegally\n",
      "0.6783\tdream_act\n",
      "0.6779\tthe_dream_act_would\n",
      "0.6729\tillegal_immigrants\n",
      "0.6683\tthe_dream_act\n",
      "0.6658\taliens\n",
      "0.6598\tlegalization\n",
      "0.6587\timmigration\n",
      "\n",
      "illegal_aliens's neareset neighbors:\n",
      "0.8860\tillegal_immigrants\n",
      "0.8162\tillegals\n",
      "0.7577\taliens\n",
      "0.7467\tnoncitizens\n",
      "0.7421\ttaxpayersubsidized\n",
      "0.7353\tlegalize\n",
      "0.7201\tillegal_immigration\n",
      "0.7131\tillegally\n",
      "0.7110\thealth_insurance_programs\n",
      "0.6922\tsecure_the_border\n",
      "\n",
      "music's neareset neighbors:\n",
      "0.8239\tgospel\n",
      "0.7362\tmusical\n",
      "0.6889\tjazz\n",
      "0.6715\tsongs\n",
      "0.6705\tbluegrass\n",
      "0.6617\tblues\n",
      "0.6427\tflatt\n",
      "0.6269\tartist\n",
      "0.6234\thymns\n",
      "0.6228\tmusicians\n",
      "\n",
      "language's neareset neighbors:\n",
      "0.6314\tprovision\n",
      "0.6253\trestates\n",
      "0.6252\twording\n",
      "0.6232\tthe_nelsonhatchcasey_amendment\n",
      "0.6153\tambiguous\n",
      "0.5998\tthe_original_amendment\n",
      "0.5982\tamendmentwhich\n",
      "0.5952\tthe_capps_amendment\n",
      "0.5952\tthe_base_bill\n",
      "0.5930\tthe_current_bill\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = models['w2v']\n",
    "cherries = [\n",
    "    'estate_tax', 'death_tax', \n",
    "    'undocumented', 'illegal_aliens', \n",
    "    'music', 'language']\n",
    "for cherry in cherries:\n",
    "    model.nearest_neighbor(cherry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cherry_pick(model):\n",
    "    \n",
    "    def print_similarities(pairs):\n",
    "        for word1, word2 in pairs:\n",
    "            try: \n",
    "                print(f'{model.cosine_similarity(word1, word2):.4f}  '\n",
    "                      f'{word1:<20}{word2:<20}')\n",
    "            except KeyError:\n",
    "                pass\n",
    "        \n",
    "    print('Same entity, different parties. Removing connotation should increase similarity:')\n",
    "    cherries = [\n",
    "        ('estate_tax', 'death_tax'), \n",
    "        ('undocumented', 'illegal_aliens'),\n",
    "        ('obamacare', 'protection_and_affordable'),\n",
    "        ('socialized_medicine', 'public_option'),\n",
    "        ('second_amendment_rights', 'guns'), \n",
    "    #     'health_care_bill', ',\n",
    "    #     'the_wall_street_reform_legislation', 'financial_stability', 'capital_gains_tax',\n",
    "    #     'deficit_spending', 'bush_tax_cuts'\n",
    "    ]    \n",
    "    print_similarities(cherries)\n",
    "     \n",
    "    print('\\n\\nDifferent entities, same party. Removing denotation should increase similarity')\n",
    "    ideologies = [\n",
    "        ('tax_cuts', 'entitlement_reform'),\n",
    "        ('religious_freedom', 'right_to_life')\n",
    "    ]\n",
    "    print_similarities(ideologies)\n",
    "    \n",
    "    print('\\n\\nDifferent entities, different parties. Removing connotation should not increase similarity:')\n",
    "    controls = [\n",
    "        ('taxes', 'antitrust_laws'),\n",
    "        ('carbon', 'guns'),\n",
    "        ('apple', 'piano'),\n",
    "        ('beef', 'burger')\n",
    "    ]\n",
    "    print_similarities(controls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cherry_pick(models['w2v'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cherry_pick(models['vanilla deno'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cherry_pick(models['deno minus cono'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
