{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/webson/Research/congressional_adversary/congressional_env/lib/python3.7/site-packages/torch/serialization.py:593: SourceChangeWarning: source code of class 'decomposer.Decomposer' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size = 138,442\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from typing import Tuple, Union, List, Dict, Iterable, Optional\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from decomposer import Decomposer, DecomposerConfig\n",
    "from recomposer import Recomposer, RecomposerConfig\n",
    "from helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f2f46da23dc49e8b12f49422b65a25f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ../../results/news/validation_transformer/-3c/epoch2.pt\n",
      "Loading ../../results/news/validation_transformer/debug cono only/epoch2.pt\n",
      "\n",
      "dict_keys(['-3c/epoch2.pt', 'debug cono only/epoch2.pt'])\n"
     ]
    }
   ],
   "source": [
    "# base_dir = Path('../../results/news/validation')\n",
    "base_dir = Path('../../results/news/validation_transformer')\n",
    "deno_space = load_decomposers_en_masse(base_dir, patterns='*/epoch2.pt')\n",
    "# deno_space.update(load_decomposers_en_masse(base_dir, patterns='*/epoch3.pt'))\n",
    "\n",
    "print(deno_space.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "             Dataset             Rho       Reference      Delta >.02\n",
      "====================================================================================================\n",
      "    EN-MTurk-771.txt         49.32%         52.84%          -3.52%\n",
      "  EN-RW-STANFORD.txt         44.11%         45.33%               ðœ€\n",
      "   EN-SIMLEX-999.txt         32.59%         32.92%               ðœ€\n",
      "    EN-MTurk-287.txt         47.04%         59.11%         -12.07%\n",
      "       EN-YP-130.txt         47.69%         50.72%          -3.03%\n",
      "   EN-WS-353-SIM.txt         66.36%         68.51%          -2.16%\n",
      "    EN-MEN-TR-3k.txt         56.20%         52.49%           3.71%\n",
      "   EN-WS-353-REL.txt         39.53%         44.70%          -5.17%\n",
      "     EN-VERB-143.txt         20.39%         29.87%          -9.48%\n",
      "   EN-WS-353-ALL.txt         52.24%         58.16%          -5.92%\n",
      " EN-SimVerb-3500.txt         25.62%         26.25%               ðœ€\n",
      "Mean Delta = -3.6195%\n"
     ]
    }
   ],
   "source": [
    "from evaluations.word_similarity.all_wordsim import mean_delta, read_plain_text, compare\n",
    "import math\n",
    "\n",
    "def convert(embed):\n",
    "    word_vecs: Dict[str, np.array] = {}\n",
    "    for word_id, vector in enumerate(embed):\n",
    "        vector /= math.sqrt((vector ** 2).sum() + 1e-6)\n",
    "        word_vecs[ITW[word_id]] = vector\n",
    "    return word_vecs\n",
    "\n",
    "\n",
    "def cf_WS(m1, m2):\n",
    "    compare(convert(m1), convert(m2))\n",
    "\n",
    "cf_WS(deno_space['-3c/epoch2.pt'], PE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cf(query):\n",
    "    GD(query)\n",
    "    print('Pretrained:')\n",
    "    nearest_neighbors(query, sup_PE)\n",
    "    print('Our model:')\n",
    "    nearest_neighbors(query, deno_space['-3c/epoch2.pt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf('estate_tax')\n",
    "cf('death_tax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf('undocumented_workers')\n",
    "cf('illegal_aliens')\n",
    "cf('chain_migration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf('obamacare')\n",
    "cf('aca')\n",
    "cf('public_option')\n",
    "cf('single_payer')\n",
    "cf('socialized_medicine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf('leftists')\n",
    "cf('antifa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf('guns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf('trade')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf('crooked_hillary')\n",
    "cf('crazy_bernie')\n",
    "cf('lyin_ted')\n",
    "cf('little_marco')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf('lgbt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "sns.set()\n",
    "\n",
    "def plot(\n",
    "        coordinates: np.ndarray,\n",
    "        words: List[GroundedWord],\n",
    "        path: Path\n",
    "        ) -> None:\n",
    "    fig, ax = plt.subplots(figsize=(15, 10))\n",
    "#     skew = [w.R_ratio for w in words]\n",
    "#     freq = [w.freq for w in words]\n",
    "    sns.scatterplot(\n",
    "        coordinates[:, 0], coordinates[:, 1],\n",
    "#         hue=skew, palette='coolwarm',  # hue_norm=(0, 1),\n",
    "#         size=freq, sizes=(200, 1000),\n",
    "        legend=None, ax=ax)\n",
    "    for coord, w in zip(coordinates, words):\n",
    "        ax.annotate(w.word, coord, fontsize=20)\n",
    "    with open(path, 'wb') as file:\n",
    "        fig.savefig(file, dpi=300)\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def plot_categorical(\n",
    "        coordinates: np.ndarray,\n",
    "        words: List[GroundedWord],\n",
    "        path: Path,\n",
    "        fancy: bool = True\n",
    "        ) -> None:\n",
    "    if fancy:\n",
    "        fig, ax = plt.subplots(figsize=(20, 10))\n",
    "        categories = [w.majority_deno for w in words]\n",
    "        freq = [w.freq for w in words]\n",
    "        sns.scatterplot(\n",
    "            coordinates[:, 0], coordinates[:, 1],\n",
    "            hue=categories, palette='muted', hue_norm=(0, 1),\n",
    "            size=freq, sizes=(200, 1000),\n",
    "            legend='brief', \n",
    "            ax=ax)\n",
    "        chartBox = ax.get_position()\n",
    "        ax.set_position(  # adjust legend\n",
    "            [chartBox.x0, chartBox.y0, chartBox.width * 0.6, chartBox.height])\n",
    "        ax.legend(loc='upper center', bbox_to_anchor=(1.45, 0.8), ncol=1)\n",
    "    else:\n",
    "        fig, ax = plt.subplots(figsize=(20, 10))\n",
    "        freq = [w.freq for w in words]\n",
    "        sns.scatterplot(\n",
    "            coordinates[:, 0], coordinates[:, 1], ax=ax)\n",
    "\n",
    "    for coord, w in zip(coordinates, words):\n",
    "        ax.annotate(w.word, coord, fontsize=12)\n",
    "    with open(path, 'wb') as file:\n",
    "        fig.savefig(file, dpi=300)\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def graph_en_masse(\n",
    "        models: Dict[str, np.ndarray],\n",
    "        out_dir: Path,\n",
    "        reduction: str,  # 'PCA', 'TSNE', or 'both'\n",
    "        words: List[GroundedWord],\n",
    "        # hues: Union[List[float], List[int]],\n",
    "        # sizes: List[int],\n",
    "        perplexity: Optional[int] = None,\n",
    "        categorical: bool = False\n",
    "        ) -> None:\n",
    "    Path.mkdir(out_dir, parents=True, exist_ok=True)\n",
    "    word_ids = np.array([w.word_id for w in words])\n",
    "    for model_name, embed in tqdm(models.items()):\n",
    "        space = embed[word_ids]\n",
    "        if reduction == 'PCA':\n",
    "            visual = PCA(n_components=2).fit_transform(space)\n",
    "        elif reduction == 'TSNE':\n",
    "            assert perplexity is not None\n",
    "            visual = TSNE(\n",
    "                perplexity=perplexity, learning_rate=10,\n",
    "                n_iter=5000, n_iter_without_progress=1000).fit_transform(space)\n",
    "        elif reduction == 'both':\n",
    "            assert perplexity is not None\n",
    "            space = PCA(n_components=30).fit_transform(space)\n",
    "            visual = TSNE(\n",
    "                perplexity=perplexity, learning_rate=10,\n",
    "                n_iter=5000, n_iter_without_progress=1000).fit_transform(space)\n",
    "        else:\n",
    "            raise ValueError('unknown dimension reduction method')\n",
    "        if categorical:\n",
    "            plot_categorical(visual, words, out_dir / f'{model_name}.png')\n",
    "        else:\n",
    "            plot(visual, words, out_dir / f'{model_name}.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cherry_words = [\n",
    "    'government', 'washington',\n",
    "    'estate_tax', 'death_tax',\n",
    "    'public_option', 'governmentrun',\n",
    "    'foreign_trade', 'international_trade',\n",
    "    'cut_taxes', 'trickledown'\n",
    "]\n",
    "\n",
    "cherry_words = [GroundedWord(w) for w in cherry_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "models = deno_space\n",
    "stuff = polarization\n",
    "\n",
    "graph_en_masse(\n",
    "    models, out_dir=base_dir / 'cherry/topic/t-SNE p5',\n",
    "    reduction='TSNE', perplexity=5, words=stuff, categorical=True)\n",
    "graph_en_masse(\n",
    "    models, out_dir=base_dir / 'cherry/topic/t-SNE p3',\n",
    "    reduction='TSNE', perplexity=3, words=stuff, categorical=True)\n",
    "graph_en_masse(\n",
    "    models, out_dir=base_dir / 'cherry/topic/t-SNE p2',\n",
    "    reduction='TSNE', perplexity=2, words=stuff, categorical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = cono_space\n",
    "\n",
    "graph_en_masse(\n",
    "    models, out_dir=base_dir / 'cherry/party/t-SNE p5',\n",
    "    reduction='TSNE', perplexity=5, words=cherry_words, categorical=False)\n",
    "\n",
    "graph_en_masse(\n",
    "    models, out_dir=base_dir / 'cherry/party/t-SNE p3',\n",
    "    reduction='TSNE', perplexity=3, words=cherry_words, categorical=False)\n",
    "\n",
    "graph_en_masse(\n",
    "    models, out_dir=base_dir / 'cherry/party/t-SNE p2',\n",
    "    reduction='TSNE', perplexity=2, words=cherry_words, categorical=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = deno_space\n",
    "\n",
    "graph_en_masse(\n",
    "    models,\n",
    "    out_dir=f'{base_dir}/decomposed deno/party/t-SNE p25',\n",
    "    reduction='TSNE', perplexity=25,\n",
    "    word_ids=J_ids, words=J_words, hues=J_skew, sizes=J_freq)\n",
    "\n",
    "graph_en_masse(\n",
    "    models,\n",
    "    out_dir=f'{base_dir}/decomposed deno/party/t-SNE p50',\n",
    "    reduction='TSNE', perplexity=50,\n",
    "    word_ids=J_ids, words=J_words, hues=J_skew, sizes=J_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = cono_space\n",
    "\n",
    "graph_en_masse(\n",
    "    models, out_dir=f'{base_dir}/decomposed cono/topic/t-SNE p5',\n",
    "    reduction='TSNE', perplexity=5,\n",
    "    word_ids=J_ids, words=J_words, hues=J_deno, sizes=J_freq,\n",
    "    categorical=True)\n",
    "\n",
    "graph_en_masse(\n",
    "    models,\n",
    "    out_dir=f'{base_dir}/decomposed cono/topic/t-SNE p3',\n",
    "    reduction='TSNE', perplexity=3,\n",
    "    word_ids=J_ids, words=J_words, hues=J_deno, sizes=J_freq,\n",
    "    categorical=True)\n",
    "\n",
    "graph_en_masse(\n",
    "    models,\n",
    "    out_dir=f'{base_dir}/decomposed cono/topic/t-SNE p10',\n",
    "    reduction='TSNE', perplexity=10,\n",
    "    word_ids=J_ids, words=J_words, hues=J_deno, sizes=J_freq,\n",
    "    categorical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = cono_space\n",
    "\n",
    "# graph_en_masse(\n",
    "#     models,\n",
    "#     out_dir=f'{base_dir}/Joint/topic/PCA',\n",
    "#     reduction='PCA',\n",
    "#     word_ids=J_ids, words=J_words, hues=J_skew, sizes=J_freq)\n",
    "\n",
    "graph_en_masse(\n",
    "    models,\n",
    "    out_dir=f'{base_dir}/decomposed cono/party/t-SNE p25',\n",
    "    reduction='TSNE', perplexity=25,\n",
    "    word_ids=J_ids, words=J_words, hues=J_skew, sizes=J_freq)\n",
    "\n",
    "graph_en_masse(\n",
    "    models,\n",
    "    out_dir=f'{base_dir}/decomposed cono/party/t-SNE p50',\n",
    "    reduction='TSNE', perplexity=50,\n",
    "    word_ids=J_ids, words=J_words, hues=J_skew, sizes=J_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homogeneity V-Measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deno space, eval deno, higher is better\n",
    "for model_name, model in deno_space.items():\n",
    "    cluster_labels, true_labels = NN_cluster_ids(\n",
    "        model, J_ids, categorical=True, top_k=10)    \n",
    "    homogeneity, completeness, v_measure = np.around(\n",
    "        homogeneity_completeness_v_measure(true_labels, cluster_labels), 4)\n",
    "    print(model_name, homogeneity, completeness, v_measure, sep='\\t')\n",
    "#     print(pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deno space, eval cono, lower is better\n",
    "for model_name, model in deno_space.items():\n",
    "    cluster_labels, true_labels = NN_cluster_ids(\n",
    "        model, J_ids, categorical=False, top_k=5)    \n",
    "    homogeneity, completeness, v_measure = np.around(\n",
    "        homogeneity_completeness_v_measure(true_labels, cluster_labels), 4)\n",
    "    print(model_name, homogeneity, completeness, v_measure, sep='\\t')\n",
    "#     print(pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cono space, eval cono, higher is better\n",
    "for model_name, model in cono_space.items():\n",
    "    cluster_labels, true_labels = NN_cluster_ids(\n",
    "        model, J_ids, categorical=False, top_k=5)    \n",
    "    homogeneity, completeness, v_measure = np.around(\n",
    "        homogeneity_completeness_v_measure(true_labels, cluster_labels), 4)\n",
    "    print(model_name, homogeneity, completeness, v_measure, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cono space, eval deno, lower is better\n",
    "for model_name, model in cono_space.items():\n",
    "    cluster_labels, true_labels = NN_cluster_ids(\n",
    "        model, J_ids, categorical=True, top_k=5)    \n",
    "    homogeneity, completeness, v_measure = np.around(\n",
    "        homogeneity_completeness_v_measure(true_labels, cluster_labels), 4)\n",
    "    print(model_name, homogeneity, completeness, v_measure, sep='\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
