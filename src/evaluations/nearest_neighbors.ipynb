{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import csv\n",
    "import os\n",
    "from typing import Set, Tuple, List, Dict, Counter\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import scipy\n",
    "from sklearn.metrics import pairwise\n",
    "from tqdm import tqdm\n",
    "\n",
    "from adversarial import AdversarialDecomposer, AdversarialConfig\n",
    "# from preprocessing.S4_export_training_corpus import Document\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "class Embedding():\n",
    "    \n",
    "    def __init__(self, source: str, path: str):\n",
    "        if source == 'adversarial':\n",
    "            self.init_from_adversarial(path)\n",
    "        elif source == 'skip_gram':\n",
    "            self.init_from_skip_gram(path)\n",
    "        elif source == 'plain_text':\n",
    "            self.init_from_plain_text(path)\n",
    "        else:\n",
    "            raise ValueError('Unknown embedding source.')\n",
    "            \n",
    "    def init_from_adversarial(self, path: str, device=torch.device('cpu')):\n",
    "        payload = torch.load(path, map_location=device)\n",
    "        model = payload['model']\n",
    "        self.word_to_id = model.word_to_id\n",
    "        self.id_to_word = model.id_to_word \n",
    "        self.Dem_frequency: Counter[str] = model.Dem_frequency\n",
    "        self.GOP_frequency: Counter[str] = model.GOP_frequency\n",
    "        \n",
    "        # encoded layer\n",
    "        self.embedding = model.export_encoded_embedding(device=device)\n",
    "#         self.embedding = model.export_decomposed_embedding(device=device)\n",
    "\n",
    "#         # manually choose which layer to export\n",
    "#         all_vocab_ids = torch.arange(\n",
    "#             len(self.word_to_id), dtype=torch.long, device=device)\n",
    "#         with torch.no_grad():\n",
    "#             embed = model.embedding(all_vocab_ids)\n",
    "#             encoded = model.encoder(embed)\n",
    "#             self.cono_logits = model.cono_decoder(encoded)\n",
    "            \n",
    "#     def init_from_adversarial(self, path: str):        \n",
    "#         config = DenotationEncoderConfig()\n",
    "#         config.input_dir = '../../data/processed/adversarial/44_Obama_1e-5'\n",
    "#         data = AdversarialDataset(config)\n",
    "#         model = DenotationEncoder(config, data)\n",
    "#         model.load_state_dict(torch.load(path))\n",
    "#         self.embedding = model.export_decomposed_embedding().to('cpu')\n",
    "#         self.word_to_id = model.word_to_id\n",
    "#         self.id_to_word = model.id_to_word\n",
    "\n",
    "    def init_from_skip_gram(self, paths: Tuple[str, str]) -> None:\n",
    "        \"\"\"Directly extract the weights of a single layer.\"\"\"\n",
    "        model_path, vocab_path = paths\n",
    "        with open(model_path, 'rb') as model_file:\n",
    "            state_dict = torch.load(model_file, map_location='cpu')\n",
    "    #     print(state_dict.keys())\n",
    "        self.embedding = state_dict['center_embedding.weight'].numpy()\n",
    "        with open(vocab_path, 'rb') as vocab_file:\n",
    "            self.word_to_id, self.id_to_word, _ = pickle.load(vocab_file)\n",
    "\n",
    "    def init_from_plain_text(self, path: str) -> Tuple[np.array, Dict[str, int]]:\n",
    "        id_generator = 0\n",
    "        word_to_id: Dict[str, int] = {}\n",
    "        embeddings: List[float] = []\n",
    "        embedding_file = open(path)\n",
    "        vocab_size, num_dimensions = map(int, embedding_file.readline().split())\n",
    "        print(f'vocab_size = {vocab_size:,}, num_dimensions = {num_dimensions}')\n",
    "        print(f'Loading embeddings from {path}', flush=True)\n",
    "        for line in embedding_file:\n",
    "            line: List[str] = line.split()  # type: ignore\n",
    "            word = line[0]\n",
    "            vector = np.array(line[-num_dimensions:], dtype=np.float64)\n",
    "            embeddings.append(vector)\n",
    "            word_to_id[word] = id_generator\n",
    "            id_generator += 1\n",
    "        embedding_file.close()\n",
    "        print('Done')\n",
    "        self.id_to_word = {val: key for key, val in word_to_id.items()}\n",
    "        self.word_to_id = word_to_id\n",
    "        self.embedding = np.array(embeddings)\n",
    "        \n",
    "    def write_to_tensorboard_projector(self, tb_dir: str) -> None:\n",
    "        from torch.utils import tensorboard\n",
    "        tb = tensorboard.SummaryWriter(log_dir=tb_dir)\n",
    "        all_vocab_ids = range(len(self.word_to_id))\n",
    "        embedding_labels = [\n",
    "            self.id_to_word[word_id]\n",
    "            for word_id in all_vocab_ids]\n",
    "        tb.add_embedding(\n",
    "            self.embedding[:9999], \n",
    "            embedding_labels[:9999], \n",
    "            global_step=0)\n",
    "        \n",
    "    def export_web_projector(self, out_dir: str) -> None:\n",
    "        random_indices = np.random.randint(len(self.embedding), size=10000)\n",
    "        subset_embedding = self.embedding[random_indices].tolist()\n",
    "        \n",
    "        vector_path = os.path.join(out_dir, 'tensorboard.tsv')\n",
    "        with open(vector_path, 'w') as vector_file:\n",
    "            for vector in subset_embedding:\n",
    "                vector_file.write('\\t'.join(map(str, vector)) + '\\n')\n",
    "\n",
    "        label_path = os.path.join(out_dir, 'tensorboard_labels.tsv')\n",
    "        with open(label_path, 'w') as label_file:\n",
    "            for index in random_indices:\n",
    "                label_file.write(self.id_to_word[index] + '\\n')\n",
    "\n",
    "    def cosine_similarity(self, query1: str, query2: str) -> float:\n",
    "        try:\n",
    "            query1_id = self.word_to_id[query1]\n",
    "        except KeyError as error:\n",
    "            print(f'Out of vocabulary: {query1}')\n",
    "            raise error\n",
    "        try:\n",
    "            query2_id = self.word_to_id[query2]\n",
    "        except KeyError as error:\n",
    "            print(f'Out of vocabulary: {query2}')\n",
    "            raise error\n",
    "        vectors = self.embedding[(query1_id, query2_id), :]\n",
    "        similarity = 1 - scipy.spatial.distance.cosine(vectors[0], vectors[1])\n",
    "        return similarity\n",
    "\n",
    "    def nearest_neighbor(self, query: str, top_k: int = 10):\n",
    "        try:\n",
    "            query_id = self.word_to_id[query]\n",
    "        except KeyError:\n",
    "            raise KeyError(f'{query} is out of vocabulary. Sorry!')    \n",
    "        query_vec = self.embedding[query_id]\n",
    "        \n",
    "        distances = [scipy.spatial.distance.cosine(query_vec, vec) \n",
    "                     for vec in self.embedding]\n",
    "        neighbors = np.argsort(distances)\n",
    "        print(f\"{query}'s neareset neighbors:\")\n",
    "        for ranking in range(1, top_k + 1):\n",
    "            word_id = neighbors[ranking]\n",
    "            word = self.id_to_word[word_id]\n",
    "            cosine_similarity = 1 - distances[word_id]\n",
    "            print(f'{cosine_similarity:.4f}\\t{word}')\n",
    "        print()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models['cono'].write_to_tensorboard_projector(\n",
    "#     '../../results/adversarial/Obama/p8_.55to.75/d0_c1/embedding_projector')\n",
    "# models['cono'].export_web_tensorboard('../../results/adversarial/Obama/p8_.55to.75/d0_c1/web_projector')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models['-10c']\n",
    "cherries = [\n",
    "    'estate_tax', 'death_tax', \n",
    "    'undocumented', 'illegal_aliens', \n",
    "    'music', 'language']\n",
    "for cherry in cherries:\n",
    "    model.nearest_neighbor(cherry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cherry-Picking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "with open('GOP_sample.tsv') as file:\n",
    "    reader = csv.DictReader(file, dialect=csv.excel_tab)\n",
    "    for row in reader:\n",
    "        data.append(row)\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_data = [\n",
    "    obs for obs in data \n",
    "    if obs['same_denotation'] or obs['same_connotation']]\n",
    "print(len(labeled_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cherry_pick(model1, model2):\n",
    "    \"\"\"prints difference := model2 - model1\"\"\"   \n",
    "#     def print_similarities(pairs):\n",
    "#         for word1, word2 in pairs:\n",
    "#             try: \n",
    "#                 print(f'{model.cosine_similarity(word1, word2):.4f}  '\n",
    "#                       f'{word1:<30}{word2:<30}')\n",
    "#             except KeyError:\n",
    "#                 pass\n",
    "    \n",
    "    def compare_similarities(pairs):\n",
    "        deltas = []\n",
    "        for word1, word2 in pairs:\n",
    "            try:\n",
    "                sim1 = model1.cosine_similarity(word1, word2)\n",
    "                sim2 = model2.cosine_similarity(word1, word2)\n",
    "            except KeyError:\n",
    "                continue\n",
    "            delta = sim2 - sim1\n",
    "            deltas.append(delta)\n",
    "            print(f'{sim1:.1%}\\t{sim2:.1%}\\t{delta:+.1%}  '\n",
    "                  f'{word1:<30}{word2:<30}')\n",
    "        median_delta = np.median(deltas)\n",
    "        print(f'Median Delta = {median_delta:+.1%}')\n",
    "        \n",
    "    print('Same entity, different parties.\\n'\n",
    "          'Removing connotation should increase similarity.\\n'\n",
    "#           'Removing denotation should decrease similarity.'\n",
    "    )\n",
    "    cherries = [\n",
    "        ('estate_tax', 'death_tax'), \n",
    "        ('undocumented_immigrants', 'illegals'),\n",
    "                \n",
    "        ('health_care_reform', 'obamacare'),\n",
    "        ('public_option', 'governmentrun'),\n",
    "        ('national_health_insurance', 'government_takeover'),\n",
    "        ('national_health_insurance', 'welfare_state'),\n",
    "        ('singlepayer', 'governmentrun_health_care'),\n",
    "        ('singlepayer', 'socialized_medicine'),\n",
    "        ('universal_health_care', 'socialized_medicine'),\n",
    "        \n",
    "        ('campaign_spending', 'political_speech'),\n",
    "        ('independent_expenditures', 'political_speech'),\n",
    "        \n",
    "        ('recovery_and_reinvestment', 'stimulus_bill'),  # Note\n",
    "        ('military_spending', 'washington_spending'),\n",
    "        ('progrowth', 'create_jobs'),\n",
    "        \n",
    "        ('unborn', 'fetus'),\n",
    "        ('prochoice', 'proabortion'),\n",
    "        ('family_planning', 'proabortion')\n",
    "#         ('icbms', 'star_wars_program'),\n",
    "#         ('excessive_speculation', 'highfrequency'),\n",
    "#         ('corporate_profits', 'earnings'), \n",
    "#         ('megabanks', 'aig'),\n",
    "#         ('unemployment_insurance_benefits', 'stimulus'),\n",
    "#         ('retroactive_immunity', 'the_fisa_bill'),\n",
    "#         ('give_tax_breaks', 'cut_taxes'),\n",
    "#         ('sodomy', 'sex'),\n",
    "#         ('flat_tax', 'income_tax')\n",
    "    # capandtax, wall_street_reform, the_recovery_act, lesbian, inequality\n",
    "    #     'health_care_bill', ',\n",
    "    #     'the_wall_street_reform_legislation', 'financial_stability', 'capital_gains_tax',\n",
    "    #     'deficit_spending', 'bush_tax_cuts'\n",
    "    ]    \n",
    "    compare_similarities(cherries)\n",
    "     \n",
    "    print('\\n\\nDifferent entities, same party.\\n'\n",
    "          'Removing connotation should decresase similarity.\\n'\n",
    "#           'Removing denotation should increase similarity.'\n",
    "    )\n",
    "    ideologies = [\n",
    "        ('tax_cuts', 'right_to_life'),\n",
    "        ('new_entitlements', 'religious_freedom'),\n",
    "        ('illegals', 'unborn'),\n",
    "        ('antitrust_laws', 'lesbian'),\n",
    "        ('wall_street_reform', 'the_recovery_act'),\n",
    "        ('nuclear_option', 'clean_energy_jobs'), \n",
    "        ('record_deficits', 'living_wage'),\n",
    "        ('manmade_global_warming', 'radical_jihadists')\n",
    "    ]\n",
    "#     ideologies = [  # difficult to distinguish\n",
    "#         ('tax_cuts', 'new_entitlements'),\n",
    "#         ('religious_freedom', 'right_to_life')\n",
    "#     ]\n",
    "    compare_similarities(ideologies)\n",
    "    \n",
    "    print('\\n\\nDifferent entities, different parties. Control group.')\n",
    "    controls = [\n",
    "        ('traditional_marriage', 'lgbt'),\n",
    "        ('taxes', 'antitrust_laws'),\n",
    "        ('carbon', 'guns'),\n",
    "        ('abortion', 'minimum_wage'),\n",
    "        ('apple', 'piano'),\n",
    "        ('beef', 'computer'),\n",
    "    ]\n",
    "    compare_similarities(controls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size = 111,387, num_dimensions = 300\n",
      "Loading embeddings from ../../data/pretrained_word2vec/for_real.txt\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "models = {}\n",
    "models['w2v'] = Embedding('plain_text', '../../data/pretrained_word2vec/for_real.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '../../results/grid/with decoder/'\n",
    "epoch = 4\n",
    "models['0c'] = Embedding('adversarial', base_dir + f'1d 0c/epoch{epoch}.pt')\n",
    "models['-1c'] = Embedding('adversarial', base_dir + f'1d -1c/epoch{epoch}.pt')\n",
    "models['-2c'] = Embedding('adversarial', base_dir + f'1d -2c/epoch{epoch}.pt')\n",
    "models['-4c'] = Embedding('adversarial', base_dir + f'1d -4c/epoch{epoch}.pt')\n",
    "models['-8c'] = Embedding('adversarial', base_dir + f'1d -8c/epoch{epoch}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '../../results/grid/sans decoder/'\n",
    "models['0c'] = Embedding('adversarial', base_dir + '1d 0c/epoch7.pt')\n",
    "models['-1c'] = Embedding('adversarial', base_dir + '1d -8c/epoch7.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locally Trained\n",
    "base_dir = '../../results/with decoder/'\n",
    "epoch = 8\n",
    "models['0c'] = Embedding('adversarial', base_dir + f'1d 0c/epoch{epoch}.pt')\n",
    "models['-10c'] = Embedding('adversarial', base_dir + f'1d -10c/epoch{epoch}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Same entity, different parties.\n",
      "Removing connotation should increase similarity.\n",
      "Removing denotation should decrease similarity.\n",
      "89.2%\t91.1%\t+1.8%  estate_tax                    death_tax                     \n",
      "75.9%\t76.0%\t+0.1%  undocumented_immigrants       illegals                      \n",
      "57.4%\t62.5%\t+5.1%  health_care_reform            obamacare                     \n",
      "72.7%\t75.3%\t+2.6%  public_option                 governmentrun                 \n",
      "48.9%\t39.2%\t-9.7%  national_health_insurance     government_takeover           \n",
      "37.6%\t39.0%\t+1.4%  national_health_insurance     welfare_state                 \n",
      "80.4%\t82.0%\t+1.6%  singlepayer                   governmentrun_health_care     \n",
      "69.9%\t69.7%\t-0.3%  singlepayer                   socialized_medicine           \n",
      "49.1%\t49.1%\t+0.0%  universal_health_care         socialized_medicine           \n",
      "68.2%\t63.4%\t-4.8%  campaign_spending             political_speech              \n",
      "62.4%\t57.7%\t-4.7%  independent_expenditures      political_speech              \n",
      "69.0%\t71.4%\t+2.4%  recovery_and_reinvestment     stimulus_bill                 \n",
      "45.2%\t36.7%\t-8.5%  military_spending             washington_spending           \n",
      "68.6%\t66.8%\t-1.8%  progrowth                     create_jobs                   \n",
      "74.0%\t73.5%\t-0.5%  unborn                        fetus                         \n",
      "86.6%\t86.0%\t-0.6%  prochoice                     proabortion                   \n",
      "65.5%\t69.8%\t+4.3%  family_planning               proabortion                   \n",
      "Median Delta = +0.0%\n",
      "\n",
      "\n",
      "Different entities, same party.\n",
      "Removing connotation should decresase similarity.\n",
      "Removing denotation should increase similarity.\n",
      "-2.2%\t-1.3%\t+0.9%  tax_cuts                      right_to_life                 \n",
      "-8.9%\t-10.8%\t-1.9%  new_entitlements              religious_freedom             \n",
      "8.9%\t5.9%\t-3.0%  illegals                      unborn                        \n",
      "-0.5%\t-2.0%\t-1.5%  antitrust_laws                lesbian                       \n",
      "42.5%\t51.4%\t+8.8%  wall_street_reform            the_recovery_act              \n",
      "12.3%\t16.8%\t+4.5%  nuclear_option                clean_energy_jobs             \n",
      "7.2%\t5.0%\t-2.2%  record_deficits               living_wage                   \n",
      "22.9%\t12.5%\t-10.4%  manmade_global_warming        radical_jihadists             \n",
      "Median Delta = -1.7%\n",
      "\n",
      "\n",
      "Different entities, different parties. Control group.\n",
      "32.5%\t35.3%\t+2.8%  traditional_marriage          lgbt                          \n",
      "10.4%\t7.0%\t-3.4%  taxes                         antitrust_laws                \n",
      "-0.8%\t1.6%\t+2.3%  carbon                        guns                          \n",
      "7.5%\t2.4%\t-5.1%  abortion                      minimum_wage                  \n",
      "13.4%\t-1.5%\t-14.9%  apple                         piano                         \n",
      "7.1%\t-0.6%\t-7.6%  beef                          computer                      \n",
      "Median Delta = -4.3%\n"
     ]
    }
   ],
   "source": [
    "cherry_pick(models['w2v'], models['0c'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Same entity, different parties.\n",
      "Removing connotation should increase similarity.\n",
      "Removing denotation should decrease similarity.\n",
      "89.2%\t88.0%\t-1.2%  estate_tax                    death_tax                     \n",
      "75.9%\t87.4%\t+11.5%  undocumented_immigrants       illegals                      \n",
      "57.4%\t68.6%\t+11.3%  health_care_reform            obamacare                     \n",
      "72.7%\t79.1%\t+6.4%  public_option                 governmentrun                 \n",
      "48.9%\t75.5%\t+26.5%  national_health_insurance     government_takeover           \n",
      "37.6%\t70.5%\t+32.9%  national_health_insurance     welfare_state                 \n",
      "80.4%\t78.9%\t-1.4%  singlepayer                   governmentrun_health_care     \n",
      "69.9%\t63.2%\t-6.8%  singlepayer                   socialized_medicine           \n",
      "49.1%\t65.3%\t+16.2%  universal_health_care         socialized_medicine           \n",
      "68.2%\t67.8%\t-0.3%  campaign_spending             political_speech              \n",
      "62.4%\t66.0%\t+3.6%  independent_expenditures      political_speech              \n",
      "69.0%\t86.7%\t+17.7%  recovery_and_reinvestment     stimulus_bill                 \n",
      "45.2%\t84.7%\t+39.5%  military_spending             washington_spending           \n",
      "68.6%\t89.3%\t+20.7%  progrowth                     create_jobs                   \n",
      "74.0%\t84.8%\t+10.9%  unborn                        fetus                         \n",
      "86.6%\t94.7%\t+8.1%  prochoice                     proabortion                   \n",
      "65.5%\t69.2%\t+3.7%  family_planning               proabortion                   \n",
      "Median Delta = +10.9%\n",
      "\n",
      "\n",
      "Different entities, same party.\n",
      "Removing connotation should decresase similarity.\n",
      "Removing denotation should increase similarity.\n",
      "-2.2%\t-46.1%\t-43.9%  tax_cuts                      right_to_life                 \n",
      "-8.9%\t-52.2%\t-43.3%  new_entitlements              religious_freedom             \n",
      "8.9%\t-8.3%\t-17.2%  illegals                      unborn                        \n",
      "-0.5%\t11.5%\t+12.0%  antitrust_laws                lesbian                       \n",
      "42.5%\t-16.6%\t-59.2%  wall_street_reform            the_recovery_act              \n",
      "12.3%\t-5.3%\t-17.6%  nuclear_option                clean_energy_jobs             \n",
      "7.2%\t53.9%\t+46.6%  record_deficits               living_wage                   \n",
      "22.9%\t81.5%\t+58.6%  manmade_global_warming        radical_jihadists             \n",
      "Median Delta = -17.4%\n",
      "\n",
      "\n",
      "Different entities, different parties. Control group.\n",
      "32.5%\t72.1%\t+39.7%  traditional_marriage          lgbt                          \n",
      "10.4%\t-15.7%\t-26.1%  taxes                         antitrust_laws                \n",
      "-0.8%\t2.0%\t+2.7%  carbon                        guns                          \n",
      "7.5%\t7.6%\t+0.1%  abortion                      minimum_wage                  \n",
      "13.4%\t47.5%\t+34.2%  apple                         piano                         \n",
      "7.1%\t43.2%\t+36.1%  beef                          computer                      \n",
      "Median Delta = +18.4%\n"
     ]
    }
   ],
   "source": [
    "cherry_pick(models['w2v'], models['-10c'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Same entity, different parties.\n",
      "Removing connotation should increase similarity.\n",
      "Removing denotation should decrease similarity.\n",
      "91.1%\t88.0%\t-3.1%  estate_tax                    death_tax                     \n",
      "76.0%\t87.4%\t+11.3%  undocumented_immigrants       illegals                      \n",
      "62.5%\t68.6%\t+6.2%  health_care_reform            obamacare                     \n",
      "75.3%\t79.1%\t+3.8%  public_option                 governmentrun                 \n",
      "39.2%\t75.5%\t+36.2%  national_health_insurance     government_takeover           \n",
      "39.0%\t70.5%\t+31.5%  national_health_insurance     welfare_state                 \n",
      "82.0%\t78.9%\t-3.0%  singlepayer                   governmentrun_health_care     \n",
      "69.7%\t63.2%\t-6.5%  singlepayer                   socialized_medicine           \n",
      "49.1%\t65.3%\t+16.2%  universal_health_care         socialized_medicine           \n",
      "63.4%\t67.8%\t+4.5%  campaign_spending             political_speech              \n",
      "57.7%\t66.0%\t+8.3%  independent_expenditures      political_speech              \n",
      "71.4%\t86.7%\t+15.3%  recovery_and_reinvestment     stimulus_bill                 \n",
      "36.7%\t84.7%\t+47.9%  military_spending             washington_spending           \n",
      "66.8%\t89.3%\t+22.5%  progrowth                     create_jobs                   \n",
      "73.5%\t84.8%\t+11.3%  unborn                        fetus                         \n",
      "86.0%\t94.7%\t+8.7%  prochoice                     proabortion                   \n",
      "69.8%\t69.2%\t-0.6%  family_planning               proabortion                   \n",
      "Median Delta = +8.7%\n",
      "\n",
      "\n",
      "Different entities, same party.\n",
      "Removing connotation should decresase similarity.\n",
      "Removing denotation should increase similarity.\n",
      "-1.3%\t-46.1%\t-44.8%  tax_cuts                      right_to_life                 \n",
      "-10.8%\t-52.2%\t-41.3%  new_entitlements              religious_freedom             \n",
      "5.9%\t-8.3%\t-14.2%  illegals                      unborn                        \n",
      "-2.0%\t11.5%\t+13.5%  antitrust_laws                lesbian                       \n",
      "51.4%\t-16.6%\t-68.0%  wall_street_reform            the_recovery_act              \n",
      "16.8%\t-5.3%\t-22.1%  nuclear_option                clean_energy_jobs             \n",
      "5.0%\t53.9%\t+48.8%  record_deficits               living_wage                   \n",
      "12.5%\t81.5%\t+69.0%  manmade_global_warming        radical_jihadists             \n",
      "Median Delta = -18.2%\n",
      "\n",
      "\n",
      "Different entities, different parties. Control group.\n",
      "35.3%\t72.1%\t+36.8%  traditional_marriage          lgbt                          \n",
      "7.0%\t-15.7%\t-22.7%  taxes                         antitrust_laws                \n",
      "1.6%\t2.0%\t+0.4%  carbon                        guns                          \n",
      "2.4%\t7.6%\t+5.3%  abortion                      minimum_wage                  \n",
      "-1.5%\t47.5%\t+49.1%  apple                         piano                         \n",
      "-0.6%\t43.2%\t+43.8%  beef                          computer                      \n",
      "Median Delta = +21.0%\n"
     ]
    }
   ],
   "source": [
    "cherry_pick(models['0c'], models['-10c'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cherry_pick(models['-10c'], models['-12c'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# does not work\n",
    "cherry_pick(models['vanilla cono'], models['cono minus deno'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top Changes in Vector Spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_all_vocab(model1, model2, top_k=100, min_freq=100, max_freq=300):  \n",
    "    assert model1.id_to_word == model2.id_to_word\n",
    "    assert model1.Dem_frequency == model2.Dem_frequency\n",
    "    assert model1.GOP_frequency == model2.GOP_frequency\n",
    "    id_to_word = model1.id_to_word\n",
    "    freq: Counter[str] = model1.Dem_frequency + model1.GOP_frequency\n",
    "\n",
    "    # TODO select by indices\n",
    "    embed1 = np.array(\n",
    "        [vec.numpy() for word_id, vec in enumerate(model1.embedding) \n",
    "         if max_freq >= freq[id_to_word[word_id]] >= min_freq])\n",
    "    embed2 = np.array(\n",
    "        [vec.numpy() for word_id, vec in enumerate(model2.embedding) \n",
    "         if max_freq >= freq[id_to_word[word_id]] >= min_freq])    \n",
    "    print(f'min_freq = {min_freq}, filtered vocab size = {len(embed1):,}')\n",
    "    \n",
    "    sim1 = pairwise.cosine_similarity(embed1)\n",
    "    sim2 = pairwise.cosine_similarity(embed2)\n",
    "    sim1 = np.triu(sim1)\n",
    "    sim2 = np.triu(sim2)\n",
    "    sim_diff = sim1 - sim2\n",
    "    top_changes = np.argsort(sim_diff, axis=None)\n",
    "    \n",
    "    top_changed = []\n",
    "    for i, row in enumerate(sim_diff):\n",
    "        sorted_indices = np.argsort(row)\n",
    "        x = sorted_indices[:top_k]\n",
    "        y = sorted_indices[-top_k:]\n",
    "        both_extremes = np.hstack((x, y))\n",
    "        top_changed += [(sim_diff[i, j], id_to_word[i], id_to_word[j]) \n",
    "                        for j in both_extremes]\n",
    "            \n",
    "    top_changed.sort(key=lambda tup: tup[0], reverse=True)\n",
    "    return top_changed\n",
    "\n",
    "#     top_changed[:100]\n",
    "    \n",
    "#     top_changed.sort(key=lambda tup: tup[0], reverse=True)\n",
    "#     for sim_delta, x, y in top_changed:\n",
    "#         if 0.3 < sim_delta < 0.4:\n",
    "#             print(f'{sim_delta:.4f}  {x:<25}{y:<25}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stuff = compare_all_vocab(models['vanilla deno'], models['-10c'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stuff[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim1 = pairwise.cosine_similarity(models['deno minus cono'].embedding)\n",
    "sim2 = pairwise.cosine_similarity(models['vanilla deno'].embedding)\n",
    "# Filter > 100 freq\n",
    "sim1 = np.triu(sim1)\n",
    "sim2 = np.triu(sim2)\n",
    "sim_diff = sim1 - sim2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = models['vanilla deno'].id_to_word\n",
    "top_k = 5\n",
    "top_changed = []\n",
    "for i, row in enumerate(sim_diff):\n",
    "    sorted_indices = np.argsort(row)\n",
    "    x = sorted_indices[:top_k]\n",
    "    y = sorted_indices[-top_k:]\n",
    "    both_extremes = np.hstack((x, y))\n",
    "    top_changed += [(sim_diff[i, j], vocab[i], vocab[j]) \n",
    "                    for j in both_extremes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_changed.sort(key=lambda tup: tup[0], reverse=True)\n",
    "top_changed[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_changed.sort(key=lambda tup: tup[0], reverse=True)\n",
    "for sim_delta, x, y in top_changed:\n",
    "    if 0.3 < sim_delta < 0.4:\n",
    "        print(f'{sim_delta:.4f}  {x:<25}{y:<25}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
