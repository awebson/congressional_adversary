{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "from typing import Set, Tuple, List, Dict\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import scipy\n",
    "from sklearn.metrics import pairwise\n",
    "from tqdm import tqdm\n",
    "\n",
    "from adversarial import AdversarialDecomposer, AdversarialConfig\n",
    "from preprocessing.S4_export_training_corpus import Document\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "class Embedding():\n",
    "    \n",
    "    def __init__(self, source: str, path: str):\n",
    "        if source == 'adversarial':\n",
    "            self.init_from_adversarial(path)\n",
    "        elif source == 'skip_gram':\n",
    "            self.init_from_skip_gram(path)\n",
    "        elif source == 'plain_text':\n",
    "            self.init_from_plain_text(path)\n",
    "        else:\n",
    "            raise ValueError('Unknown embedding source.')\n",
    "            \n",
    "    def init_from_adversarial(self, path: str, device=torch.device('cpu')):\n",
    "        payload = torch.load(path, map_location=device)\n",
    "        model = payload['model']\n",
    "        self.word_to_id = model.word_to_id\n",
    "        self.id_to_word = model.id_to_word \n",
    "        # encoded layer\n",
    "        self.embedding = model.export_decomposed_embedding(device=device)\n",
    "        \n",
    "        # manually choose which layer to export\n",
    "#         all_vocab_ids = torch.arange(\n",
    "#             len(self.word_to_id), dtype=torch.long, device=device)\n",
    "#         with torch.no_grad():\n",
    "#             self.embedding = model.center_decoder(model.encoder_forward(all_vocab_ids))\n",
    "            \n",
    "#             self.embedding = self.encoder_forward(all_vocab_ids)\n",
    "#             self.embedding = model.denotation_decoder(model.encoder_forward(all_vocab_ids))\n",
    "#             self.embeddings = model.context_embedding(all_vocab_ids)  # galaxy brain\n",
    "            \n",
    "#     def init_from_adversarial(self, path: str):        \n",
    "#         config = DenotationEncoderConfig()\n",
    "#         config.input_dir = '../../data/processed/adversarial/44_Obama_1e-5'\n",
    "#         data = AdversarialDataset(config)\n",
    "#         model = DenotationEncoder(config, data)\n",
    "#         model.load_state_dict(torch.load(path))\n",
    "#         self.embedding = model.export_decomposed_embedding().to('cpu')\n",
    "#         self.word_to_id = model.word_to_id\n",
    "#         self.id_to_word = model.id_to_word\n",
    "\n",
    "    def init_from_skip_gram(self, paths: Tuple[str, str]) -> None:\n",
    "        \"\"\"Directly extract the weights of a single layer.\"\"\"\n",
    "        model_path, vocab_path = paths\n",
    "        with open(model_path, 'rb') as model_file:\n",
    "            state_dict = torch.load(model_file, map_location='cpu')\n",
    "    #     print(state_dict.keys())\n",
    "        self.embedding = state_dict['center_embedding.weight'].numpy()\n",
    "        with open(vocab_path, 'rb') as vocab_file:\n",
    "            self.word_to_id, self.id_to_word, _ = pickle.load(vocab_file)\n",
    "\n",
    "    def init_from_plain_text(self, path: str) -> Tuple[np.array, Dict[str, int]]:\n",
    "        id_generator = 0\n",
    "        word_to_id: Dict[str, int] = {}\n",
    "        embeddings: List[float] = []\n",
    "        embedding_file = open(path)\n",
    "        vocab_size, num_dimensions = map(int, embedding_file.readline().split())\n",
    "        print(f'vocab_size = {vocab_size:,}, num_dimensions = {num_dimensions}')\n",
    "        print(f'Loading embeddings from {path}', flush=True)\n",
    "        for line in embedding_file:\n",
    "            line: List[str] = line.split()  # type: ignore\n",
    "            word = line[0]\n",
    "            vector = np.array(line[-num_dimensions:], dtype=np.float64)\n",
    "            embeddings.append(vector)\n",
    "            word_to_id[word] = id_generator\n",
    "            id_generator += 1\n",
    "        embedding_file.close()\n",
    "        print('Done')\n",
    "        self.id_to_word = {val: key for key, val in word_to_id.items()}\n",
    "        self.word_to_id = word_to_id\n",
    "        self.embedding = np.array(embeddings)\n",
    "        \n",
    "    def write_to_tensorboard_projector(self, tb_dir: str) -> None:\n",
    "        from torch.utils import tensorboard\n",
    "        tb = tensorboard.SummaryWriter(log_dir=tb_dir)\n",
    "        all_vocab_ids = range(len(self.word_to_id))\n",
    "        embedding_labels = [\n",
    "            self.id_to_word[word_id]\n",
    "            for word_id in all_vocab_ids]\n",
    "        tb.add_embedding(\n",
    "            self.embedding[:9999], \n",
    "            embedding_labels[:9999], \n",
    "            global_step=0)\n",
    "        \n",
    "    def export_web_projector(self, out_dir: str) -> None:\n",
    "        random_indices = np.random.randint(len(self.embedding), size=10000)\n",
    "        subset_embedding = self.embedding[random_indices].tolist()\n",
    "        \n",
    "        vector_path = os.path.join(out_dir, 'tensorboard.tsv')\n",
    "        with open(vector_path, 'w') as vector_file:\n",
    "            for vector in subset_embedding:\n",
    "                vector_file.write('\\t'.join(map(str, vector)) + '\\n')\n",
    "\n",
    "        label_path = os.path.join(out_dir, 'tensorboard_labels.tsv')\n",
    "        with open(label_path, 'w') as label_file:\n",
    "            for index in random_indices:\n",
    "                label_file.write(self.id_to_word[index] + '\\n')\n",
    "\n",
    "    def cosine_similarity(self, query1: str, query2: str) -> float:\n",
    "        try:\n",
    "            query1_id = self.word_to_id[query1]\n",
    "        except KeyError as error:\n",
    "            print(f'{query1} is out of vocabulary. Sorry!')\n",
    "            raise error\n",
    "        try:\n",
    "            query2_id = self.word_to_id[query2]\n",
    "        except KeyError as error:\n",
    "            print(f'{query2} is out of vocabulary. Sorry!')\n",
    "            raise error\n",
    "        vectors = self.embedding[(query1_id, query2_id), :]\n",
    "        similarity = 1 - scipy.spatial.distance.cosine(vectors[0], vectors[1])\n",
    "        return similarity\n",
    "\n",
    "    def nearest_neighbor(self, query: str, top_k: int = 10):\n",
    "        try:\n",
    "            query_id = self.word_to_id[query]\n",
    "        except KeyError:\n",
    "            raise KeyError(f'{query} is out of vocabulary. Sorry!')    \n",
    "        query_vec = self.embedding[query_id]\n",
    "        \n",
    "        distances = [scipy.spatial.distance.cosine(query_vec, vec) \n",
    "                     for vec in self.embedding]\n",
    "        neighbors = np.argsort(distances)\n",
    "        print(f\"{query}'s neareset neighbors:\")\n",
    "        for ranking in range(1, top_k + 1):\n",
    "            word_id = neighbors[ranking]\n",
    "            word = self.id_to_word[word_id]\n",
    "            cosine_similarity = 1 - distances[word_id]\n",
    "            print(f'{cosine_similarity:.4f}\\t{word}')\n",
    "        print()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size = 34,100, num_dimensions = 300\n",
      "Loading embeddings from ../../results/baseline/word2vec_Obama.txt\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "models['w2v'] = Embedding(  # comparable to d1c0\n",
    "    'plain_text', \n",
    "    '../../results/baseline/word2vec_Obama.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/webson/Research/congressional_adversary/congressional_env/lib/python3.7/site-packages/torch/serialization.py:454: SourceChangeWarning: source code of class 'adversarial.AdversarialDecomposer' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    }
   ],
   "source": [
    "# linear-ReLU-linear encoder, frozen uniform init\n",
    "base_dir = '../../results/adversarial/MLP encoder/'\n",
    "\n",
    "models['vanilla deno'] = Embedding(\n",
    "    'adversarial', base_dir + '1d0c/epoch50.pt')\n",
    "\n",
    "models['deno minus cono'] = Embedding(\n",
    "    'adversarial', base_dir + '1d-1c/epoch15.pt')  # need more epochs\n",
    "\n",
    "models['vanilla cono'] = Embedding(\n",
    "    'adversarial', base_dir + '0d1c/epoch50.pt')\n",
    "\n",
    "# models['cono minus deno'] = Embedding(\n",
    "#     'adversarial', "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear-ReLU-linear encoder, frozen uniform init, adversary-free context, no gradient clipping\n",
    "base_dir = '../../results/adversarial/context sans adversary/'\n",
    "\n",
    "models['vanilla deno'] = Embedding(\n",
    "    'adversarial', base_dir + '1d0c bs1/epoch100.pt')\n",
    "\n",
    "# models['deno minus cono'] = Embedding(\n",
    "#     'adversarial', \n",
    "\n",
    "models['vanilla cono'] = Embedding(\n",
    "    'adversarial', base_dir + '0d1c/epoch30.pt')\n",
    "\n",
    "# models['cono minus deno'] = Embedding(\n",
    "#     'adversarial', "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear-ReLU encoder, frozen uniform init\n",
    "\n",
    "models['vanilla deno'] = Embedding(\n",
    "    'adversarial', \n",
    "    '../../results/adversarial/nonlinear encoder/d1c0/epoch45.pt')\n",
    "\n",
    "models['deno minus cono'] = Embedding(\n",
    "    'adversarial', \n",
    "    '../../results/adversarial/nonlinear encoder/d1c-1_w2v/epoch50.pt')  # inconsistent init!\n",
    "\n",
    "models['vanilla cono'] = Embedding(\n",
    "    'adversarial', \n",
    "    '../../results/adversarial/nonlinear encoder/d0c1/epoch25.pt')\n",
    "\n",
    "models['cono mius deno'] = Embedding(\n",
    "    'adversarial', \n",
    "    '../../results/adversarial/nonlinear encoder/d-0.1c1/epoch50.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models['cono'].write_to_tensorboard_projector(\n",
    "#     '../../results/adversarial/Obama/p8_.55to.75/d0_c1/embedding_projector')\n",
    "# models['cono'].export_web_tensorboard('../../results/adversarial/Obama/p8_.55to.75/d0_c1/web_projector')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models['vanilla cono']\n",
    "model.nearest_neighbor('estate_tax')\n",
    "model.nearest_neighbor('death_tax')\n",
    "# model.nearest_neighbor('undocumented_immigrants')\n",
    "model.nearest_neighbor('illegal_aliens')\n",
    "model.nearest_neighbor('music')\n",
    "model.nearest_neighbor('cry')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cherry_pick(model):\n",
    "    \n",
    "    def print_similarities(pairs):\n",
    "        for word1, word2 in pairs:\n",
    "            try: \n",
    "                print(f'{model.cosine_similarity(word1, word2):.4f}  '\n",
    "                      f'{word1:<20}{word2:<20}')\n",
    "            except KeyError:\n",
    "                pass\n",
    "        \n",
    "    print('Same entity, different parties. Removing connotation should increase similarity:')\n",
    "    cherries = [\n",
    "        ('estate_tax', 'death_tax'), \n",
    "        ('undocumented', 'illegal_aliens'),\n",
    "        ('obamacare', 'protection_and_affordable'),\n",
    "        ('socialized_medicine', 'public_option'),\n",
    "        ('second_amendment_rights', 'guns'), \n",
    "    #     'health_care_bill', ',\n",
    "    #     'the_wall_street_reform_legislation', 'financial_stability', 'capital_gains_tax',\n",
    "    #     'deficit_spending', 'bush_tax_cuts'\n",
    "    ]    \n",
    "    print_similarities(cherries)\n",
    "     \n",
    "    print('\\n\\nDifferent entities, same party. Removing denotation should increase similarity')\n",
    "    ideologies = [\n",
    "        ('tax_cuts', 'entitlement_reform'),\n",
    "        ('religious_freedom', 'right_to_life')\n",
    "    ]\n",
    "    print_similarities(ideologies)\n",
    "    \n",
    "    print('\\n\\nDifferent entities, different parties. Removing connotation should not increase similarity:')\n",
    "    controls = [\n",
    "        ('taxes', 'antitrust_laws'),\n",
    "        ('carbon', 'guns'),\n",
    "        ('apple', 'piano'),\n",
    "        ('beef', 'burger')\n",
    "    ]\n",
    "    print_similarities(controls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Same entity, different parties. Removing connotation should increase similarity:\n",
      "0.7583  estate_tax          death_tax           \n",
      "0.6868  undocumented        illegal_aliens      \n",
      "0.3852  obamacare           protection_and_affordable\n",
      "0.4785  socialized_medicine public_option       \n",
      "0.4932  second_amendment_rightsguns                \n",
      "\n",
      "\n",
      "Different entities, same party. Removing denotation should increase similarity\n",
      "entitlement_reform is out of vocabulary. Sorry!\n",
      "0.5378  religious_freedom   right_to_life       \n",
      "\n",
      "\n",
      "Different entities, different parties. Removing connotation should not increase similarity:\n",
      "0.2094  taxes               abortion            \n",
      "0.1810  carbon              guns                \n",
      "0.5238  apple               piano               \n",
      "0.2939  beef                burger              \n"
     ]
    }
   ],
   "source": [
    "cherry_pick(models['w2v'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Same entity, different parties. Removing connotation should increase similarity:\n",
      "0.9995  estate_tax          death_tax           \n",
      "0.6701  undocumented        illegal_aliens      \n",
      "0.9935  obamacare           protection_and_affordable\n",
      "0.1158  socialized_medicine public_option       \n",
      "0.9998  second_amendment_rightsguns                \n",
      "\n",
      "\n",
      "Different entities, same party. Removing denotation should increase similarity\n",
      "entitlement_reform is out of vocabulary. Sorry!\n",
      "0.8678  religious_freedom   right_to_life       \n",
      "\n",
      "\n",
      "Different entities, different parties. Removing connotation should not increase similarity:\n",
      "0.1889  taxes               abortion            \n",
      "0.3363  carbon              guns                \n",
      "0.2952  apple               piano               \n",
      "0.9531  beef                burger              \n"
     ]
    }
   ],
   "source": [
    "cherry_pick(models['vanilla deno'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Same entity, different parties. Removing connotation should increase similarity:\n",
      "0.8995  estate_tax          death_tax           \n",
      "0.8180  undocumented        illegal_aliens      \n",
      "0.8484  obamacare           protection_and_affordable\n",
      "0.6634  socialized_medicine public_option       \n",
      "0.9449  second_amendment_rightsguns                \n",
      "\n",
      "\n",
      "Different entities, same party. Removing denotation should increase similarity\n",
      "entitlement_reform is out of vocabulary. Sorry!\n",
      "0.8614  religious_freedom   right_to_life       \n",
      "\n",
      "\n",
      "Different entities, different parties. Removing connotation should not increase similarity:\n",
      "0.8174  taxes               abortion            \n",
      "0.8542  carbon              guns                \n",
      "0.4833  apple               piano               \n",
      "0.7790  beef                burger              \n"
     ]
    }
   ],
   "source": [
    "cherry_pick(models['deno minus cono'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
