{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Tuple, Union, List, Dict, Iterable, Optional\n",
    "\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from decomposer import Decomposer, DecomposerConfig\n",
    "from recomposer import Recomposer, RecomposerConfig\n",
    "# from evaluations.helpers import GroundedWord, load_recomposers_en_masse\n",
    "# from evaluations.clustering import graph_en_masse\n",
    "# from evaluations.euphemism import cherry_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size = 77,647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/webson/Research/congressional_adversary/congressional_env/lib/python3.7/site-packages/torch/serialization.py:593: SourceChangeWarning: source code of class 'decomposer.Decomposer' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29 capitalists\n",
      "64 socialists\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "BASE_DIR = Path.home() / 'Research/congressional_adversary/results'\n",
    "# sup_PE = torch.load(BASE_DIR / 'SGNS deno/pretrained super large/init.pt')['model']\n",
    "sup_PE = torch.load(BASE_DIR / 'news/validation/pretrained/init.pt')['model']\n",
    "WTI = sup_PE.word_to_id\n",
    "ITW = sup_PE.id_to_word\n",
    "sup_PE = sup_PE.embedding.weight.detach().cpu().numpy()\n",
    "print(f'Vocab size = {len(WTI):,}')\n",
    "\n",
    "\n",
    "sub_PE = torch.load(BASE_DIR / 'bill topic/pretrained subset/init.pt')['model']\n",
    "sub_PE_WID = sub_PE.word_to_id\n",
    "sub_PE_GD = sub_PE.grounding\n",
    "del sub_PE\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GroundedWord():\n",
    "    word: str\n",
    "\n",
    "#     def __post_init__(self) -> None:\n",
    "#         self.word_id: int = WTI[self.word]\n",
    "#         metadata = sub_PE_GD[self.word]\n",
    "#         self.freq: int = metadata['freq']\n",
    "#         self.R_ratio: float = metadata['R_ratio']\n",
    "#         self.majority_deno: int = metadata['majority_deno']\n",
    "\n",
    "#         self.PE_neighbors = self.neighbors(sup_PE)\n",
    "            \n",
    "#     def neighbors(self, embed, top_k=10): \n",
    "#         query_id = sup_PE.word_to_id[self.word]\n",
    "#         query_vec = sup_PE[query_id]\n",
    "#         distances = [\n",
    "#             distance.cosine(query_vec, neighbor_vec)\n",
    "#             for neighbor_vec in sup_PE]\n",
    "#         self.sup_PE_neighbors = set()\n",
    "#         for sort_rank, neighbor_id in enumerate(sorted_neighbor_indices):\n",
    "#             if num_neighbors == top_k:\n",
    "#                 break\n",
    "#             if query_id == neighbor_id:\n",
    "#                 continue\n",
    "#             neighbor_word = self.id_to_word[neighbor_id]\n",
    "#             if editdistance.eval(query_word, neighbor_word) < 3:\n",
    "#                 continue\n",
    "            \n",
    "#         self.sub_PE_neighbors: List[str] = nearest(, sub_PE)\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return str(vars(self))\n",
    "    \n",
    "    \n",
    "capitalism: List[GroundedWord] = []\n",
    "socialism: List[GroundedWord] = []\n",
    "for word in sub_PE_WID.keys():\n",
    "    ratio = sub_PE_GD[word]['R_ratio']\n",
    "    freq = sub_PE_GD[word]['freq']\n",
    "    word = GroundedWord(word)\n",
    "    if ratio < 0.2 and freq > 100:  # 0.2:\n",
    "        socialism.append(word)\n",
    "    elif ratio > 0.8 and freq > 100:  # 0.8:\n",
    "        capitalism.append(word)\n",
    "\n",
    "print(\n",
    "    f'{len(capitalism)} capitalists\\n'\n",
    "    f'{len(socialism)} socialists')\n",
    "polarization = capitalism + socialism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_embed(model: Decomposer) -> np.ndarray:\n",
    "    return model.embedding.weight.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "def load(\n",
    "        path: Path,\n",
    "        match_vocab: bool = False,\n",
    "        device: str = 'cpu'\n",
    "        ) -> np.ndarray:\n",
    "    model = torch.load(path, map_location=device)['model']\n",
    "    try:\n",
    "        assert model.word_to_id == WTI\n",
    "    except AssertionError:\n",
    "        print(f'Vocabulary mismatch: {path}')\n",
    "        print(f'Vocab size = {len(model.word_to_id)}')\n",
    "        if match_vocab:\n",
    "            raise RuntimeError\n",
    "        else:\n",
    "            return None\n",
    "    return get_embed(model)\n",
    "\n",
    "\n",
    "def load_decomposers_en_masse(\n",
    "        in_dirs: Union[Path, List[Path]],\n",
    "        patterns: Union[str, List[str]]\n",
    "        ) -> Tuple[Dict[str, np.ndarray], ...]:\n",
    "    if not isinstance(in_dirs, List):\n",
    "        in_dirs = [in_dirs, ]\n",
    "    if not isinstance(patterns, List):\n",
    "        patterns = [patterns, ]\n",
    "    checkpoints: List[Path] = []\n",
    "    for in_dir in in_dirs:\n",
    "        for pattern in patterns:\n",
    "            checkpoints += list(in_dir.glob(pattern))\n",
    "    if len(checkpoints) == 0:\n",
    "        raise FileNotFoundError('No model with path pattern found at in_dir?')\n",
    "\n",
    "    models = {\n",
    "#         'pretrained superset': load(BASE_DIR / 'bill topic/pretrained superset/init.pt'),\n",
    "#         'pretrained subset': load(BASE_DIR / 'bill topic/pretrained subset/init.pt')\n",
    "    }\n",
    "    for path in tqdm(checkpoints):\n",
    "        tqdm.write(f'Loading {path}')\n",
    "        embed = load(path) \n",
    "        if embed is None:\n",
    "            continue\n",
    "#         name = path.parent.name\n",
    "        name = path.name \n",
    "        models[name] = embed\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "sns.set()\n",
    "\n",
    "def plot(\n",
    "        coordinates: np.ndarray,\n",
    "        words: List[GroundedWord],\n",
    "        path: Path\n",
    "        ) -> None:\n",
    "    fig, ax = plt.subplots(figsize=(15, 10))\n",
    "#     skew = [w.R_ratio for w in words]\n",
    "#     freq = [w.freq for w in words]\n",
    "    sns.scatterplot(\n",
    "        coordinates[:, 0], coordinates[:, 1],\n",
    "#         hue=skew, palette='coolwarm',  # hue_norm=(0, 1),\n",
    "#         size=freq, sizes=(200, 1000),\n",
    "        legend=None, ax=ax)\n",
    "    for coord, w in zip(coordinates, words):\n",
    "        ax.annotate(w.word, coord, fontsize=20)\n",
    "    with open(path, 'wb') as file:\n",
    "        fig.savefig(file, dpi=300)\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def plot_categorical(\n",
    "        coordinates: np.ndarray,\n",
    "        words: List[GroundedWord],\n",
    "        path: Path,\n",
    "        fancy: bool = True\n",
    "        ) -> None:\n",
    "    if fancy:\n",
    "        fig, ax = plt.subplots(figsize=(20, 10))\n",
    "        categories = [w.majority_deno for w in words]\n",
    "        freq = [w.freq for w in words]\n",
    "        sns.scatterplot(\n",
    "            coordinates[:, 0], coordinates[:, 1],\n",
    "            hue=categories, palette='muted', hue_norm=(0, 1),\n",
    "            size=freq, sizes=(200, 1000),\n",
    "            legend='brief', \n",
    "            ax=ax)\n",
    "        chartBox = ax.get_position()\n",
    "        ax.set_position(  # adjust legend\n",
    "            [chartBox.x0, chartBox.y0, chartBox.width * 0.6, chartBox.height])\n",
    "        ax.legend(loc='upper center', bbox_to_anchor=(1.45, 0.8), ncol=1)\n",
    "    else:\n",
    "        fig, ax = plt.subplots(figsize=(20, 10))\n",
    "        freq = [w.freq for w in words]\n",
    "        sns.scatterplot(\n",
    "            coordinates[:, 0], coordinates[:, 1], ax=ax)\n",
    "\n",
    "    for coord, w in zip(coordinates, words):\n",
    "        ax.annotate(w.word, coord, fontsize=12)\n",
    "    with open(path, 'wb') as file:\n",
    "        fig.savefig(file, dpi=300)\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def graph_en_masse(\n",
    "        models: Dict[str, np.ndarray],\n",
    "        out_dir: Path,\n",
    "        reduction: str,  # 'PCA', 'TSNE', or 'both'\n",
    "        words: List[GroundedWord],\n",
    "        # hues: Union[List[float], List[int]],\n",
    "        # sizes: List[int],\n",
    "        perplexity: Optional[int] = None,\n",
    "        categorical: bool = False\n",
    "        ) -> None:\n",
    "    Path.mkdir(out_dir, parents=True, exist_ok=True)\n",
    "    word_ids = np.array([w.word_id for w in words])\n",
    "    for model_name, embed in tqdm(models.items()):\n",
    "        space = embed[word_ids]\n",
    "        if reduction == 'PCA':\n",
    "            visual = PCA(n_components=2).fit_transform(space)\n",
    "        elif reduction == 'TSNE':\n",
    "            assert perplexity is not None\n",
    "            visual = TSNE(\n",
    "                perplexity=perplexity, learning_rate=10,\n",
    "                n_iter=5000, n_iter_without_progress=1000).fit_transform(space)\n",
    "        elif reduction == 'both':\n",
    "            assert perplexity is not None\n",
    "            space = PCA(n_components=30).fit_transform(space)\n",
    "            visual = TSNE(\n",
    "                perplexity=perplexity, learning_rate=10,\n",
    "                n_iter=5000, n_iter_without_progress=1000).fit_transform(space)\n",
    "        else:\n",
    "            raise ValueError('unknown dimension reduction method')\n",
    "        if categorical:\n",
    "            plot_categorical(visual, words, out_dir / f'{model_name}.png')\n",
    "        else:\n",
    "            plot(visual, words, out_dir / f'{model_name}.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cherry_words = [\n",
    "    'government', 'washington',\n",
    "    'estate_tax', 'death_tax',\n",
    "    'public_option', 'governmentrun',\n",
    "    'foreign_trade', 'international_trade',\n",
    "    'cut_taxes', 'trickledown'\n",
    "]\n",
    "\n",
    "cherry_words = [GroundedWord(w) for w in cherry_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42d6708552484956aaf571aa6c048beb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ../../results/news/validation/-3c/epoch3.pt\n",
      "Loading ../../results/news/validation/-3c/epoch5.pt\n",
      "Loading ../../results/news/validation/-3c/epoch4.pt\n",
      "Loading ../../results/news/validation/-3c/epoch2.pt\n",
      "Loading ../../results/news/validation/-3c/epoch1.pt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# base_dir = Path('../../results/SGNS deno/sans recomposer')\n",
    "# deno_space = load_decomposers_en_masse(base_dir, patterns='*/epoch10.pt')\n",
    "\n",
    "base_dir = Path('../../results/news/validation')\n",
    "deno_space = load_decomposers_en_masse(base_dir, patterns='*/epoch*.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "models = deno_space\n",
    "stuff = polarization\n",
    "\n",
    "graph_en_masse(\n",
    "    models, out_dir=base_dir / 'cherry/topic/t-SNE p5',\n",
    "    reduction='TSNE', perplexity=5, words=stuff, categorical=True)\n",
    "graph_en_masse(\n",
    "    models, out_dir=base_dir / 'cherry/topic/t-SNE p3',\n",
    "    reduction='TSNE', perplexity=3, words=stuff, categorical=True)\n",
    "graph_en_masse(\n",
    "    models, out_dir=base_dir / 'cherry/topic/t-SNE p2',\n",
    "    reduction='TSNE', perplexity=2, words=stuff, categorical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine as cos_dist\n",
    "import editdistance\n",
    "\n",
    "def vec(query: str, embed: np.ndarray) -> np.ndarray:\n",
    "    try:\n",
    "        query_id = WTI[query]\n",
    "    except KeyError:\n",
    "        raise KeyError(f'Out of vocabulary: {query}')\n",
    "    return embed[query_id]\n",
    "\n",
    "\n",
    "def nearest_neighbors(\n",
    "        query: str,\n",
    "        embed: np.ndarray,\n",
    "        top_k: int = 10\n",
    "        ) -> None:\n",
    "    query_vec = vec(query, embed)\n",
    "    print(f\"{query}'s neareset neighbors:\")\n",
    "    distances = [\n",
    "        cos_dist(query_vec, neighbor_vec)\n",
    "        for neighbor_vec in embed]\n",
    "    neighbor_indices = np.argsort(distances)\n",
    "    num_neighbors = 0        \n",
    "    for sort_rank, neighbor_id in enumerate(neighbor_indices):\n",
    "        if num_neighbors == top_k:\n",
    "            break\n",
    "#         if query_id == neighbor_id:\n",
    "#             continue\n",
    "        neighbor_word = ITW[neighbor_id]\n",
    "\n",
    "        if editdistance.eval(query, neighbor_word) < 3:\n",
    "            continue\n",
    "        cosine_similarity = 1 - distances[neighbor_id]\n",
    "        # neighbor_ids.append(neighbor_id)\n",
    "        num_neighbors += 1\n",
    "        print(f'{cosine_similarity:.4f}\\t{neighbor_word}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deno_space['bill topic'] = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['epoch3.pt', 'epoch5.pt', 'epoch4.pt', 'epoch2.pt', 'epoch1.pt'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deno_space.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = nearest_neighbors\n",
    "our_model = deno_space['epoch5.pt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Out of vocabulary: estate_tax'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-4b93ce5d13b1>\u001b[0m in \u001b[0;36mvec\u001b[0;34m(query, embed)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mquery_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWTI\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'estate_tax'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-a6e0e891b0af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'estate_tax'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'estate_tax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msup_PE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'estate_tax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mour_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-4b93ce5d13b1>\u001b[0m in \u001b[0;36mnearest_neighbors\u001b[0;34m(query, embed, top_k)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mtop_k\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         ) -> None:\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mquery_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{query}'s neareset neighbors:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     distances = [\n",
      "\u001b[0;32m<ipython-input-5-4b93ce5d13b1>\u001b[0m in \u001b[0;36mvec\u001b[0;34m(query, embed)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mquery_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWTI\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Out of vocabulary: {query}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0membed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mquery_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Out of vocabulary: estate_tax'"
     ]
    }
   ],
   "source": [
    "query = 'estate_tax'\n",
    "nn(query, sup_PE)\n",
    "nn(query, our_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Out of vocabulary: pro_choice'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-4b93ce5d13b1>\u001b[0m in \u001b[0;36mvec\u001b[0;34m(query, embed)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mquery_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWTI\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'pro_choice'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-a830765979da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'pro_choice'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msup_PE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mour_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-4b93ce5d13b1>\u001b[0m in \u001b[0;36mnearest_neighbors\u001b[0;34m(query, embed, top_k)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mtop_k\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         ) -> None:\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mquery_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{query}'s neareset neighbors:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     distances = [\n",
      "\u001b[0;32m<ipython-input-5-4b93ce5d13b1>\u001b[0m in \u001b[0;36mvec\u001b[0;34m(query, embed)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mquery_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWTI\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Out of vocabulary: {query}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0membed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mquery_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Out of vocabulary: pro_choice'"
     ]
    }
   ],
   "source": [
    "query = 'pro_choice'\n",
    "nn(query, sup_PE)\n",
    "nn(query, our_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undocumented's neareset neighbors:\n",
      "0.9609\timmigrants\n",
      "0.9382\tunauthorized\n",
      "0.9333\taliens\n",
      "0.9316\tdeportation\n",
      "0.9298\timmigrant\n",
      "0.9217\tcitizenship\n",
      "0.9067\teligible\n",
      "0.9053\tsanctuary\n",
      "0.9024\tdeport\n",
      "0.9001\tillegal\n",
      "\n",
      "undocumented's neareset neighbors:\n",
      "0.6909\timmigrants\n",
      "0.6336\tdeportation\n",
      "0.5718\timmigration\n",
      "0.5396\tcitizenship\n",
      "0.5113\tdeport\n",
      "0.5095\timmigrant\n",
      "0.5012\taliens\n",
      "0.4949\tillegals\n",
      "0.4696\tdeported\n",
      "0.4663\tborder\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = 'undocumented'\n",
    "nn(query, sup_PE)\n",
    "nn(query, our_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "illegals's neareset neighbors:\n",
      "0.9825\tdreamers\n",
      "0.9733\tdeport\n",
      "0.9731\tdeporting\n",
      "0.9725\tbirthright\n",
      "0.9701\tstripped\n",
      "0.9680\tdiscriminating\n",
      "0.9646\tregistering\n",
      "0.9641\tpenalized\n",
      "0.9638\tsecond_amendment\n",
      "0.9632\tbribe\n",
      "\n",
      "illegals's neareset neighbors:\n",
      "0.5105\tdeportation\n",
      "0.5005\timmigrants\n",
      "0.4949\tundocumented\n",
      "0.4411\tamnesty\n",
      "0.4403\taliens\n",
      "0.3950\tdeport\n",
      "0.3861\talien\n",
      "0.3749\timmigration\n",
      "0.3669\tminors\n",
      "0.3661\tdeported\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = 'illegals'\n",
    "nn(query, sup_PE)\n",
    "nn(query, our_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aliens's neareset neighbors:\n",
      "0.9636\tunauthorized\n",
      "0.9584\tdeportation\n",
      "0.9532\tillegal\n",
      "0.9431\toffenses\n",
      "0.9406\tinmates\n",
      "0.9400\tjails\n",
      "0.9372\tprisons\n",
      "0.9333\tundocumented\n",
      "0.9311\tdeport\n",
      "0.9292\tfelons\n",
      "\n",
      "aliens's neareset neighbors:\n",
      "0.5995\tillegal\n",
      "0.5481\tdeportation\n",
      "0.5104\timmigrants\n",
      "0.5012\tundocumented\n",
      "0.4507\tamnesty\n",
      "0.4403\tillegals\n",
      "0.4392\tdeport\n",
      "0.4235\tdeporting\n",
      "0.4070\timmigration\n",
      "0.3991\tcitizenship\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = 'aliens'\n",
    "nn(query, sup_PE)\n",
    "nn(query, our_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leftists's neareset neighbors:\n",
      "0.9869\tfundamentalists\n",
      "0.9849\tatheists\n",
      "0.9832\tpatriotism\n",
      "0.9818\tfeminists\n",
      "0.9774\trighteous\n",
      "0.9764\tpatriotic\n",
      "0.9753\tbelievers\n",
      "0.9752\tfundamentalist\n",
      "0.9743\tfanatics\n",
      "0.9740\tintolerant\n",
      "\n",
      "leftists's neareset neighbors:\n",
      "0.3259\tprogressives\n",
      "0.3237\tliberals\n",
      "0.3066\tmoderates\n",
      "0.2980\tanarchists\n",
      "0.2965\tmovement\n",
      "0.2925\tanti-american\n",
      "0.2820\tislamists\n",
      "0.2793\tnationalists\n",
      "0.2793\tactivists\n",
      "0.2742\tsupremacists\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = 'leftists'\n",
    "nn(query, sup_PE)\n",
    "nn(query, our_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "antifa's neareset neighbors:\n",
      "0.9769\tmob\n",
      "0.9750\tkkk\n",
      "0.9749\tneo-nazis\n",
      "0.9701\tslurs\n",
      "0.9679\tmartyr\n",
      "0.9676\tmuhammad\n",
      "0.9630\tblack_lives_matter\n",
      "0.9617\tmohammed\n",
      "0.9606\tpreacher\n",
      "0.9604\tsavage\n",
      "\n",
      "antifa's neareset neighbors:\n",
      "0.3674\tblack_lives_matter\n",
      "0.3640\tmarches\n",
      "0.3483\tsupremacists\n",
      "0.3327\tcharlottesville\n",
      "0.3285\trally\n",
      "0.3278\toccupy_wall_street\n",
      "0.3072\tprotesters\n",
      "0.2905\tsupremacist\n",
      "0.2844\tchants\n",
      "0.2842\tsympathizer\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = 'antifa'\n",
    "nn(query, sup_PE)\n",
    "nn(query, our_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anti-abortion's neareset neighbors:\n",
      "0.9377\tchallenged\n",
      "0.9376\tsonia_sotomayor\n",
      "0.9361\tclarence_thomas\n",
      "0.9349\tthe_national_rifle_association\n",
      "0.9348\tanthony_kennedy\n",
      "0.9348\tnaacp\n",
      "0.9333\taclu\n",
      "0.9329\tpro-life\n",
      "0.9327\tu.s._supreme_court\n",
      "0.9327\tsamuel_alito\n",
      "\n",
      "anti-abortion's neareset neighbors:\n",
      "0.5418\tabortion\n",
      "0.4835\tfetal\n",
      "0.4782\tplanned_parenthood\n",
      "0.4431\tabortions\n",
      "0.4280\tfetus\n",
      "0.4271\treproductive\n",
      "0.4253\tfetuses\n",
      "0.3958\tpro-life\n",
      "0.3908\taborted\n",
      "0.3750\tclinics\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = 'anti-abortion'\n",
    "nn(query, sup_PE)\n",
    "nn(query, our_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wealthiest's neareset neighbors:\n",
      "0.9701\tearners\n",
      "0.9667\tbenefit\n",
      "0.9638\tentitlements\n",
      "0.9632\tmillionaires\n",
      "0.9622\tbenefiting\n",
      "0.9616\tbrackets\n",
      "0.9604\tretirees\n",
      "0.9602\tjobless\n",
      "0.9598\tregressive\n",
      "0.9589\trichest\n",
      "\n",
      "wealthiest's neareset neighbors:\n",
      "0.4102\ttax\n",
      "0.4082\tearners\n",
      "0.4035\trich\n",
      "0.3713\ttaxes\n",
      "0.3692\twealthy\n",
      "0.3581\tincome\n",
      "0.3485\tcorporations\n",
      "0.3439\tcuts\n",
      "0.3406\trichest\n",
      "0.3352\tbreaks\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = 'wealthiest'\n",
    "nn(query, sup_PE)\n",
    "nn(query, our_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wall's neareset neighbors:\n",
      "0.9527\tstreet\n",
      "0.8507\toff\n",
      "0.8440\tback\n",
      "0.8347\tfence\n",
      "0.8298\talong\n",
      "0.8263\tonto\n",
      "0.8225\taway\n",
      "0.8206\tstops\n",
      "0.8204\tstarted\n",
      "0.8198\tpulling\n",
      "\n",
      "wall's neareset neighbors:\n",
      "0.6686\tstreet\n",
      "0.3769\twall_street\n",
      "0.3458\tborder\n",
      "0.3075\tbailout\n",
      "0.3048\tbuild\n",
      "0.2892\tmexican\n",
      "0.2856\twindow\n",
      "0.2788\tbuilding\n",
      "0.2679\tmarket\n",
      "0.2647\tmanaged\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = 'wall'\n",
    "nn(query, sup_PE)\n",
    "nn(query, our_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = ''\n",
    "nn(query, sup_PE)\n",
    "nn(query, our_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obamacare's neareset neighbors:\n",
      "0.9803\tthe_affordable_care_act\n",
      "0.9688\trepeal\n",
      "0.9678\trepealing\n",
      "0.9674\taca\n",
      "0.9565\tmedicare\n",
      "0.9515\tpayer\n",
      "0.9392\thealthcare\n",
      "0.9346\tahca\n",
      "0.9302\tskinny\n",
      "0.9254\tsocial_security\n",
      "\n",
      "obamacare's neareset neighbors:\n",
      "0.5771\tthe_affordable_care_act\n",
      "0.5704\trepeal\n",
      "0.5469\tinsurance\n",
      "0.5284\taca\n",
      "0.5032\trepealing\n",
      "0.4914\thealthcare\n",
      "0.4864\tinsurers\n",
      "0.4863\tmedicaid\n",
      "0.4794\tpremiums\n",
      "0.4517\tmarketplaces\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = 'obamacare'\n",
    "nn(query, sup_PE)\n",
    "nn(query, our_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aca's neareset neighbors:\n",
      "0.9905\tthe_affordable_care_act\n",
      "0.9862\trepealing\n",
      "0.9713\tpayer\n",
      "0.9674\tobamacare\n",
      "0.9626\tmedicare\n",
      "0.9607\trepealed\n",
      "0.9553\tentitlement\n",
      "0.9547\tmarketplaces\n",
      "0.9543\tenrollees\n",
      "0.9537\teligibility\n",
      "\n",
      "aca's neareset neighbors:\n",
      "0.5284\tobamacare\n",
      "0.5278\tinsurers\n",
      "0.5112\tinsurance\n",
      "0.5005\tthe_affordable_care_act\n",
      "0.4786\tpre-existing\n",
      "0.4594\tpremiums\n",
      "0.4515\tuninsured\n",
      "0.4511\tcoverage\n",
      "0.4305\tmarketplaces\n",
      "0.4281\tmandate\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = 'aca'\n",
    "nn(query, sup_PE)\n",
    "nn(query, our_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "socialized's neareset neighbors:\n",
      "0.9868\tneedy\n",
      "0.9863\tsicker\n",
      "0.9845\thandouts\n",
      "0.9835\tlivelihood\n",
      "0.9830\tseparating\n",
      "0.9821\tprofessions\n",
      "0.9818\tunaffordable\n",
      "0.9807\tchronically\n",
      "0.9794\ttakers\n",
      "0.9792\tvaccinations\n",
      "\n",
      "socialized's neareset neighbors:\n",
      "0.4164\tmedicine\n",
      "0.3018\thancock\n",
      "0.2846\tnostalgic\n",
      "0.2830\talleys\n",
      "0.2787\tjettisoned\n",
      "0.2777\tsanford\n",
      "0.2770\tasses\n",
      "0.2748\tthe_french_revolution\n",
      "0.2726\talphabet\n",
      "0.2692\tunalienable\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = 'socialized'\n",
    "nn(query, sup_PE)\n",
    "nn(query, our_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "guns's neareset neighbors:\n",
      "0.9117\tcops\n",
      "0.9054\tcarry\n",
      "0.9046\tfirearms\n",
      "0.9013\tstealing\n",
      "0.8996\tphysically\n",
      "0.8964\tcriminals\n",
      "0.8947\tsmoking\n",
      "0.8851\tbars\n",
      "0.8837\talien\n",
      "0.8836\taliens\n",
      "\n",
      "guns's neareset neighbors:\n",
      "0.5922\tammunition\n",
      "0.5623\tfirearms\n",
      "0.5047\trifles\n",
      "0.4887\tnra\n",
      "0.4859\trifle\n",
      "0.4815\tsemi-automatic\n",
      "0.4495\thandgun\n",
      "0.4401\tautomatic\n",
      "0.4378\tconcealed\n",
      "0.4259\tfirearm\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = 'guns'\n",
    "nn(query, sup_PE)\n",
    "nn(query, our_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trade's neareset neighbors:\n",
      "0.9401\tnafta\n",
      "0.9194\tdeficits\n",
      "0.9179\trestructuring\n",
      "0.9134\tliberalization\n",
      "0.9096\tchina_economics\n",
      "0.9086\tconsolidation\n",
      "0.9086\tthe_european_central_bank\n",
      "0.9083\texpansionary\n",
      "0.9081\tagreements\n",
      "0.9078\tcompetitiveness\n",
      "\n",
      "trade's neareset neighbors:\n",
      "0.4774\tagreements\n",
      "0.4137\tthe_trans_-_pacific_partnership\n",
      "0.3983\tnafta\n",
      "0.3968\tagreement\n",
      "0.3711\twto\n",
      "0.3511\tfree\n",
      "0.3454\tdeals\n",
      "0.3384\ttpp\n",
      "0.3326\tthe_world_trade_organization\n",
      "0.3273\tliberalization\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = 'trade'\n",
    "nn(query, sup_PE)\n",
    "nn(query, our_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bernie's neareset neighbors:\n",
      "0.9878\tjeb\n",
      "0.9877\tnewt\n",
      "0.9803\tloser\n",
      "0.9794\tmitt\n",
      "0.9709\tgopers\n",
      "0.9705\thuckabee\n",
      "0.9694\tbarack\n",
      "0.9684\tdems\n",
      "0.9658\tcavuto\n",
      "0.9646\tflop\n",
      "\n",
      "bernie's neareset neighbors:\n",
      "0.3841\tbernie_sanders\n",
      "0.3549\thillary\n",
      "0.3037\tsanders\n",
      "0.2437\tdelegate\n",
      "0.2394\tdelegates\n",
      "0.2379\topponent\n",
      "0.2339\tralph_nader\n",
      "0.2320\ttying\n",
      "0.2283\tdictate\n",
      "0.2234\tdnc\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = 'bernie'\n",
    "nn(query, sup_PE)\n",
    "nn(query, our_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = cono_space\n",
    "\n",
    "graph_en_masse(\n",
    "    models, out_dir=base_dir / 'cherry/party/t-SNE p5',\n",
    "    reduction='TSNE', perplexity=5, words=cherry_words, categorical=False)\n",
    "\n",
    "graph_en_masse(\n",
    "    models, out_dir=base_dir / 'cherry/party/t-SNE p3',\n",
    "    reduction='TSNE', perplexity=3, words=cherry_words, categorical=False)\n",
    "\n",
    "graph_en_masse(\n",
    "    models, out_dir=base_dir / 'cherry/party/t-SNE p2',\n",
    "    reduction='TSNE', perplexity=2, words=cherry_words, categorical=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = deno_space\n",
    "\n",
    "graph_en_masse(\n",
    "    models,\n",
    "    out_dir=f'{base_dir}/decomposed deno/party/t-SNE p25',\n",
    "    reduction='TSNE', perplexity=25,\n",
    "    word_ids=J_ids, words=J_words, hues=J_skew, sizes=J_freq)\n",
    "\n",
    "graph_en_masse(\n",
    "    models,\n",
    "    out_dir=f'{base_dir}/decomposed deno/party/t-SNE p50',\n",
    "    reduction='TSNE', perplexity=50,\n",
    "    word_ids=J_ids, words=J_words, hues=J_skew, sizes=J_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = cono_space\n",
    "\n",
    "graph_en_masse(\n",
    "    models, out_dir=f'{base_dir}/decomposed cono/topic/t-SNE p5',\n",
    "    reduction='TSNE', perplexity=5,\n",
    "    word_ids=J_ids, words=J_words, hues=J_deno, sizes=J_freq,\n",
    "    categorical=True)\n",
    "\n",
    "graph_en_masse(\n",
    "    models,\n",
    "    out_dir=f'{base_dir}/decomposed cono/topic/t-SNE p3',\n",
    "    reduction='TSNE', perplexity=3,\n",
    "    word_ids=J_ids, words=J_words, hues=J_deno, sizes=J_freq,\n",
    "    categorical=True)\n",
    "\n",
    "graph_en_masse(\n",
    "    models,\n",
    "    out_dir=f'{base_dir}/decomposed cono/topic/t-SNE p10',\n",
    "    reduction='TSNE', perplexity=10,\n",
    "    word_ids=J_ids, words=J_words, hues=J_deno, sizes=J_freq,\n",
    "    categorical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = cono_space\n",
    "\n",
    "# graph_en_masse(\n",
    "#     models,\n",
    "#     out_dir=f'{base_dir}/Joint/topic/PCA',\n",
    "#     reduction='PCA',\n",
    "#     word_ids=J_ids, words=J_words, hues=J_skew, sizes=J_freq)\n",
    "\n",
    "graph_en_masse(\n",
    "    models,\n",
    "    out_dir=f'{base_dir}/decomposed cono/party/t-SNE p25',\n",
    "    reduction='TSNE', perplexity=25,\n",
    "    word_ids=J_ids, words=J_words, hues=J_skew, sizes=J_freq)\n",
    "\n",
    "graph_en_masse(\n",
    "    models,\n",
    "    out_dir=f'{base_dir}/decomposed cono/party/t-SNE p50',\n",
    "    reduction='TSNE', perplexity=50,\n",
    "    word_ids=J_ids, words=J_words, hues=J_skew, sizes=J_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homogeneity V-Measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deno space, eval deno, higher is better\n",
    "for model_name, model in deno_space.items():\n",
    "    cluster_labels, true_labels = NN_cluster_ids(\n",
    "        model, J_ids, categorical=True, top_k=10)    \n",
    "    homogeneity, completeness, v_measure = np.around(\n",
    "        homogeneity_completeness_v_measure(true_labels, cluster_labels), 4)\n",
    "    print(model_name, homogeneity, completeness, v_measure, sep='\\t')\n",
    "#     print(pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deno space, eval cono, lower is better\n",
    "for model_name, model in deno_space.items():\n",
    "    cluster_labels, true_labels = NN_cluster_ids(\n",
    "        model, J_ids, categorical=False, top_k=5)    \n",
    "    homogeneity, completeness, v_measure = np.around(\n",
    "        homogeneity_completeness_v_measure(true_labels, cluster_labels), 4)\n",
    "    print(model_name, homogeneity, completeness, v_measure, sep='\\t')\n",
    "#     print(pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cono space, eval cono, higher is better\n",
    "for model_name, model in cono_space.items():\n",
    "    cluster_labels, true_labels = NN_cluster_ids(\n",
    "        model, J_ids, categorical=False, top_k=5)    \n",
    "    homogeneity, completeness, v_measure = np.around(\n",
    "        homogeneity_completeness_v_measure(true_labels, cluster_labels), 4)\n",
    "    print(model_name, homogeneity, completeness, v_measure, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cono space, eval deno, lower is better\n",
    "for model_name, model in cono_space.items():\n",
    "    cluster_labels, true_labels = NN_cluster_ids(\n",
    "        model, J_ids, categorical=True, top_k=5)    \n",
    "    homogeneity, completeness, v_measure = np.around(\n",
    "        homogeneity_completeness_v_measure(true_labels, cluster_labels), 4)\n",
    "    print(model_name, homogeneity, completeness, v_measure, sep='\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
