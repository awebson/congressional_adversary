{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "from typing import Set, Tuple, List, Dict\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import scipy\n",
    "from sklearn.metrics import pairwise\n",
    "from tqdm import tqdm\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "\n",
    "class SkipGramNegativeSampling():\n",
    "    \n",
    "    def __init__(self, source: str, path: str):\n",
    "        if source == 'pytorch':\n",
    "            self.init_from_pytorch(path)\n",
    "        elif source == 'plain_text':\n",
    "            self.init_from_plain_text(path)\n",
    "\n",
    "#     def from_pytorch(path: str):\n",
    "#         from embeddings.pretrain_embeddings import SkipGramNegativeSampling\n",
    "#         model = SkipGramNegativeSampling().load_state_dict(torch.load(model_path)) \n",
    "#         with open(path, 'rb') as model_file:\n",
    "#             state_dict = torch.load(model_file, map_location='cpu')\n",
    "#         print(state_dict.keys())\n",
    "#         embeddings = state_dict['center_embedding.weight'].numpy()\n",
    "#         assert False\n",
    "\n",
    "    def init_from_pytorch(self, paths: Tuple[str, str]) -> None:\n",
    "        model_path, vocab_path = paths\n",
    "        with open(model_path, 'rb') as model_file:\n",
    "            state_dict = torch.load(model_file, map_location='cpu')\n",
    "    #     print(state_dict.keys())\n",
    "        self.embedding = state_dict['center_embedding.weight'].numpy()\n",
    "        with open(vocab_path, 'rb') as vocab_file:\n",
    "            self.word_to_id, self.id_to_word, _ = pickle.load(vocab_file)\n",
    "\n",
    "    def init_from_plain_text(self, path: str) -> Tuple[np.array, Dict[str, int]]:\n",
    "        id_generator = 0\n",
    "        word_to_id: Dict[str, int] = {}\n",
    "        embeddings: List[float] = []\n",
    "        embedding_file = open(path)\n",
    "        vocab_size, num_dimensions = map(int, embedding_file.readline().split())\n",
    "        print(f'vocab_size = {vocab_size:,}, num_dimensions = {num_dimensions}')\n",
    "        print(f'Loading embeddings from {path}', flush=True)\n",
    "        for line in embedding_file:\n",
    "            line: List[str] = line.split()  # type: ignore\n",
    "            word = line[0]\n",
    "            vector = np.array(line[-num_dimensions:], dtype=np.float64)\n",
    "            embeddings.append(vector)\n",
    "            word_to_id[word] = id_generator\n",
    "            id_generator += 1\n",
    "        embedding_file.close()\n",
    "        print('Done')\n",
    "        self.id_to_word = {val: key for key, val in word_to_id.items()}\n",
    "        self.word_to_id = word_to_id\n",
    "        self.embedding = np.array(embeddings)\n",
    "\n",
    "    def cosine_similarity(self, query1: str, query2: str) -> float:\n",
    "        try:\n",
    "            query1_id = self.word_to_id[query1]\n",
    "        except KeyError:\n",
    "            print(f'{query1} is out of vocabulary. Sorry!')\n",
    "            return\n",
    "        try:\n",
    "            query2_id = self.word_to_id[query1]\n",
    "        except KeyError:\n",
    "            print(f'{query2} is out of vocabulary. Sorry!')\n",
    "            return\n",
    "        vectors = self.embedding[(query1_id, query2_id), :]\n",
    "        similarity = 1 - scipy.spatial.distance.cosine(vectors[0], vectors[1])\n",
    "        return similarity\n",
    "\n",
    "    def nearest_neighbor(self, query: str, top_k: int = 10):\n",
    "        try:\n",
    "            query_id = self.word_to_id[query]\n",
    "        except KeyError:\n",
    "            raise f'{query} is out of vocabulary. Sorry!'    \n",
    "        query_vec = self.embedding[query_id]\n",
    "        \n",
    "        distances = [scipy.spatial.distance.cosine(query_vec, vec) \n",
    "                     for vec in self.embedding]\n",
    "        neighbors = np.argsort(distances)\n",
    "        print(f\"{query}'s neareset neighbors:\")\n",
    "        for ranking in range(1, top_k + 1):\n",
    "            word_id = neighbors[ranking]\n",
    "            word = self.id_to_word[word_id]\n",
    "            cosine_similarity = 1 - distances[word_id]\n",
    "            print(f'{ranking}: {word}\\t\\t{cosine_similarity:.4f}')\n",
    "            if ranking > top_k:\n",
    "                break\n",
    "        print()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "decade_model = SkipGramNegativeSampling(\n",
    "    'pytorch',\n",
    "    ('../../results/skip_gram/decade_bs32anneal_1e-5/epoch10.pt',\n",
    "     '../../data/processed/skip_gram/decade_paper1e-5/vocab.pickle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "postwar_model = SkipGramNegativeSampling(\n",
    "    'pytorch',\n",
    "    ('../../results/skip_gram/postwar_1e-5/epoch10.pt',\n",
    "     '../../data/processed/skip_gram/postwar_paper1e-5/vocab.pickle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size = 109,123, num_dimensions = 300\n",
      "Loading embeddings from ../../results/baseline/word2vec_decade.txt\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "ref_decade_model = SkipGramNegativeSampling(\n",
    "    'plain_text', '../../results/baseline/word2vec_decade.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size = 109,123, num_dimensions = 300\n",
      "Loading embeddings from ../../results/baseline/word2vec_decade.txt\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "ref_postwar_model = SkipGramNegativeSampling(\n",
    "    'plain_text', '../../results/baseline/word2vec_decade.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unemployment_benefits's neareset neighbors:\n",
      "1: unemployment_insurance\t\t0.8851\n",
      "2: unemployed_workers\t\t0.8455\n",
      "3: unemployment_compensation\t\t0.8432\n",
      "4: extension_of_unemployment\t\t0.8398\n",
      "5: unemployment_insurance_benefits\t\t0.8396\n",
      "6: unemployed\t\t0.8355\n",
      "7: extend_unemployment_benefits\t\t0.8248\n",
      "8: longterm_unemployed\t\t0.7791\n",
      "9: jobless\t\t0.7768\n",
      "10: federal_unemployment_benefits\t\t0.7647\n",
      "11: looking_for_work\t\t0.7640\n",
      "12: extended_benefits\t\t0.7517\n",
      "13: extended_unemployment_benefits\t\t0.7513\n",
      "14: unemployment\t\t0.7480\n",
      "15: extending_unemployment_benefits\t\t0.7465\n",
      "16: extend_unemployment\t\t0.7429\n",
      "17: teuc\t\t0.7388\n",
      "18: outofwork\t\t0.7349\n",
      "19: exhausted_their_benefits\t\t0.7321\n",
      "20: euc\t\t0.7303\n",
      "21: extra_weeks\t\t0.7285\n",
      "22: lost_their_jobs\t\t0.7173\n",
      "23: extending_unemployment\t\t0.7171\n",
      "24: exhausted_their_unemployment\t\t0.7137\n",
      "25: find_work\t\t0.7095\n",
      "26: unemployed_americans\t\t0.7048\n",
      "27: exhaustees\t\t0.6915\n",
      "28: emergency_unemployment_compensation\t\t0.6915\n",
      "29: unemployment_benefitsand\t\t0.6873\n",
      "30: ui\t\t0.6805\n",
      "\n",
      "unemployment_benefits's neareset neighbors:\n",
      "1: unemployment_insurance\t\t0.8628\n",
      "2: unemployment_insurance_benefits\t\t0.8247\n",
      "3: unemployment_compensation\t\t0.8114\n",
      "4: federal_unemployment_benefits\t\t0.7499\n",
      "5: unemployed_workers\t\t0.7174\n",
      "6: extended_unemployment_benefits\t\t0.7084\n",
      "7: extended_benefits\t\t0.7066\n",
      "8: extension_of_unemployment\t\t0.7055\n",
      "9: extend_unemployment_benefits\t\t0.7027\n",
      "10: longterm_unemployed\t\t0.6917\n",
      "11: jobless\t\t0.6873\n",
      "12: unemployed\t\t0.6850\n",
      "13: teuc\t\t0.6624\n",
      "14: outofwork\t\t0.6474\n",
      "15: exhausted_their_benefits\t\t0.6445\n",
      "16: extending_unemployment_benefits\t\t0.6382\n",
      "17: euc\t\t0.6287\n",
      "18: unemployment\t\t0.6257\n",
      "19: weeks_of_benefits\t\t0.6223\n",
      "20: extra_weeks\t\t0.6190\n",
      "21: extend_unemployment\t\t0.6176\n",
      "22: insurance_benefits\t\t0.6143\n",
      "23: ui\t\t0.6120\n",
      "24: exhausted_their_unemployment\t\t0.6100\n",
      "25: unemployed_americans\t\t0.6073\n",
      "26: looking_for_work\t\t0.6001\n",
      "27: laidoff_workers\t\t0.5975\n",
      "28: extending_unemployment\t\t0.5905\n",
      "29: emergency_unemployment_compensation\t\t0.5841\n",
      "30: unemployment_benefitsand\t\t0.5716\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = ref_postwar_model\n",
    "decade_model.nearest_neighbor('unemployment_benefits', top_k=30)\n",
    "ref_decade_model.nearest_neighbor('unemployment_benefits', top_k=30)\n",
    "# model.nearest_neighbor('estate_tax')\n",
    "# model.nearest_neighbor('death_tax')\n",
    "# model.nearest_neighbor('undocumented_immigrants')\n",
    "# model.nearest_neighbor('illegal_aliens')\n",
    "# model.nearest_neighbor('illegal_aliens')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
