{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import csv\n",
    "import os\n",
    "from typing import Set, Tuple, NamedTuple, List, Dict, Counter, Optional\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "from scipy.stats import spearmanr\n",
    "import editdistance\n",
    "\n",
    "from evaluations.intrinsic_eval import Embedding, PhrasePair\n",
    "from bill_decomposer import Decomposer, DecomposerConfig\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "DEVICE = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heterogeneity_continuous(\n",
    "        model,\n",
    "        query_ids,\n",
    "        top_k: int = 10,\n",
    "        pretty_print: bool = True\n",
    "        ) -> float:\n",
    "    query_ids = query_ids.to(DEVICE)\n",
    "    query_embed = model.embedding(query_ids)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        top_neighbor_ids = [\n",
    "            nn.functional.cosine_similarity(\n",
    "                q.view(1, -1), model.embedding.weight).argsort(descending=True)\n",
    "            for q in query_embed]\n",
    "\n",
    "    homogeneity = []  # RENAME to hetero\n",
    "    inspect = []  # List of rows, where each row is [dist, query, n1, n2...]\n",
    "    \n",
    "    for query_index, sorted_target_indices in enumerate(top_neighbor_ids):\n",
    "        query_id = query_ids[query_index].item()\n",
    "        query_words = model.id_to_word[query_id]\n",
    "        num_neighbors = 0\n",
    "\n",
    "        query_R_ratio = model.R_ratio(query_words)\n",
    "        freq_ratio_distances = []\n",
    "        inspect.append([0, query_words])\n",
    "        \n",
    "        for sort_rank, target_id in enumerate(sorted_target_indices):\n",
    "            target_id = target_id.item()\n",
    "            if num_neighbors == top_k:\n",
    "                break\n",
    "            if query_id == target_id:\n",
    "                continue\n",
    "            target_words = model.id_to_word[target_id]\n",
    "            if editdistance.eval(query_words, target_words) < 3:\n",
    "                continue\n",
    "            num_neighbors += 1\n",
    "            target_R_ratio = model.R_ratio(target_words)\n",
    "            # freq_ratio_distances.append((target_R_ratio - query_R_ratio) ** 2)\n",
    "            freq_ratio_distances.append(abs(target_R_ratio - query_R_ratio))\n",
    "            \n",
    "            inspect[-1].append(target_words)\n",
    "            \n",
    "        inspect[-1][0] = np.mean(freq_ratio_distances)\n",
    "        \n",
    "        # homogeneity.append(np.sqrt(np.mean(freq_ratio_distances)))\n",
    "        homogeneity.append(np.mean(freq_ratio_distances))\n",
    "        \n",
    "#     inspect = random.sample(inspect, 100)\n",
    "\n",
    "    if pretty_print:\n",
    "        for i in sorted(inspect[:100], key=lambda t: t[0]):\n",
    "            print(f'{i[0]:.4f} {\"  \".join(i[1:6])}')\n",
    "        return np.mean(homogeneity)\n",
    "    else:\n",
    "        return inspect\n",
    "\n",
    "# def heterogeneity_continuous_compare(\n",
    "#         model,\n",
    "#         baseline_model,\n",
    "#         query_ids,\n",
    "#         top_k=10,\n",
    "#         sample=None,\n",
    "#         ) -> None:\n",
    "    \n",
    "#     for model_name, model in models.item():\n",
    "#         models[model_name]\n",
    "    \n",
    "#     table0 = heterogeneity_continuous(baseline_model, query_ids, pretty_print=False)\n",
    "#     table1 = heterogeneity_continuous(model, query_ids, pretty_print=False)\n",
    "    \n",
    "    \n",
    "#     def print_row(model_name, row):\n",
    "#         print(f'{row[0]:.4f} {\"\\t\".join(row[1:])}')\n",
    "        \n",
    "    \n",
    "#     for index, row in enumerate(table0):\n",
    "#         print('Pretrained:', end='\\t')\n",
    "#         print_row(row)\n",
    "#         print('Model:', end='\\t')\n",
    "#         print_row(table1[index])\n",
    "#         print()\n",
    "    \n",
    "    \n",
    "def heterogeneity_continuous_export(\n",
    "        models,\n",
    "        query_ids,\n",
    "        out_path,\n",
    "        top_k=10,\n",
    "        print_freq=False,\n",
    "        ) -> None:\n",
    "    tables = {}\n",
    "    for model_name, model in models.items():\n",
    "        table = heterogeneity_continuous(model, query_ids, pretty_print=False)\n",
    "        tables[model_name] = table\n",
    "    \n",
    "    m = models['pretrained']\n",
    "    def detail_freq(word):\n",
    "        combined_freq = m.Dem_frequency[word] + m.GOP_frequency[word]\n",
    "        ratio = m.R_ratio(word)\n",
    "        entry = f'{word}, {combined_freq}, {ratio:.2%}'   \n",
    "        return entry\n",
    "\n",
    "    table_len = len(tables['pretrained'])\n",
    "    with open(out_path, 'w') as file:\n",
    "        file.write('model\\tquery\\theterogeneity\\tn1\\tn2\\tn3\\tn4\\tn5\\tn6\\tn7\\tn8\\tn9\\tn10\\n')\n",
    "        for row_index in range(table_len):  # iter over queries\n",
    "            for model_name, table in tables.items():\n",
    "                row = table[row_index]\n",
    "                query = row[1]\n",
    "                hetero = f'{row[0]:.4f}'\n",
    "                neighbors = row[2:]\n",
    "                \n",
    "                query = detail_freq(query)\n",
    "                if print_freq:\n",
    "                    neighbors = [detail_freq(n) for n in neighbors]\n",
    "#                 else:        \n",
    "#                     m = models[model_name]\n",
    "#                     combined_freq = m.Dem_frequency[query] + m.GOP_frequency[query]\n",
    "#                     ratio = m.R_ratio(query)\n",
    "#                     query = f'{query}, {combined_freq}, {ratio:.2%}'\n",
    "\n",
    "                print(model_name, query, hetero, *neighbors, sep='\\t', file=file)\n",
    "            file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cherries = (\n",
    "    'military_budget', 'defense_budget',\n",
    "    # 'nuclear_option', 'constitutional_option',\n",
    "    'prochoice', # 'proabortion',\n",
    "    'star_wars', # 'strategic_defense_initiative',\n",
    "    'political_speech', 'campaign_spending',\n",
    "    'singlepayer', # 'socialized_medicine',\n",
    "    # 'voodoo', 'supplyside',\n",
    "    'tax_expenditures', # 'spending_programs',\n",
    "    'waterboarding', 'interrogation',\n",
    "    'cap_and_trade', 'national_energy_tax',\n",
    "    'governmentrun', 'public_option',\n",
    "    'medical_liability_reform', # 'tort_reform',\n",
    "    # 'corporate_profits', 'earnings',\n",
    "    'equal_pay',  # 'the_paycheck_fairness_act',\n",
    "    'military_spending', # 'washington_spending',\n",
    "    'higher_taxes', # 'bigger_government',\n",
    "    'social_justice', # 'womens_rights',\n",
    "    # 'national_health_insurance', # 'welfare_state', \n",
    "    'nuclear_war', 'deterrence',\n",
    "    'suffrage', # 'womens_rights',\n",
    "    'inequality', 'racism',\n",
    "    # 'sweatshops', 'factories',\n",
    "    'trickledown', 'cut_taxes',\n",
    "    'equal_pay', 'pay_discrimination',\n",
    "    'wealthiest_americans', 'tax_breaks',\n",
    "    'record_profits', 'big_oil_companies',\n",
    "    # 'private_insurance_companies', 'medicare_advantage_program',\n",
    "    'trickledown', # 'universal_health_care',\n",
    "    'big_banks', # 'occupation_of_iraq',\n",
    "    # 'obamacare', 'islamists'\n",
    ")\n",
    "\n",
    "generics = (\n",
    "    'government',\n",
    "    'taxes',\n",
    "    'laws',\n",
    "    'jobs',\n",
    "    'tariff',\n",
    "    'health_care',\n",
    "    'finance',\n",
    "    'social_security',\n",
    "    'medicare',\n",
    "    'regulations',\n",
    "    'immigration',\n",
    "    'research',\n",
    "    'technology',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../../results/pretrained/init.pt'\n",
    "pretrained_model = torch.load(path, map_location=DEVICE)['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../../results/cono space remove deno/L1 -0.05d/epoch50.pt'\n",
    "model = torch.load(path, map_location=DEVICE)['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.R_ratio('marriage_tax_penalty')\n",
    "# model.GOP_frequency['four_trillion']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cherries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "PM = pretrained_model\n",
    "cherry_ids = torch.tensor([PM.word_to_id[c] for c in cherries])\n",
    "generic_ids = torch.tensor([PM.word_to_id[c] for c in generics])\n",
    "\n",
    "select = torch.tensor(\n",
    "    [i.item() \n",
    "     for i in PM.GOP_ids \n",
    "     if PM.GOP_frequency[PM.id_to_word[i.item()]] >= 100])\n",
    "sample = torch.randint(high=len(select), size=(100,))\n",
    "GOP_ids = select[sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(path):\n",
    "    path = '../../results/' + path\n",
    "    return torch.load(path, map_location=DEVICE)['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'pretrained': load('pretrained/init.pt'),\n",
    "    'L1 -0.05d': load('cono space remove deno/L1 -0.05d/epoch50.pt'),\n",
    "    'L4 -0.05d': load('cono space remove deno/L4 -0.05d/epoch50.pt'),\n",
    "\n",
    "    'L4 +5 -0.05d': load('affine/L4 +5 -0.05d/epoch50.pt'),\n",
    "    'L4 +5 -0.1d': load('affine/L4 +5 -0.1d/epoch50.pt'),\n",
    "    'L4 +5 -0.2d': load('affine/L4 +5 -0.2d/epoch50.pt'),\n",
    "    'L4 +5 -0.5d': load('affine/L4 +5 -0.5d/epoch50.pt'),\n",
    "    'L4 +5 -1d': load('affine/L4 +5 -1d/epoch50.pt'),\n",
    "    'L4 +10 -1.5d': load('affine/L4 +10 -1.5d/epoch50.pt'),\n",
    "    'L4 +10 -2d': load('affine/L4 +10 -2d/epoch50.pt'),\n",
    "    'L4 +5 0c -1d': load('affine/L4 +5 0c/epoch50.pt'),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models = {\n",
    "#     'pretrained': load('pretrained/init.pt'),\n",
    "#     '1': load('analysis/retrained/L4 from L4 LLR/epoch10.pt'),\n",
    "#     '2': load('analysis/retrained/L4 from L4 LLR/epoch20.pt'),\n",
    "#     '3': load('analysis/retrained/L4 from L4 LLR/epoch30.pt'),\n",
    "#     '4': load('analysis/retrained/L4 from L4 LLR/epoch40.pt'),\n",
    "#     '5': load('analysis/retrained/L4 from L4 LLR/epoch50.pt'),\n",
    "#     '6': load('analysis/retrained/L4 from L4 LLR/epoch80.pt'),\n",
    "#     '7': load('analysis/retrained/L4 from L4 LLR/epoch100.pt'),\n",
    "#     '8': load('analysis/retrained/L4 from L4 LLR/epoch150.pt'),\n",
    "#     '9': load('analysis/retrained/L4 from L4 LLR/epoch200.pt'),\n",
    "#     '10': load('analysis/retrained/L4 from L4 LLR/epoch250.pt'),\n",
    "#     '11': load('analysis/retrained/L4 from L4 LLR/epoch300.pt'),\n",
    "#     '12': load('analysis/retrained/L4 from L4 LLR/epoch400.pt'),\n",
    "#     '13': load('analysis/retrained/L4 from L4 LLR/epoch500.pt'),\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "heterogeneity_continuous_export(\n",
    "    models, cherry_ids, '../../results/detail_cherry_neighborhood.tsv', print_freq=True)\n",
    "heterogeneity_continuous_export(\n",
    "    models, GOP_ids, '../../results/detail_GOP_neighborhood.tsv', print_freq=True)\n",
    "heterogeneity_continuous_export(\n",
    "    models, generic_ids, '../../results/detail_generic_neighborhood.tsv', print_freq=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "heterogeneity_continuous_export(\n",
    "    models, cherry_ids, '../../results/cherry_neighborhood.tsv')\n",
    "heterogeneity_continuous_export(\n",
    "    models, GOP_ids, '../../results/GOP_neighborhood.tsv')\n",
    "heterogeneity_continuous_export(\n",
    "    models, generic_ids, '../../results/generic_neighborhood.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partisan Skew > 80%\n",
    "Average difference between query R_ratio and neighbor R_ratios "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "heterogeneity_continuous(pretrained_model, model.GOP_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "heterogeneity_continuous(model, model.GOP_ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
