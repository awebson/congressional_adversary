{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import csv\n",
    "import os\n",
    "from typing import Set, Tuple, NamedTuple, List, Dict, Counter, Optional\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "from scipy.stats import spearmanr\n",
    "import editdistance\n",
    "\n",
    "from evaluations.intrinsic_eval import Embedding, PhrasePair\n",
    "from evaluations.intrinsic_eval import cherry_words, generic_words\n",
    "from decomposer import Decomposer, DecomposerConfig\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "DEVICE = torch.device('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54\n"
     ]
    }
   ],
   "source": [
    "PE = torch.load(\n",
    "    '../../results/pretrained/init.pt', map_location=DEVICE)['model']\n",
    "GD = PE.grounding\n",
    "cherry_ids = torch.tensor([PE.word_to_id[c] for c in cherry_words])\n",
    "generic_ids = torch.tensor([PE.word_to_id[c] for c in generic_words])\n",
    "\n",
    "GOP_words = [w for w in PE.word_to_id.keys()\n",
    "             if GD[w]['freq'] > 99 and GD[w]['R_ratio'] > 0.75]\n",
    "print(len(GOP_words))\n",
    "GOP_ids = torch.tensor([PE.word_to_id[w] for w in GOP_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heterogeneity_continuous(\n",
    "        model,\n",
    "        query_ids,\n",
    "        top_k: int = 10,\n",
    "        pretty_print: bool = True\n",
    "        ) -> float:\n",
    "    query_ids = query_ids.to(DEVICE)\n",
    "    query_embed = model.embedding(query_ids)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        top_neighbor_ids = [\n",
    "            nn.functional.cosine_similarity(\n",
    "                q.view(1, -1), model.embedding.weight).argsort(descending=True)\n",
    "            for q in query_embed]\n",
    "\n",
    "    homogeneity = []  # RENAME to hetero\n",
    "    inspect = []  # List of rows, where each row is [dist, query, n1, n2...]\n",
    "    \n",
    "    for query_index, sorted_target_indices in enumerate(top_neighbor_ids):\n",
    "        query_id = query_ids[query_index].item()\n",
    "        query_words = model.id_to_word[query_id]\n",
    "        num_neighbors = 0\n",
    "\n",
    "        query_R_ratio = GD[query_words]['R_ratio']\n",
    "        freq_ratio_distances = []\n",
    "        inspect.append([0, query_words])\n",
    "        \n",
    "        for sort_rank, target_id in enumerate(sorted_target_indices):\n",
    "            target_id = target_id.item()\n",
    "            if num_neighbors == top_k:\n",
    "                break\n",
    "            if query_id == target_id:\n",
    "                continue\n",
    "            target_words = model.id_to_word[target_id]\n",
    "            if editdistance.eval(query_words, target_words) < 3:\n",
    "                continue\n",
    "            num_neighbors += 1\n",
    "            target_R_ratio = GD[target_words]['R_ratio']\n",
    "            # freq_ratio_distances.append((target_R_ratio - query_R_ratio) ** 2)\n",
    "            freq_ratio_distances.append(abs(target_R_ratio - query_R_ratio))\n",
    "            \n",
    "            inspect[-1].append(target_words)\n",
    "            \n",
    "        inspect[-1][0] = np.mean(freq_ratio_distances)\n",
    "        \n",
    "        # homogeneity.append(np.sqrt(np.mean(freq_ratio_distances)))\n",
    "        homogeneity.append(np.mean(freq_ratio_distances))\n",
    "        \n",
    "#     inspect = random.sample(inspect, 100)\n",
    "\n",
    "    if pretty_print:\n",
    "        for i in sorted(inspect[:100], key=lambda t: t[0]):\n",
    "            print(f'{i[0]:.4f} {\"  \".join(i[1:6])}')\n",
    "        return np.mean(homogeneity)\n",
    "    else:\n",
    "        return inspect\n",
    "\n",
    "# def heterogeneity_continuous_compare(\n",
    "#         model,\n",
    "#         baseline_model,\n",
    "#         query_ids,\n",
    "#         top_k=10,\n",
    "#         sample=None,\n",
    "#         ) -> None:\n",
    "    \n",
    "#     for model_name, model in models.item():\n",
    "#         models[model_name]\n",
    "    \n",
    "#     table0 = heterogeneity_continuous(baseline_model, query_ids, pretty_print=False)\n",
    "#     table1 = heterogeneity_continuous(model, query_ids, pretty_print=False)\n",
    "    \n",
    "    \n",
    "#     def print_row(model_name, row):\n",
    "#         print(f'{row[0]:.4f} {\"\\t\".join(row[1:])}')\n",
    "        \n",
    "    \n",
    "#     for index, row in enumerate(table0):\n",
    "#         print('Pretrained:', end='\\t')\n",
    "#         print_row(row)\n",
    "#         print('Model:', end='\\t')\n",
    "#         print_row(table1[index])\n",
    "#         print()\n",
    "    \n",
    "    \n",
    "def heterogeneity_continuous_export(\n",
    "        models,\n",
    "        query_ids,\n",
    "        out_path,\n",
    "        top_k=10,\n",
    "        print_freq=False,\n",
    "        ) -> None:\n",
    "    tables = {}\n",
    "    for model_name, model in models.items():\n",
    "        table = heterogeneity_continuous(model, query_ids, pretty_print=False)\n",
    "        tables[model_name] = table\n",
    "    \n",
    "    def detail_freq(word):\n",
    "        combined_freq = GD[word]['freq']\n",
    "        ratio = GD[word]['R_ratio']\n",
    "        entry = f'{word}, {combined_freq}, {ratio:.2%}'   \n",
    "        return entry\n",
    "\n",
    "    first_table = tuple(tables.keys())[0]\n",
    "    table_len = len(tables[first_table])\n",
    "    with open(out_path, 'w') as file:\n",
    "        file.write('model\\tquery\\theterogeneity\\tn1\\tn2\\tn3\\tn4\\tn5\\tn6\\tn7\\tn8\\tn9\\tn10\\n')\n",
    "        for row_index in range(table_len):  # iter over queries\n",
    "            for model_name, table in tables.items():\n",
    "                row = table[row_index]\n",
    "                query = row[1]\n",
    "                hetero = f'{row[0]:.4f}'\n",
    "                neighbors = row[2:]\n",
    "                \n",
    "                query = detail_freq(query)\n",
    "                if print_freq:\n",
    "                    neighbors = [detail_freq(n) for n in neighbors]\n",
    "#                 else:        \n",
    "#                     m = models[model_name]\n",
    "#                     combined_freq = m.Dem_frequency[query] + m.GOP_frequency[query]\n",
    "#                     ratio = m.R_ratio(query)\n",
    "#                     query = f'{query}, {combined_freq}, {ratio:.2%}'\n",
    "\n",
    "                print(model_name, query, hetero, *neighbors, sep='\\t', file=file)\n",
    "            file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models = {\n",
    "#     'pretrained': load('pretrained/init.pt'),\n",
    "#     '1': load('analysis/retrained/L4 from L4 LLR/epoch10.pt'),\n",
    "#     '2': load('analysis/retrained/L4 from L4 LLR/epoch20.pt'),\n",
    "#     '3': load('analysis/retrained/L4 from L4 LLR/epoch30.pt'),\n",
    "#     '4': load('analysis/retrained/L4 from L4 LLR/epoch40.pt'),\n",
    "#     '5': load('analysis/retrained/L4 from L4 LLR/epoch50.pt'),\n",
    "#     '6': load('analysis/retrained/L4 from L4 LLR/epoch80.pt'),\n",
    "#     '7': load('analysis/retrained/L4 from L4 LLR/epoch100.pt'),\n",
    "#     '8': load('analysis/retrained/L4 from L4 LLR/epoch150.pt'),\n",
    "#     '9': load('analysis/retrained/L4 from L4 LLR/epoch200.pt'),\n",
    "#     '10': load('analysis/retrained/L4 from L4 LLR/epoch250.pt'),\n",
    "#     '11': load('analysis/retrained/L4 from L4 LLR/epoch300.pt'),\n",
    "#     '12': load('analysis/retrained/L4 from L4 LLR/epoch400.pt'),\n",
    "#     '13': load('analysis/retrained/L4 from L4 LLR/epoch500.pt'),\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(path):\n",
    "    path = '../../results/' + path\n",
    "    return torch.load(path, map_location=DEVICE)['model']\n",
    "\n",
    "models = {\n",
    "    'pretrained': load('pretrained/init.pt'),\n",
    "    'L1 -0.05d': load('cono space remove deno/L1 -0.05d/epoch50.pt'),\n",
    "    'L4 -0.05d': load('cono space remove deno/L4 -0.05d/epoch50.pt'),\n",
    "\n",
    "    'L4 +5 -0.05d': load('affine/L4 +5 -0.05d/epoch50.pt'),\n",
    "    'L4 +5 -0.1d': load('affine/L4 +5 -0.1d/epoch50.pt'),\n",
    "    'L4 +5 -0.2d': load('affine/L4 +5 -0.2d/epoch50.pt'),\n",
    "    'L4 +5 -0.5d': load('affine/L4 +5 -0.5d/epoch50.pt'),\n",
    "    'L4 +5 -1d': load('affine/L4 +5 -1d/epoch50.pt'),\n",
    "    'L4 +10 -1.5d': load('affine/L4 +10 -1.5d/epoch50.pt'),\n",
    "    'L4 +10 -2d': load('affine/L4 +10 -2d/epoch50.pt'),\n",
    "    'L4 +5 0c -1d': load('affine/L4 +5 0c/epoch50.pt'),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heterogeneity_continuous_export(\n",
    "    models, cherry_ids, '../../results/detail_cherry_neighborhood.tsv', print_freq=True)\n",
    "heterogeneity_continuous_export(\n",
    "    models, GOP_ids, '../../results/detail_GOP_neighborhood.tsv', print_freq=True)\n",
    "heterogeneity_continuous_export(\n",
    "    models, generic_ids, '../../results/detail_generic_neighborhood.tsv', print_freq=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heterogeneity_continuous_export(\n",
    "    models, cherry_ids, '../../results/cherry_neighborhood.tsv')\n",
    "heterogeneity_continuous_export(\n",
    "    models, GOP_ids, '../../results/GOP_neighborhood.tsv')\n",
    "heterogeneity_continuous_export(\n",
    "    models, generic_ids, '../../results/generic_neighborhood.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confirm_invariant(in_dir, out_path):\n",
    "    models = {}\n",
    "    for dirpath, _, filenames in os.walk(in_dir):\n",
    "        for file in filenames:\n",
    "            if file.endswith('epoch30.pt'):\n",
    "                path = os.path.join(dirpath, file)\n",
    "                name = path.lstrip(in_dir)\n",
    "                models[name] = torch.load(\n",
    "                    path, map_location=DEVICE)['model']\n",
    "    heterogeneity_continuous_export(models, cherry_ids, out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "confirm_invariant(\n",
    "    in_dir='../../results/batch sizes', \n",
    "    out_path='../../analysis/confirm_invariant.tsv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
