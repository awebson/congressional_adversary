{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm import tqdm\n",
    "\n",
    "from evaluations.intrinsic_eval import cherry_words, generic_words\n",
    "from decomposer import Decomposer, DecomposerConfig\n",
    "\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "sns.set()\n",
    "\n",
    "DEVICE = 'cpu'\n",
    "PE = torch.load(\n",
    "    '../../results/pretrained/init.pt', map_location=DEVICE)['model']\n",
    "GD = PE.grounding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(path):\n",
    "    stuff = torch.load(path, map_location=DEVICE)['model']\n",
    "    return stuff.embedding.weight.detach().numpy()\n",
    "\n",
    "def gather(words):\n",
    "    word_ids = [PE.word_to_id[w] for w in words]\n",
    "    freq = [GD[w]['freq'] for w in words]\n",
    "    skew = [GD[w]['R_ratio'] for w in words]\n",
    "    maj_deno = [GD[w]['majority_deno'] for w in words]\n",
    "    return word_ids, freq, skew, maj_deno\n",
    "\n",
    "def plot(coordinates, words, freq, skew, path):\n",
    "    fig, ax = plt.subplots(figsize=(15,10))    \n",
    "    sns.scatterplot(\n",
    "        coordinates[:,0], coordinates[:,1], \n",
    "        hue=skew, palette='coolwarm', # hue_norm=(0, 1), \n",
    "        size=freq, sizes=(100, 1000), \n",
    "        legend=None, ax=ax)\n",
    "    for coord, word in zip(coordinates, words):\n",
    "        ax.annotate(word, coord, fontsize=12)\n",
    "    with open(path, 'wb') as file:\n",
    "        fig.savefig(file, dpi=300)\n",
    "    plt.close(fig)\n",
    "\n",
    "def plot_categorical(coordinates, words, freq, skew, path):\n",
    "    fig, ax = plt.subplots(figsize=(20,10))    \n",
    "    sns.scatterplot(\n",
    "        coordinates[:,0], coordinates[:,1], \n",
    "        hue=skew, palette='muted', hue_norm=(0, 1),\n",
    "        size=freq, sizes=(100, 1000), \n",
    "        legend='brief', ax=ax)\n",
    "    chartBox = ax.get_position()\n",
    "    ax.set_position([chartBox.x0, chartBox.y0, chartBox.width*0.6, chartBox.height])\n",
    "    ax.legend(loc='upper center', bbox_to_anchor=(1.45, 0.8), ncol=1)\n",
    "    for coord, word in zip(coordinates, words):\n",
    "        ax.annotate(word, coord, fontsize=12)\n",
    "    with open(path, 'wb') as file:\n",
    "        fig.savefig(file, dpi=300)\n",
    "    plt.close(fig)\n",
    "    \n",
    "def load_en_masse(in_dir, endswith):\n",
    "    models = {}\n",
    "    for dirpath, _, filenames in tqdm(os.walk(in_dir)):\n",
    "        for file in filenames:\n",
    "            if file.endswith(endswith):\n",
    "                path = os.path.join(dirpath, file)\n",
    "                name = path.lstrip(in_dir).replace('/', ' ')\n",
    "                models[name] = load(path)\n",
    "    print(*models.keys(), sep='\\n')\n",
    "    return models\n",
    "    \n",
    "def graph_en_masse(\n",
    "        in_dir,\n",
    "        out_dir, \n",
    "        reduction,  #  'PCA', 'TSNE', or 'both'\n",
    "        word_ids,  \n",
    "        words, \n",
    "        hues,\n",
    "        sizes,\n",
    "        perplexity=None,\n",
    "        categorical=False):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    for model_name, embed in tqdm(models.items()):\n",
    "        space = embed[word_ids]\n",
    "        if reduction == 'PCA':\n",
    "            visual = PCA(n_components=2).fit_transform(space)\n",
    "        elif reduction == 'TSNE':\n",
    "            assert perplexity is not None\n",
    "            visual = TSNE(\n",
    "                perplexity=perplexity, learning_rate=10, \n",
    "                n_iter=5000, n_iter_without_progress=1000).fit_transform(space)\n",
    "        elif reduction == 'both':\n",
    "            assert perplexity is not None\n",
    "            space = PCA(n_components=30).fit_transform(space)\n",
    "            visual = TSNE(\n",
    "                perplexity=perplexity, learning_rate=10, \n",
    "                n_iter=5000, n_iter_without_progress=1000).fit_transform(space)\n",
    "        else: \n",
    "            raise ValueError('unknown dimension reduction method')\n",
    "        if not categorical:\n",
    "            plot(visual, words, sizes, hues, \n",
    "                 os.path.join(out_dir, f'{model_name}.png'))\n",
    "        else:\n",
    "            plot_categorical(visual, words, sizes, hues, \n",
    "                 os.path.join(out_dir, f'{model_name}.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51\n"
     ]
    }
   ],
   "source": [
    "ch_ids, ch_freq, ch_skew, ch_deno = gather(cherry_words)\n",
    "gen_ids, gen_freq, gen_skew, gen_deno = gather(generic_words)\n",
    "\n",
    "random_words = [w for w in PE.word_to_id.keys() \n",
    "                if GD[w]['freq'] > 99]\n",
    "random_words = random.sample(random_words, 50)\n",
    "rand_ids, rand_freq, rand_skew, rand_deno = gather(random_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "R_words = [w for w in PE.word_to_id.keys()\n",
    "             if GD[w]['freq'] > 99 and GD[w]['R_ratio'] > 0.75]\n",
    "R_words.remove('federal_debt_stood')  # outliers in clustering graphs\n",
    "R_words.remove('statements_relating')\n",
    "R_words.remove('legislative_days_within')\n",
    "print(len(GOP_words))\n",
    "# GOP_words = random.sample(GOP_words, 50)\n",
    "R_ids, R_freq, R_skew, R_deno = gather(R_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81\n"
     ]
    }
   ],
   "source": [
    "# D_words = [w for w in PE.word_to_id.keys()\n",
    "#            if GD[w]['freq'] > 99 and GD[w]['R_ratio'] < 0.25]\n",
    "\n",
    "D_words = ['war_in_iraq', 'unemployed', 'detainees', 'solar', \n",
    "    'wealthiest', 'minorities', 'gun_violence', \n",
    "    'amtrak', 'unemployment_benefits', \n",
    "    'citizens_united', 'mayors', 'prosecutor', 'working_families', \n",
    "    'cpsc', 'sexual_assault',\n",
    "    'affordable_housing', 'vietnam_veterans', 'drug_companies', 'handguns',\n",
    "    'hungry', 'college_education', \n",
    "    'main_street', 'trauma', 'simon', 'pandemic', \n",
    "    'reagan_administration', 'guns', \n",
    "    'million_jobs', 'airline_industry', 'mergers', 'blacks', \n",
    "    'industrial_base', 'unemployment_insurance',\n",
    "    'vacancies', 'trade_deficit', 'lost_their_jobs', 'food_safety', \n",
    "    'darfur', 'trains', 'deportation', 'credit_cards', \n",
    "    'surface_transportation', 'solar_energy', 'ecosystems', 'layoffs', \n",
    "    'wall_street', 'steelworkers', 'puerto_rico', 'hunger', \n",
    "    'child_support', 'naacp', 'domestic_violence', 'seaports', \n",
    "    'hate_crimes', 'underfunded', 'registrants', 'sanctuary', \n",
    "    'coastal_zone_management', 'vermonters', 'automakers', \n",
    "    'violence_against_women', 'unemployment_rate', \n",
    "    'select_committee_on_indian_affairs', 'judicial_nominees', \n",
    "    'school_construction', 'clarence_mitchell', 'confidential', \n",
    "    'domain_name', 'community_development', 'pell_grant', 'asylum', 'vawa', \n",
    "    'somalia', 'african_american', 'traders', 'jersey', 'fdic', 'shameful', \n",
    "    'homelessness', 'african_americans', 'payroll_tax',]\n",
    "#     'retraining', 'unemployed_workers', 'the_disclose_act', 'baltimore', \n",
    "#     'assault_weapons', 'credit_card', 'the_patriot_act', 'young_woman', \n",
    "#     'trades', 'aye', 'poisoning', 'police_officers', 'mammal', 'toys', \n",
    "#     'whistleblowers', 'north_dakota', 'californias', 'computer_crime', \n",
    "#     'explosives', 'fast_track', 'bus', 'redlining', 'seclusion', 'gender', \n",
    "#     'hawaiian', 'pay_discrimination', 'ledbetter', 'phd', 'supra', 'baggage', \n",
    "#     'las_vegas', 'the_voting_rights_act', 'enron', 'richest', 'vra', 'chip', \n",
    "#     'tax_break', 'the_usa_patriot_act', 'advance_notice', 'derivatives', \n",
    "#     'the_patients_bill_of_rights', 'shelf', 'divestment', 'sa', \n",
    "#     'submitted_an_amendment', 'bill_hr', 'first_responders',\n",
    "#     'unemployment_compensation', 'tax_breaks', 'carbon', \n",
    "#     'college_cost_reduction', 'clean_energy', 'waives', \n",
    "#     'unregulated', 'taa', 'truman', 'lesbian', 'coupons', \n",
    "#     'large_numbers', 'anonymous', 'whites', 'logging']\n",
    "\n",
    "print(len(D_words))\n",
    "D_words = random.sample(D_words, 50)\n",
    "D_ids, D_freq, D_skew, D_deno = gather(D_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "J_words = D_words + R_words\n",
    "J_ids = D_ids + R_ids\n",
    "J_freq = D_freq + R_freq\n",
    "J_skew = D_skew + R_skew\n",
    "J_deno = D_deno + R_deno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'Labor and employment': 6,\n",
       "         'R': 105,\n",
       "         'Taxation': 87,\n",
       "         'Law': 2,\n",
       "         'D': 7,\n",
       "         'Health': 3,\n",
       "         'Public lands and natural resources': 1,\n",
       "         'International affairs': 5,\n",
       "         'Transportation and public works': 2,\n",
       "         'Science, technology, communications': 2,\n",
       "         'Families': 2,\n",
       "         'Crime and law enforcement': 1,\n",
       "         'Education': 1,\n",
       "         'majority_deno': 'Taxation',\n",
       "         'freq': 112,\n",
       "         'R_ratio': 0.9375})"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GD['joliet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21it [00:03,  5.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E1 A1 epoch10.pt\n",
      "E4 A1 superset epoch10.pt\n",
      "E4 A1 epoch10.pt\n",
      "E1 A1 superset epoch10.pt\n"
     ]
    }
   ],
   "source": [
    "# base_dir = '../../results/only remove deno BS128'\n",
    "base_dir = '../../results/cono space remove deno'\n",
    "# base_dir = '../../results/deno space remove cono'\n",
    "models = load_en_masse(base_dir, endswith='epoch10.pt')\n",
    "models['pretrained superset'] = load('../../results/pretrained/init.pt')\n",
    "models['pretrained'] = load('../../results/pretrained bill mentions/init.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph by Party Skew (for removing connotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:02<00:00,  1.36s/it]\n",
      "100%|██████████| 2/2 [00:03<00:00,  1.84s/it]\n",
      "100%|██████████| 2/2 [00:04<00:00,  2.12s/it]\n"
     ]
    }
   ],
   "source": [
    "graph_en_masse(\n",
    "    models,\n",
    "    out_dir=f'{base_dir}/PCA',\n",
    "    reduction='PCA',\n",
    "    word_ids=R_ids,\n",
    "    words=R_words,\n",
    "    hues=R_skew,\n",
    "    sizes=R_freq,\n",
    ")\n",
    "\n",
    "graph_en_masse(\n",
    "    models,\n",
    "    out_dir=f'{base_dir}/t-SNE p5',\n",
    "    reduction='TSNE',\n",
    "    perplexity=5,\n",
    "    word_ids=R_ids,\n",
    "    words=R_words,\n",
    "    hues=R_skew,\n",
    "    sizes=R_freq,\n",
    ")\n",
    "\n",
    "graph_en_masse(\n",
    "    models,\n",
    "    out_dir=f'{base_dir}/t-SNE p3',\n",
    "    reduction='TSNE',\n",
    "    perplexity=3,\n",
    "    word_ids=R_ids,\n",
    "    words=R_words,\n",
    "    hues=R_skew,\n",
    "    sizes=R_freq,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:08<00:00,  1.36s/it]\n",
      "100%|██████████| 6/6 [00:14<00:00,  2.35s/it]\n",
      "100%|██████████| 6/6 [00:14<00:00,  2.35s/it]\n"
     ]
    }
   ],
   "source": [
    "graph_en_masse(\n",
    "    models,\n",
    "    out_dir=f'{base_dir}/PCA',\n",
    "    reduction='PCA',\n",
    "    word_ids=J_ids,\n",
    "    words=J_words,\n",
    "    hues=J_skew,\n",
    "    sizes=J_freq,\n",
    ")\n",
    "\n",
    "graph_en_masse(\n",
    "    models,\n",
    "    out_dir=f'{base_dir}/t-SNE p5',\n",
    "    reduction='TSNE',\n",
    "    perplexity=5,\n",
    "    word_ids=J_ids,\n",
    "    words=J_words,\n",
    "    hues=J_skew,\n",
    "    sizes=J_freq,\n",
    ")\n",
    "\n",
    "graph_en_masse(\n",
    "    models,\n",
    "    out_dir=f'{base_dir}/t-SNE p3',\n",
    "    reduction='TSNE',\n",
    "    perplexity=3,\n",
    "    word_ids=J_ids,\n",
    "    words=J_words,\n",
    "    hues=J_skew,\n",
    "    sizes=J_freq,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph by Topic Denotation (for removing denotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:09<00:00,  1.86s/it]\n",
      "100%|██████████| 5/5 [00:11<00:00,  2.37s/it]\n",
      "100%|██████████| 5/5 [00:12<00:00,  2.46s/it]\n"
     ]
    }
   ],
   "source": [
    "graph_en_masse(\n",
    "    models,\n",
    "    out_dir=f'{base_dir}/Highly GOP/PCA',\n",
    "    reduction='PCA',\n",
    "    perplexity=5,\n",
    "    word_ids=GOP_ids,\n",
    "    words=GOP_words,\n",
    "    hues=GOP_deno,\n",
    "    sizes=GOP_freq,\n",
    "    categorical=True\n",
    ")\n",
    "\n",
    "graph_en_masse(\n",
    "    models,\n",
    "    out_dir=f'{base_dir}/Highly GOP/t-SNE p5',\n",
    "    reduction='TSNE',\n",
    "    perplexity=5,\n",
    "    word_ids=GOP_ids,\n",
    "    words=GOP_words,\n",
    "    hues=GOP_deno,\n",
    "    sizes=GOP_freq,\n",
    "    categorical=True\n",
    ")\n",
    "\n",
    "graph_en_masse(\n",
    "    models,\n",
    "    out_dir=f'{base_dir}/Highly GOP/t-SNE p3',\n",
    "    reduction='TSNE',\n",
    "    perplexity=3,\n",
    "    word_ids=GOP_ids,\n",
    "    words=GOP_words,\n",
    "    hues=GOP_deno,\n",
    "    sizes=GOP_freq,\n",
    "    categorical=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:09<00:00,  1.91s/it]\n",
      "100%|██████████| 5/5 [00:12<00:00,  2.47s/it]\n",
      "100%|██████████| 5/5 [00:12<00:00,  2.41s/it]\n"
     ]
    }
   ],
   "source": [
    "graph_en_masse(\n",
    "    models,\n",
    "    out_dir=f'{base_dir}/Highly Dem/PCA',\n",
    "    reduction='PCA',\n",
    "    perplexity=5,\n",
    "    word_ids=D_ids,\n",
    "    words=D_words,\n",
    "    hues=D_deno,\n",
    "    sizes=D_freq,\n",
    "    categorical=True\n",
    ")\n",
    "\n",
    "graph_en_masse(\n",
    "    models,\n",
    "    out_dir=f'{base_dir}/Highly Dem/t-SNE p5',\n",
    "    reduction='TSNE',\n",
    "    perplexity=5,\n",
    "    word_ids=D_ids,\n",
    "    words=D_words,\n",
    "    hues=D_deno,\n",
    "    sizes=D_freq,\n",
    "    categorical=True\n",
    ")\n",
    "\n",
    "graph_en_masse(\n",
    "    models,\n",
    "    out_dir=f'{base_dir}/Highly Dem/t-SNE p3',\n",
    "    reduction='TSNE',\n",
    "    perplexity=3,\n",
    "    word_ids=D_ids,\n",
    "    words=D_words,\n",
    "    hues=D_deno,\n",
    "    sizes=D_freq,\n",
    "    categorical=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:09<00:00,  1.94s/it]\n",
      "100%|██████████| 5/5 [00:15<00:00,  3.07s/it]\n",
      "100%|██████████| 5/5 [00:15<00:00,  3.02s/it]\n"
     ]
    }
   ],
   "source": [
    "graph_en_masse(\n",
    "    models,\n",
    "    out_dir=f'{base_dir}/Joint/PCA',\n",
    "    reduction='PCA',\n",
    "    perplexity=5,\n",
    "    word_ids=J_ids,\n",
    "    words=J_words,\n",
    "    hues=J_deno,\n",
    "    sizes=J_freq,\n",
    "    categorical=True\n",
    ")\n",
    "\n",
    "graph_en_masse(\n",
    "    models,\n",
    "    out_dir=f'{base_dir}/Joint/t-SNE p5',\n",
    "    reduction='TSNE',\n",
    "    perplexity=5,\n",
    "    word_ids=J_ids,\n",
    "    words=J_words,\n",
    "    hues=J_deno,\n",
    "    sizes=J_freq,\n",
    "    categorical=True\n",
    ")\n",
    "\n",
    "graph_en_masse(\n",
    "    models,\n",
    "    out_dir=f'{base_dir}/Joint/t-SNE p3',\n",
    "    reduction='TSNE',\n",
    "    perplexity=3,\n",
    "    word_ids=J_ids,\n",
    "    words=J_words,\n",
    "    hues=J_deno,\n",
    "    sizes=J_freq,\n",
    "    categorical=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "out_dir = '../../analysis/PCA/cherry/topic_live'\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "for model_name, embed in models.items():\n",
    "    space = embed[ch_ids]\n",
    "    visual = PCA(n_components=2).fit_transform(space)\n",
    "#     visual = TSNE(\n",
    "#         perplexity=4, learning_rate=10, \n",
    "#         n_iter=5000, n_iter_without_progress=1000).fit_transform(visual)\n",
    "    plot(visual, cherry_words, ch_freq, ch_deno, \n",
    "         os.path.join(out_dir, f'{model_name}.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = '../../analysis/t-SNE'\n",
    "for model_name, embed in models.items():\n",
    "    space = embed[generic_ids]\n",
    "    visual = TSNE(perplexity=5, learning_rate=1).fit_transform(space)\n",
    "    plot(visual, generic_words, generic_freq, generic_skew, \n",
    "         f'{out_dir}/generic {model_name}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = '../../analysis/t-SNE'\n",
    "for model_name, embed in models.items():\n",
    "    space = embed[random_ids]\n",
    "    visual = TSNE(\n",
    "        perplexity=20, learning_rate=10, n_iter=5000).fit_transform(space)\n",
    "    plot(visual, random_words, random_freq, random_skew, \n",
    "         f'{out_dir}/random {model_name}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'M0 pretrained': load('pretrained/init.pt'),\n",
    "    'M1 L1 -0.05d': load('cono space remove deno/L1 -0.05d/epoch50.pt'),\n",
    "    'M2 L4 -0.05d': load('cono space remove deno/L4 -0.05d/epoch50.pt'),\n",
    "    'M3 +5 -0.05d': load('affine/L4 +5 -0.05d/epoch50.pt'),\n",
    "    'M4 +5 -0.1d': load('affine/L4 +5 -0.1d/epoch50.pt'),\n",
    "    'M5 +5 -0.2d': load('affine/L4 +5 -0.2d/epoch50.pt'),\n",
    "    'M6 +5 -0.5d': load('affine/L4 +5 -0.5d/epoch50.pt'),\n",
    "    'M7 +5 -1d': load('affine/L4 +5 -1d/epoch50.pt'),\n",
    "    'M8 +10 -1.5d': load('affine/L4 +10 -1.5d/epoch50.pt'),\n",
    "    'M9 +10 -2d': load('affine/L4 +10 -2d/epoch50.pt'),\n",
    "    'M10 +5 0c -1d': load('affine/L4 +5 0c/epoch50.pt'),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'all': load('../../results/pretrained/init.pt'),\n",
    "    'bill mentioned': load('../../results/pretrained bill mentions/init.pt')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "out_dir = '../../analysis/pretrained/t-SNE p5'\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "for model_name, embed in models.items():\n",
    "    space = embed[ch_ids]\n",
    "    visual = TSNE(perplexity=5, learning_rate=1).fit_transform(space)\n",
    "    plot(visual, cherry_words, ch_freq, ch_skew, \n",
    "         f'{out_dir}/cherry {model_name}.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
