{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm import tqdm\n",
    "\n",
    "from evaluations.intrinsic_eval import cherry_words, generic_words\n",
    "from decomposer import Decomposer, DecomposerConfig\n",
    "\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "sns.set()\n",
    "\n",
    "DEVICE = 'cpu'\n",
    "PE = torch.load(\n",
    "    '../../results/pretrained/init.pt', map_location=DEVICE)['model']\n",
    "GD = PE.grounding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(path):\n",
    "    stuff = torch.load(path, map_location=DEVICE)['model']\n",
    "    return stuff.embedding.weight.detach().numpy()\n",
    "\n",
    "def gather(words):\n",
    "    word_ids = [PE.word_to_id[w] for w in words]\n",
    "    freq = [GD[w]['freq'] for w in words]\n",
    "    skew = [GD[w]['R_ratio'] for w in words]\n",
    "    maj_deno = [GD[w]['majority_deno'] for w in words]\n",
    "    return word_ids, freq, skew, maj_deno\n",
    "\n",
    "def plot(coordinates, words, freq, skew, path):\n",
    "    fig, ax = plt.subplots(figsize=(15,10))    \n",
    "    sns.scatterplot(\n",
    "        coordinates[:,0], coordinates[:,1], \n",
    "        hue=skew, palette='coolwarm', # hue_norm=(0, 1), \n",
    "        size=freq, sizes=(100, 1000), \n",
    "        legend=None, ax=ax)\n",
    "    for coord, word in zip(coordinates, words):\n",
    "        ax.annotate(word, coord, fontsize=12)\n",
    "    with open(path, 'wb') as file:\n",
    "        fig.savefig(file, dpi=300)\n",
    "    plt.close(fig)\n",
    "\n",
    "def plot_categorical(coordinates, words, freq, skew, path):\n",
    "    fig, ax = plt.subplots(figsize=(20,10))    \n",
    "    sns.scatterplot(\n",
    "        coordinates[:,0], coordinates[:,1], \n",
    "        hue=skew, palette='muted', hue_norm=(0, 1),\n",
    "        size=freq, sizes=(100, 1000), \n",
    "        legend='brief', ax=ax)\n",
    "    chartBox = ax.get_position()\n",
    "    ax.set_position([chartBox.x0, chartBox.y0, chartBox.width*0.6, chartBox.height])\n",
    "    ax.legend(loc='upper center', bbox_to_anchor=(1.45, 0.8), ncol=1)\n",
    "    for coord, word in zip(coordinates, words):\n",
    "        ax.annotate(word, coord, fontsize=12)\n",
    "    with open(path, 'wb') as file:\n",
    "        fig.savefig(file, dpi=300)\n",
    "    plt.close(fig)\n",
    "    \n",
    "def load_en_masse(in_dir, endswith):\n",
    "    models = {}\n",
    "    for dirpath, _, filenames in tqdm(os.walk(in_dir)):\n",
    "        for file in filenames:\n",
    "            if file.endswith(endswith):\n",
    "                path = os.path.join(dirpath, file)\n",
    "                name = path.lstrip(in_dir).replace('/', ' ')\n",
    "                models[name] = load(path)\n",
    "    print(*models.keys(), sep='\\n')\n",
    "    return models\n",
    "    \n",
    "def graph_en_masse(\n",
    "        models,\n",
    "        out_dir, \n",
    "        reduction,  #  'PCA', 'TSNE', or 'both'\n",
    "        word_ids,  \n",
    "        words, \n",
    "        hues,\n",
    "        sizes,\n",
    "        perplexity=None,\n",
    "        categorical=False):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    for model_name, embed in tqdm(models.items()):\n",
    "        space = embed[word_ids]\n",
    "        if reduction == 'PCA':\n",
    "            visual = PCA(n_components=2).fit_transform(space)\n",
    "        elif reduction == 'TSNE':\n",
    "            assert perplexity is not None\n",
    "            visual = TSNE(\n",
    "                perplexity=perplexity, learning_rate=10, \n",
    "                n_iter=5000, n_iter_without_progress=1000).fit_transform(space)\n",
    "        elif reduction == 'both':\n",
    "            assert perplexity is not None\n",
    "            space = PCA(n_components=30).fit_transform(space)\n",
    "            visual = TSNE(\n",
    "                perplexity=perplexity, learning_rate=10, \n",
    "                n_iter=5000, n_iter_without_progress=1000).fit_transform(space)\n",
    "        else: \n",
    "            raise ValueError('unknown dimension reduction method')\n",
    "        if not categorical:\n",
    "            plot(visual, words, sizes, hues, \n",
    "                 os.path.join(out_dir, f'{model_name}.png'))\n",
    "        else:\n",
    "            plot_categorical(visual, words, sizes, hues, \n",
    "                 os.path.join(out_dir, f'{model_name}.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_ids, ch_freq, ch_skew, ch_deno = gather(cherry_words)\n",
    "gen_ids, gen_freq, gen_skew, gen_deno = gather(generic_words)\n",
    "\n",
    "random_words = [w for w in PE.word_to_id.keys() \n",
    "                if GD[w]['freq'] > 99]\n",
    "random_words = random.sample(random_words, 50)\n",
    "rand_ids, rand_freq, rand_skew, rand_deno = gather(random_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51\n"
     ]
    }
   ],
   "source": [
    "R_words = [w for w in PE.word_to_id.keys()\n",
    "             if GD[w]['freq'] > 99 and GD[w]['R_ratio'] > 0.75]\n",
    "R_words.remove('federal_debt_stood')  # outliers in clustering graphs\n",
    "R_words.remove('statements_relating')\n",
    "R_words.remove('legislative_days_within')\n",
    "print(len(R_words))\n",
    "# GOP_words = random.sample(GOP_words, 50)\n",
    "R_ids, R_freq, R_skew, R_deno = gather(R_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81\n"
     ]
    }
   ],
   "source": [
    "# D_words = [w for w in PE.word_to_id.keys()\n",
    "#            if GD[w]['freq'] > 99 and GD[w]['R_ratio'] < 0.25]\n",
    "\n",
    "D_words = ['war_in_iraq', 'unemployed', 'detainees', 'solar', \n",
    "    'wealthiest', 'minorities', 'gun_violence', \n",
    "    'amtrak', 'unemployment_benefits', \n",
    "    'citizens_united', 'mayors', 'prosecutor', 'working_families', \n",
    "    'cpsc', 'sexual_assault',\n",
    "    'affordable_housing', 'vietnam_veterans', 'drug_companies', 'handguns',\n",
    "    'hungry', 'college_education', \n",
    "    'main_street', 'trauma', 'simon', 'pandemic', \n",
    "    'reagan_administration', 'guns', \n",
    "    'million_jobs', 'airline_industry', 'mergers', 'blacks', \n",
    "    'industrial_base', 'unemployment_insurance',\n",
    "    'vacancies', 'trade_deficit', 'lost_their_jobs', 'food_safety', \n",
    "    'darfur', 'trains', 'deportation', 'credit_cards', \n",
    "    'surface_transportation', 'solar_energy', 'ecosystems', 'layoffs', \n",
    "    'wall_street', 'steelworkers', 'puerto_rico', 'hunger', \n",
    "    'child_support', 'naacp', 'domestic_violence', 'seaports', \n",
    "    'hate_crimes', 'underfunded', 'registrants', 'sanctuary', \n",
    "    'coastal_zone_management', 'vermonters', 'automakers', \n",
    "    'violence_against_women', 'unemployment_rate', \n",
    "    'select_committee_on_indian_affairs', 'judicial_nominees', \n",
    "    'school_construction', 'clarence_mitchell', 'confidential', \n",
    "    'domain_name', 'community_development', 'pell_grant', 'asylum', 'vawa', \n",
    "    'somalia', 'african_american', 'traders', 'jersey', 'fdic', 'shameful', \n",
    "    'homelessness', 'african_americans', 'payroll_tax',]\n",
    "#     'retraining', 'unemployed_workers', 'the_disclose_act', 'baltimore', \n",
    "#     'assault_weapons', 'credit_card', 'the_patriot_act', 'young_woman', \n",
    "#     'trades', 'aye', 'poisoning', 'police_officers', 'mammal', 'toys', \n",
    "#     'whistleblowers', 'north_dakota', 'californias', 'computer_crime', \n",
    "#     'explosives', 'fast_track', 'bus', 'redlining', 'seclusion', 'gender', \n",
    "#     'hawaiian', 'pay_discrimination', 'ledbetter', 'phd', 'supra', 'baggage', \n",
    "#     'las_vegas', 'the_voting_rights_act', 'enron', 'richest', 'vra', 'chip', \n",
    "#     'tax_break', 'the_usa_patriot_act', 'advance_notice', 'derivatives', \n",
    "#     'the_patients_bill_of_rights', 'shelf', 'divestment', 'sa', \n",
    "#     'submitted_an_amendment', 'bill_hr', 'first_responders',\n",
    "#     'unemployment_compensation', 'tax_breaks', 'carbon', \n",
    "#     'college_cost_reduction', 'clean_energy', 'waives', \n",
    "#     'unregulated', 'taa', 'truman', 'lesbian', 'coupons', \n",
    "#     'large_numbers', 'anonymous', 'whites', 'logging']\n",
    "\n",
    "print(len(D_words))\n",
    "D_words = random.sample(D_words, 50)\n",
    "D_ids, D_freq, D_skew, D_deno = gather(D_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "J_words = D_words + R_words\n",
    "J_ids = D_ids + R_ids\n",
    "J_freq = D_freq + R_freq\n",
    "J_skew = D_skew + R_skew\n",
    "J_deno = D_deno + R_deno\n",
    "J_cono = [0 if skew < 0.5 else 1 for skew in J_skew]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GD['joliet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/home/webson/Research/congressional_adversary/congressional_env/lib/python3.7/site-packages/torch/serialization.py:593: SourceChangeWarning: source code of class 'decomposer.Decomposer' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "5it [00:01,  2.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E1 A1 epoch100.pt\n",
      "E4 A1 epoch100.pt\n"
     ]
    }
   ],
   "source": [
    "# base_dir = '../../results/only remove deno BS128'\n",
    "# base_dir = '../../results/cono space remove deno/subset pretrained'\n",
    "base_dir = '../../results/deno space remove cono/superset pretrained'\n",
    "models = load_en_masse(base_dir, endswith='epoch100.pt')\n",
    "models['pretrained superset'] = load('../../results/pretrained/init.pt')\n",
    "models['pretrained'] = load('../../results/pretrained bill mentions/init.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph by Party Skew (for removing connotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_en_masse(\n",
    "    models,\n",
    "    out_dir=f'{base_dir}/PCA',\n",
    "    reduction='PCA',\n",
    "    word_ids=R_ids,\n",
    "    words=R_words,\n",
    "    hues=R_skew,\n",
    "    sizes=R_freq,\n",
    ")\n",
    "\n",
    "graph_en_masse(\n",
    "    models,\n",
    "    out_dir=f'{base_dir}/t-SNE p5',\n",
    "    reduction='TSNE',\n",
    "    perplexity=5,\n",
    "    word_ids=R_ids,\n",
    "    words=R_words,\n",
    "    hues=R_skew,\n",
    "    sizes=R_freq,\n",
    ")\n",
    "\n",
    "graph_en_masse(\n",
    "    models,\n",
    "    out_dir=f'{base_dir}/t-SNE p3',\n",
    "    reduction='TSNE',\n",
    "    perplexity=3,\n",
    "    word_ids=R_ids,\n",
    "    words=R_words,\n",
    "    hues=R_skew,\n",
    "    sizes=R_freq,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:07<00:00,  1.89s/it]\n",
      "100%|██████████| 4/4 [00:07<00:00,  1.94s/it]\n"
     ]
    }
   ],
   "source": [
    "# graph_en_masse(\n",
    "#     models,\n",
    "#     out_dir=f'{base_dir}/Joint/PCA',\n",
    "#     reduction='PCA',\n",
    "#     word_ids=J_ids,\n",
    "#     words=J_words,\n",
    "#     hues=J_skew,\n",
    "#     sizes=J_freq,\n",
    "# )\n",
    "\n",
    "# graph_en_masse(\n",
    "#     models,\n",
    "#     out_dir=f'{base_dir}/Joint/t-SNE p5',\n",
    "#     reduction='TSNE',\n",
    "#     perplexity=5,\n",
    "#     word_ids=J_ids,\n",
    "#     words=J_words,\n",
    "#     hues=J_skew,\n",
    "#     sizes=J_freq,\n",
    "# )\n",
    "\n",
    "# graph_en_masse(\n",
    "#     models,\n",
    "#     out_dir=f'{base_dir}/Joint/t-SNE p3',\n",
    "#     reduction='TSNE',\n",
    "#     perplexity=3,\n",
    "#     word_ids=J_ids,\n",
    "#     words=J_words,\n",
    "#     hues=J_skew,\n",
    "#     sizes=J_freq,\n",
    "# )\n",
    "\n",
    "graph_en_masse(\n",
    "    models,\n",
    "    out_dir=f'{base_dir}/Joint/t-SNE p25',\n",
    "    reduction='TSNE',\n",
    "    perplexity=25,\n",
    "    word_ids=J_ids,\n",
    "    words=J_words,\n",
    "    hues=J_skew,\n",
    "    sizes=J_freq,\n",
    ")\n",
    "\n",
    "graph_en_masse(\n",
    "    models,\n",
    "    out_dir=f'{base_dir}/Joint/t-SNE p50',\n",
    "    reduction='TSNE',\n",
    "    perplexity=50,\n",
    "    word_ids=J_ids,\n",
    "    words=J_words,\n",
    "    hues=J_skew,\n",
    "    sizes=J_freq,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph by Topic Denotation (for removing denotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "graph_en_masse(\n",
    "    models,\n",
    "    out_dir=f'{base_dir}/Highly GOP/PCA',\n",
    "    reduction='PCA',\n",
    "    perplexity=5,\n",
    "    word_ids=GOP_ids,\n",
    "    words=GOP_words,\n",
    "    hues=GOP_deno,\n",
    "    sizes=GOP_freq,\n",
    "    categorical=True\n",
    ")\n",
    "\n",
    "graph_en_masse(\n",
    "    models,\n",
    "    out_dir=f'{base_dir}/Highly GOP/t-SNE p5',\n",
    "    reduction='TSNE',\n",
    "    perplexity=5,\n",
    "    word_ids=GOP_ids,\n",
    "    words=GOP_words,\n",
    "    hues=GOP_deno,\n",
    "    sizes=GOP_freq,\n",
    "    categorical=True\n",
    ")\n",
    "\n",
    "graph_en_masse(\n",
    "    models,\n",
    "    out_dir=f'{base_dir}/Highly GOP/t-SNE p3',\n",
    "    reduction='TSNE',\n",
    "    perplexity=3,\n",
    "    word_ids=GOP_ids,\n",
    "    words=GOP_words,\n",
    "    hues=GOP_deno,\n",
    "    sizes=GOP_freq,\n",
    "    categorical=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_en_masse(\n",
    "    models,\n",
    "    out_dir=f'{base_dir}/Highly Dem/PCA',\n",
    "    reduction='PCA',\n",
    "    perplexity=5,\n",
    "    word_ids=D_ids,\n",
    "    words=D_words,\n",
    "    hues=D_deno,\n",
    "    sizes=D_freq,\n",
    "    categorical=True\n",
    ")\n",
    "\n",
    "graph_en_masse(\n",
    "    models,\n",
    "    out_dir=f'{base_dir}/Highly Dem/t-SNE p5',\n",
    "    reduction='TSNE',\n",
    "    perplexity=5,\n",
    "    word_ids=D_ids,\n",
    "    words=D_words,\n",
    "    hues=D_deno,\n",
    "    sizes=D_freq,\n",
    "    categorical=True\n",
    ")\n",
    "\n",
    "graph_en_masse(\n",
    "    models,\n",
    "    out_dir=f'{base_dir}/Highly Dem/t-SNE p3',\n",
    "    reduction='TSNE',\n",
    "    perplexity=3,\n",
    "    word_ids=D_ids,\n",
    "    words=D_words,\n",
    "    hues=D_deno,\n",
    "    sizes=D_freq,\n",
    "    categorical=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:07<00:00,  1.84s/it]\n",
      "100%|██████████| 4/4 [00:11<00:00,  2.82s/it]\n",
      "100%|██████████| 4/4 [00:11<00:00,  2.78s/it]\n"
     ]
    }
   ],
   "source": [
    "# graph_en_masse(\n",
    "#     models,\n",
    "#     out_dir=f'{base_dir}/Joint/PCA',\n",
    "#     reduction='PCA',\n",
    "#     perplexity=5,\n",
    "#     word_ids=J_ids,\n",
    "#     words=J_words,\n",
    "#     hues=J_deno,\n",
    "#     sizes=J_freq,\n",
    "#     categorical=True\n",
    "# )\n",
    "\n",
    "graph_en_masse(\n",
    "    models,\n",
    "    out_dir=f'{base_dir}/Joint/t-SNE p5',\n",
    "    reduction='TSNE',\n",
    "    perplexity=5,\n",
    "    word_ids=J_ids,\n",
    "    words=J_words,\n",
    "    hues=J_deno,\n",
    "    sizes=J_freq,\n",
    "    categorical=True\n",
    ")\n",
    "\n",
    "graph_en_masse(\n",
    "    models,\n",
    "    out_dir=f'{base_dir}/Joint/t-SNE p3',\n",
    "    reduction='TSNE',\n",
    "    perplexity=3,\n",
    "    word_ids=J_ids,\n",
    "    words=J_words,\n",
    "    hues=J_deno,\n",
    "    sizes=J_freq,\n",
    "    categorical=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Recomposers\n",
    "Want to show that...\n",
    "\n",
    "For deno vectors, topic cluster better than pretrained\n",
    "\n",
    "For cono vectors, skew cluster better than pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_recomposer(path):\n",
    "    stuff = torch.load(path, map_location=DEVICE)['model']\n",
    "    D_embed = stuff.deno_decomposer.embedding.weight.detach().numpy()\n",
    "    C_embed = stuff.cono_decomposer.embedding.weight.detach().numpy()\n",
    "    return D_embed, C_embed\n",
    "\n",
    "def load_recomposers_en_masse(in_dir, endswith):\n",
    "    D_models = {\n",
    "        'pretrained superset': load('../../results/pretrained/init.pt'),\n",
    "        'pretrained': load('../../results/pretrained bill mentions/init.pt')}\n",
    "    C_models = {\n",
    "        'pretrained superset': load('../../results/pretrained/init.pt'),\n",
    "        'pretrained': load('../../results/pretrained bill mentions/init.pt')}\n",
    "    for dirpath, _, filenames in os.walk(in_dir):\n",
    "        for file in filenames:\n",
    "            if file.endswith(endswith):\n",
    "                path = os.path.join(dirpath, file)\n",
    "                name = path.lstrip(in_dir).replace('/', ' ')\n",
    "                D_embed, C_embed = load_recomposer(path)\n",
    "                # Brittle Hack\n",
    "                name = name.split()\n",
    "                D_name = ' '.join(name[0:2] + name[4:])\n",
    "                R_name = ' '.join(name[2:])\n",
    "                D_models[D_name] = D_embed\n",
    "                C_models[R_name] = C_embed\n",
    "                print(name)\n",
    "    return D_models, C_models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/webson/Research/congressional_adversary/congressional_env/lib/python3.7/site-packages/torch/serialization.py:593: SourceChangeWarning: source code of class 'decomposer.Decomposer' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dd0.9', 'Dg-3.5', 'Cd-2.4', 'Cg2.2', 'R1.5', 'epoch100.pt']\n",
      "['Dd0.8', 'Dg-4.7', 'Cd-0.7', 'Cg3.0', 'R3.5', 'epoch100.pt']\n",
      "['Dd3.0', 'Dg-4.1', 'Cd-4.7', 'Cg4.7', 'R4.8', 'epoch100.pt']\n",
      "['Dd1.9', 'Dg-0.2', 'Cd-1.3', 'Cg3.0', 'R0.8', 'epoch100.pt']\n",
      "['Dd3.1', 'Dg-4.3', 'Cd-3.5', 'Cg1.8', 'R2.3', 'epoch100.pt']\n",
      "['Dd4.0', 'Dg-3.5', 'Cd-4.5', 'Cg3.4', 'R2.2', 'epoch100.pt']\n",
      "['Dd3.9', 'Dg-4.0', 'Cd-2.4', 'Cg3.0', 'R0.2', 'epoch100.pt']\n",
      "['Dd0.1', 'Dg-0.2', 'Cd-0.8', 'Cg1.1', 'R0.9', 'epoch100.pt']\n"
     ]
    }
   ],
   "source": [
    "base_dir = '../../results/recomposer/superset pretrained'\n",
    "D_models, C_models = load_recomposers_en_masse(base_dir, endswith='epoch100.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|█         | 1/10 [00:02<00:24,  2.71s/it]\u001b[A\n",
      " 20%|██        | 2/10 [00:05<00:21,  2.68s/it]\u001b[A\n",
      " 30%|███       | 3/10 [00:07<00:18,  2.65s/it]\u001b[A\n",
      " 40%|████      | 4/10 [00:10<00:16,  2.69s/it]\u001b[A\n",
      " 50%|█████     | 5/10 [00:13<00:12,  2.60s/it]\u001b[A\n",
      " 60%|██████    | 6/10 [00:15<00:10,  2.56s/it]\u001b[A\n",
      " 70%|███████   | 7/10 [00:17<00:07,  2.50s/it]\u001b[A\n",
      " 80%|████████  | 8/10 [00:20<00:04,  2.49s/it]\u001b[A\n",
      " 90%|█████████ | 9/10 [00:22<00:02,  2.52s/it]\u001b[A\n",
      "100%|██████████| 10/10 [00:25<00:00,  2.54s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|█         | 1/10 [00:03<00:27,  3.09s/it]\u001b[A\n",
      " 20%|██        | 2/10 [00:06<00:24,  3.11s/it]\u001b[A\n",
      " 30%|███       | 3/10 [00:09<00:21,  3.07s/it]\u001b[A\n",
      " 40%|████      | 4/10 [00:12<00:17,  3.00s/it]\u001b[A\n",
      " 50%|█████     | 5/10 [00:15<00:15,  3.06s/it]\u001b[A\n",
      " 60%|██████    | 6/10 [00:18<00:11,  3.00s/it]\u001b[A\n",
      " 70%|███████   | 7/10 [00:20<00:08,  2.83s/it]\u001b[A\n",
      " 80%|████████  | 8/10 [00:23<00:05,  2.79s/it]\u001b[A\n",
      " 90%|█████████ | 9/10 [00:25<00:02,  2.70s/it]\u001b[A\n",
      "100%|██████████| 10/10 [00:28<00:00,  2.81s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|█         | 1/10 [00:03<00:27,  3.03s/it]\u001b[A\n",
      " 20%|██        | 2/10 [00:06<00:24,  3.02s/it]\u001b[A\n",
      " 30%|███       | 3/10 [00:08<00:20,  2.93s/it]\u001b[A\n",
      " 40%|████      | 4/10 [00:11<00:16,  2.81s/it]\u001b[A\n",
      " 50%|█████     | 5/10 [00:14<00:14,  2.96s/it]\u001b[A\n",
      " 60%|██████    | 6/10 [00:17<00:12,  3.01s/it]\u001b[A\n",
      " 70%|███████   | 7/10 [00:20<00:08,  2.97s/it]\u001b[A\n",
      " 80%|████████  | 8/10 [00:23<00:06,  3.01s/it]\u001b[A\n",
      " 90%|█████████ | 9/10 [00:26<00:03,  3.04s/it]\u001b[A\n",
      "100%|██████████| 10/10 [00:29<00:00,  2.96s/it]\u001b[A\n"
     ]
    }
   ],
   "source": [
    " # Evaluating Denotation\n",
    "models = D_models\n",
    "\n",
    "graph_en_masse(\n",
    "    models, out_dir=f'{base_dir}/Joint/topic/t-SNE p5',\n",
    "    reduction='TSNE', perplexity=5,\n",
    "    word_ids=J_ids, words=J_words, hues=J_deno, sizes=J_freq,\n",
    "    categorical=True\n",
    ")\n",
    "\n",
    "graph_en_masse(\n",
    "    models,\n",
    "    out_dir=f'{base_dir}/Joint/topic/t-SNE p3',\n",
    "    reduction='TSNE', perplexity=3,\n",
    "    word_ids=J_ids, words=J_words, hues=J_deno, sizes=J_freq,\n",
    "    categorical=True\n",
    ")\n",
    "\n",
    "graph_en_masse(\n",
    "    models,\n",
    "    out_dir=f'{base_dir}/Joint/topic/t-SNE p10',\n",
    "    reduction='TSNE', perplexity=10,\n",
    "    word_ids=J_ids, words=J_words, hues=J_deno, sizes=J_freq,\n",
    "    categorical=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|█         | 1/10 [00:02<00:18,  2.10s/it]\u001b[A\n",
      " 20%|██        | 2/10 [00:04<00:16,  2.05s/it]\u001b[A\n",
      " 30%|███       | 3/10 [00:06<00:14,  2.07s/it]\u001b[A\n",
      " 40%|████      | 4/10 [00:08<00:12,  2.05s/it]\u001b[A\n",
      " 50%|█████     | 5/10 [00:10<00:10,  2.04s/it]\u001b[A\n",
      " 60%|██████    | 6/10 [00:12<00:08,  2.05s/it]\u001b[A\n",
      " 70%|███████   | 7/10 [00:14<00:06,  2.19s/it]\u001b[A\n",
      " 80%|████████  | 8/10 [00:16<00:04,  2.14s/it]\u001b[A\n",
      " 90%|█████████ | 9/10 [00:19<00:02,  2.22s/it]\u001b[A\n",
      "100%|██████████| 10/10 [00:21<00:00,  2.12s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|█         | 1/10 [00:01<00:17,  1.89s/it]\u001b[A\n",
      " 20%|██        | 2/10 [00:03<00:15,  1.88s/it]\u001b[A\n",
      " 30%|███       | 3/10 [00:05<00:13,  1.88s/it]\u001b[A\n",
      " 40%|████      | 4/10 [00:07<00:11,  1.93s/it]\u001b[A\n",
      " 50%|█████     | 5/10 [00:09<00:10,  2.00s/it]\u001b[A\n",
      " 60%|██████    | 6/10 [00:11<00:07,  1.99s/it]\u001b[A\n",
      " 70%|███████   | 7/10 [00:13<00:05,  1.98s/it]\u001b[A\n",
      " 80%|████████  | 8/10 [00:16<00:04,  2.09s/it]\u001b[A\n",
      " 90%|█████████ | 9/10 [00:18<00:02,  2.08s/it]\u001b[A\n",
      "100%|██████████| 10/10 [00:20<00:00,  2.02s/it]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "# Evaluating Connotation\n",
    "models = C_models\n",
    "\n",
    "graph_en_masse(\n",
    "    models,\n",
    "    out_dir=f'{base_dir}/Joint/party/t-SNE p25',\n",
    "    reduction='TSNE', perplexity=25,\n",
    "    word_ids=J_ids, words=J_words, hues=J_skew, sizes=J_freq,\n",
    ")\n",
    "\n",
    "graph_en_masse(\n",
    "    models,\n",
    "    out_dir=f'{base_dir}/Joint/party/t-SNE p50',\n",
    "    reduction='TSNE', perplexity=50,\n",
    "    word_ids=J_ids, words=J_words, hues=J_skew, sizes=J_freq,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering + Homogeneity V-Measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import homogeneity_completeness_v_measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['E4 A1 superset epoch200.pt', 'E1 A1 superset epoch200.pt', 'pretrained superset', 'pretrained'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E1 A1 epoch100.pt\t0.6428\t0.5346\t0.5838\n",
      "pretrained superset\t0.7669\t0.6505\t0.7039\n",
      "pretrained\t0.6125\t0.5461\t0.5774\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for model_name, model in models.items():\n",
    "    embed = model[J_ids]\n",
    "#     Cono_Space = KMeans(n_clusters=2).fit(embed)\n",
    "    Deno_Space = KMeans(n_clusters=41).fit(embed)\n",
    "    pred_labels = Deno_Space.predict(embed)\n",
    "    homogeneity, completeness, v_measure = np.around(homogeneity_completeness_v_measure(\n",
    "        J_deno, pred_labels), 4)\n",
    "    print(model_name, homogeneity, completeness, v_measure, sep='\\t')\n",
    "#     print(pred_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0829\t0.2076\t0.1185\n"
     ]
    }
   ],
   "source": [
    "embed = models['pretrained superset'][J_ids]\n",
    "Cono_Space = KMeans(n_clusters=2).fit(embed)\n",
    "pred_labels = Cono_Space.predict(embed)\n",
    "homogeneity, completeness, v_measure = np.around(homogeneity_completeness_v_measure(\n",
    "    J_cono, pred_labels), 4)\n",
    "print(homogeneity, completeness, v_measure, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "       0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], dtype=int32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.24267782426778242,\n",
       " 0.21937842778793418,\n",
       " 0.16216216216216217,\n",
       " 0.21495327102803738,\n",
       " 0.21782178217821782,\n",
       " 0.2392638036809816,\n",
       " 0.20078740157480315,\n",
       " 0.19637462235649547,\n",
       " 0.24217462932454695,\n",
       " 0.22916666666666666,\n",
       " 0.24,\n",
       " 0.17692307692307693,\n",
       " 0.2231404958677686,\n",
       " 0.23529411764705882,\n",
       " 0.18181818181818182,\n",
       " 0.22321428571428573,\n",
       " 0.1342281879194631,\n",
       " 0.1415525114155251,\n",
       " 0.11382113821138211,\n",
       " 0.23780487804878048,\n",
       " 0.19597989949748743,\n",
       " 0.23921568627450981,\n",
       " 0.16363636363636364,\n",
       " 0.23931623931623933,\n",
       " 0.21666666666666667,\n",
       " 0.24742268041237114,\n",
       " 0.18543046357615894,\n",
       " 0.20909090909090908,\n",
       " 0.23562152133580705,\n",
       " 0.192,\n",
       " 0.23636363636363636,\n",
       " 0.18333333333333332,\n",
       " 0.1650485436893204,\n",
       " 0.18691588785046728,\n",
       " 0.03225806451612903,\n",
       " 0.22764227642276422,\n",
       " 0.13970588235294118,\n",
       " 0.09,\n",
       " 0.23076923076923078,\n",
       " 0.22448979591836735,\n",
       " 0.1885245901639344,\n",
       " 0.1888111888111888,\n",
       " 0.1893939393939394,\n",
       " 0.21238938053097345,\n",
       " 0.23148148148148148,\n",
       " 0.13679245283018868,\n",
       " 0.22053231939163498,\n",
       " 0.2037037037037037,\n",
       " 0.23076923076923078,\n",
       " 0.211864406779661,\n",
       " 0.7605633802816901,\n",
       " 0.7670250896057348,\n",
       " 0.815,\n",
       " 0.8118811881188119,\n",
       " 0.8333333333333334,\n",
       " 0.7674418604651163,\n",
       " 0.7753479125248509,\n",
       " 0.7623762376237624,\n",
       " 0.7615384615384615,\n",
       " 0.7863383637807784,\n",
       " 0.8151260504201681,\n",
       " 0.9375,\n",
       " 0.7751479289940828,\n",
       " 0.8421052631578947,\n",
       " 0.7524271844660194,\n",
       " 0.828125,\n",
       " 0.9485981308411215,\n",
       " 0.7676348547717843,\n",
       " 0.7924528301886793,\n",
       " 0.9789473684210527,\n",
       " 0.9661458333333334,\n",
       " 0.8849557522123894,\n",
       " 0.8024316109422492,\n",
       " 0.8155339805825242,\n",
       " 0.7807017543859649,\n",
       " 0.8514851485148515,\n",
       " 0.7883597883597884,\n",
       " 0.8260869565217391,\n",
       " 0.8857142857142857,\n",
       " 0.8278145695364238,\n",
       " 0.7516339869281046,\n",
       " 0.8,\n",
       " 0.8211382113821138,\n",
       " 0.8037974683544303,\n",
       " 0.7790055248618785,\n",
       " 0.8278688524590164,\n",
       " 0.8865979381443299,\n",
       " 0.8134715025906736,\n",
       " 0.7652582159624414,\n",
       " 0.8235294117647058,\n",
       " 0.8803418803418803,\n",
       " 0.7524752475247525,\n",
       " 0.7560975609756098,\n",
       " 0.8220338983050848,\n",
       " 0.7925531914893617,\n",
       " 0.7621951219512195,\n",
       " 0.7844827586206896,\n",
       " 0.9272727272727272,\n",
       " 0.9402985074626866,\n",
       " 0.7870370370370371,\n",
       " 0.8389830508474576]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "J_skew"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
