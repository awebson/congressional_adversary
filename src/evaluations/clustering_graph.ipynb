{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm import tqdm\n",
    "\n",
    "from evaluations.intrinsic_eval import cherry_words, generic_words\n",
    "from decomposer import Decomposer, DecomposerConfig\n",
    "\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "sns.set()\n",
    "\n",
    "DEVICE = 'cpu'\n",
    "PE = torch.load(\n",
    "    '../../results/pretrained/init.pt', map_location=DEVICE)['model']\n",
    "GD = PE.grounding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(path):\n",
    "    stuff = torch.load(path, map_location=DEVICE)['model']\n",
    "    return stuff.embedding.weight.detach().numpy()\n",
    "\n",
    "def gather(words):\n",
    "    word_ids = [PE.word_to_id[w] for w in words]\n",
    "    freq = [GD[w]['freq'] for w in words]\n",
    "    skew = [GD[w]['R_ratio'] for w in words]\n",
    "    maj_deno = [GD[w]['majority_deno'] for w in words]\n",
    "    return word_ids, freq, skew, maj_deno\n",
    "\n",
    "def plot(coordinates, words, freq, skew, path):\n",
    "    fig, ax = plt.subplots(figsize=(15,10))    \n",
    "    sns.scatterplot(\n",
    "        coordinates[:,0], coordinates[:,1], \n",
    "        hue=skew, palette='coolwarm', # hue_norm=(0, 1), \n",
    "        size=freq, sizes=(100, 1000), \n",
    "        legend=None, ax=ax)\n",
    "    for coord, word in zip(coordinates, words):\n",
    "        ax.annotate(word, coord, fontsize=12)\n",
    "    with open(path, 'wb') as file:\n",
    "        fig.savefig(file, dpi=300)\n",
    "    plt.close(fig)\n",
    "\n",
    "def plot_categorical(coordinates, words, freq, skew, path):\n",
    "    fig, ax = plt.subplots(figsize=(20,10))    \n",
    "    sns.scatterplot(\n",
    "        coordinates[:,0], coordinates[:,1], \n",
    "        hue=skew, palette='muted', hue_norm=(0, 1),\n",
    "        size=freq, sizes=(100, 1000), \n",
    "        legend='brief', ax=ax)\n",
    "    chartBox = ax.get_position()\n",
    "    ax.set_position([chartBox.x0, chartBox.y0, chartBox.width*0.6, chartBox.height])\n",
    "    ax.legend(loc='upper center', bbox_to_anchor=(1.45, 0.8), ncol=1)\n",
    "    for coord, word in zip(coordinates, words):\n",
    "        ax.annotate(word, coord, fontsize=12)\n",
    "    with open(path, 'wb') as file:\n",
    "        fig.savefig(file, dpi=300)\n",
    "    plt.close(fig)\n",
    "    \n",
    "def load_en_masse(in_dir, endswith):\n",
    "    models = {}\n",
    "    for dirpath, _, filenames in tqdm(os.walk(in_dir)):\n",
    "        for file in filenames:\n",
    "            if file.endswith(endswith):\n",
    "                path = os.path.join(dirpath, file)\n",
    "                name = path.lstrip(in_dir).replace('/', ' ')\n",
    "                models[name] = load(path)\n",
    "    print(*models.keys(), sep='\\n')\n",
    "    return models\n",
    "    \n",
    "def graph_en_masse(\n",
    "        models,\n",
    "        out_dir, \n",
    "        reduction,  #  'PCA', 'TSNE', or 'both'\n",
    "        word_ids,  \n",
    "        words, \n",
    "        hues,\n",
    "        sizes,\n",
    "        perplexity=None,\n",
    "        categorical=False):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    for model_name, embed in tqdm(models.items()):\n",
    "        space = embed[word_ids]\n",
    "        if reduction == 'PCA':\n",
    "            visual = PCA(n_components=2).fit_transform(space)\n",
    "        elif reduction == 'TSNE':\n",
    "            assert perplexity is not None\n",
    "            visual = TSNE(\n",
    "                perplexity=perplexity, learning_rate=10, \n",
    "                n_iter=5000, n_iter_without_progress=1000).fit_transform(space)\n",
    "        elif reduction == 'both':\n",
    "            assert perplexity is not None\n",
    "            space = PCA(n_components=30).fit_transform(space)\n",
    "            visual = TSNE(\n",
    "                perplexity=perplexity, learning_rate=10, \n",
    "                n_iter=5000, n_iter_without_progress=1000).fit_transform(space)\n",
    "        else: \n",
    "            raise ValueError('unknown dimension reduction method')\n",
    "        if not categorical:\n",
    "            plot(visual, words, sizes, hues, \n",
    "                 os.path.join(out_dir, f'{model_name}.png'))\n",
    "        else:\n",
    "            plot_categorical(visual, words, sizes, hues, \n",
    "                 os.path.join(out_dir, f'{model_name}.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_ids, ch_freq, ch_skew, ch_deno = gather(cherry_words)\n",
    "gen_ids, gen_freq, gen_skew, gen_deno = gather(generic_words)\n",
    "\n",
    "random_words = [w for w in PE.word_to_id.keys() \n",
    "                if GD[w]['freq'] > 99]\n",
    "random_words = random.sample(random_words, 50)\n",
    "rand_ids, rand_freq, rand_skew, rand_deno = gather(random_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51\n"
     ]
    }
   ],
   "source": [
    "R_words = [w for w in PE.word_to_id.keys()\n",
    "             if GD[w]['freq'] > 99 and GD[w]['R_ratio'] > 0.75]\n",
    "R_words.remove('federal_debt_stood')  # outliers in clustering graphs\n",
    "R_words.remove('statements_relating')\n",
    "R_words.remove('legislative_days_within')\n",
    "print(len(R_words))\n",
    "# GOP_words = random.sample(GOP_words, 50)\n",
    "R_ids, R_freq, R_skew, R_deno = gather(R_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81\n"
     ]
    }
   ],
   "source": [
    "# D_words = [w for w in PE.word_to_id.keys()\n",
    "#            if GD[w]['freq'] > 99 and GD[w]['R_ratio'] < 0.25]\n",
    "\n",
    "D_words = ['war_in_iraq', 'unemployed', 'detainees', 'solar', \n",
    "    'wealthiest', 'minorities', 'gun_violence', \n",
    "    'amtrak', 'unemployment_benefits', \n",
    "    'citizens_united', 'mayors', 'prosecutor', 'working_families', \n",
    "    'cpsc', 'sexual_assault',\n",
    "    'affordable_housing', 'vietnam_veterans', 'drug_companies', 'handguns',\n",
    "    'hungry', 'college_education', \n",
    "    'main_street', 'trauma', 'simon', 'pandemic', \n",
    "    'reagan_administration', 'guns', \n",
    "    'million_jobs', 'airline_industry', 'mergers', 'blacks', \n",
    "    'industrial_base', 'unemployment_insurance',\n",
    "    'vacancies', 'trade_deficit', 'lost_their_jobs', 'food_safety', \n",
    "    'darfur', 'trains', 'deportation', 'credit_cards', \n",
    "    'surface_transportation', 'solar_energy', 'ecosystems', 'layoffs', \n",
    "    'wall_street', 'steelworkers', 'puerto_rico', 'hunger', \n",
    "    'child_support', 'naacp', 'domestic_violence', 'seaports', \n",
    "    'hate_crimes', 'underfunded', 'registrants', 'sanctuary', \n",
    "    'coastal_zone_management', 'vermonters', 'automakers', \n",
    "    'violence_against_women', 'unemployment_rate', \n",
    "    'select_committee_on_indian_affairs', 'judicial_nominees', \n",
    "    'school_construction', 'clarence_mitchell', 'confidential', \n",
    "    'domain_name', 'community_development', 'pell_grant', 'asylum', 'vawa', \n",
    "    'somalia', 'african_american', 'traders', 'jersey', 'fdic', 'shameful', \n",
    "    'homelessness', 'african_americans', 'payroll_tax',]\n",
    "#     'retraining', 'unemployed_workers', 'the_disclose_act', 'baltimore', \n",
    "#     'assault_weapons', 'credit_card', 'the_patriot_act', 'young_woman', \n",
    "#     'trades', 'aye', 'poisoning', 'police_officers', 'mammal', 'toys', \n",
    "#     'whistleblowers', 'north_dakota', 'californias', 'computer_crime', \n",
    "#     'explosives', 'fast_track', 'bus', 'redlining', 'seclusion', 'gender', \n",
    "#     'hawaiian', 'pay_discrimination', 'ledbetter', 'phd', 'supra', 'baggage', \n",
    "#     'las_vegas', 'the_voting_rights_act', 'enron', 'richest', 'vra', 'chip', \n",
    "#     'tax_break', 'the_usa_patriot_act', 'advance_notice', 'derivatives', \n",
    "#     'the_patients_bill_of_rights', 'shelf', 'divestment', 'sa', \n",
    "#     'submitted_an_amendment', 'bill_hr', 'first_responders',\n",
    "#     'unemployment_compensation', 'tax_breaks', 'carbon', \n",
    "#     'college_cost_reduction', 'clean_energy', 'waives', \n",
    "#     'unregulated', 'taa', 'truman', 'lesbian', 'coupons', \n",
    "#     'large_numbers', 'anonymous', 'whites', 'logging']\n",
    "\n",
    "print(len(D_words))\n",
    "D_words = random.sample(D_words, 50)\n",
    "D_ids, D_freq, D_skew, D_deno = gather(D_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "J_words = D_words + R_words\n",
    "J_ids = D_ids + R_ids\n",
    "J_freq = D_freq + R_freq\n",
    "J_skew = D_skew + R_skew\n",
    "J_deno = D_deno + R_deno\n",
    "J_cono = [0 if skew < 0.5 else 1 for skew in J_skew]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GD['joliet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# base_dir = '../../results/only remove deno BS128'\n",
    "# base_dir = '../../results/cono space remove deno/subset pretrained'\n",
    "base_dir = '../../results/deno space remove cono/superset pretrained'\n",
    "models = load_en_masse(base_dir, endswith='epoch100.pt')\n",
    "models['pretrained superset'] = load('../../results/pretrained/init.pt')\n",
    "models['pretrained'] = load('../../results/pretrained bill mentions/init.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph by Party Skew (for removing connotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_en_masse(\n",
    "    models,\n",
    "    out_dir=f'{base_dir}/PCA',\n",
    "    reduction='PCA',\n",
    "    word_ids=R_ids,\n",
    "    words=R_words,\n",
    "    hues=R_skew,\n",
    "    sizes=R_freq,\n",
    ")\n",
    "\n",
    "graph_en_masse(\n",
    "    models,\n",
    "    out_dir=f'{base_dir}/t-SNE p5',\n",
    "    reduction='TSNE',\n",
    "    perplexity=5,\n",
    "    word_ids=R_ids,\n",
    "    words=R_words,\n",
    "    hues=R_skew,\n",
    "    sizes=R_freq,\n",
    ")\n",
    "\n",
    "graph_en_masse(\n",
    "    models,\n",
    "    out_dir=f'{base_dir}/t-SNE p3',\n",
    "    reduction='TSNE',\n",
    "    perplexity=3,\n",
    "    word_ids=R_ids,\n",
    "    words=R_words,\n",
    "    hues=R_skew,\n",
    "    sizes=R_freq,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph_en_masse(\n",
    "#     models,\n",
    "#     out_dir=f'{base_dir}/Joint/PCA',\n",
    "#     reduction='PCA',\n",
    "#     word_ids=J_ids,\n",
    "#     words=J_words,\n",
    "#     hues=J_skew,\n",
    "#     sizes=J_freq,\n",
    "# )\n",
    "\n",
    "# graph_en_masse(\n",
    "#     models,\n",
    "#     out_dir=f'{base_dir}/Joint/t-SNE p5',\n",
    "#     reduction='TSNE',\n",
    "#     perplexity=5,\n",
    "#     word_ids=J_ids,\n",
    "#     words=J_words,\n",
    "#     hues=J_skew,\n",
    "#     sizes=J_freq,\n",
    "# )\n",
    "\n",
    "# graph_en_masse(\n",
    "#     models,\n",
    "#     out_dir=f'{base_dir}/Joint/t-SNE p3',\n",
    "#     reduction='TSNE',\n",
    "#     perplexity=3,\n",
    "#     word_ids=J_ids,\n",
    "#     words=J_words,\n",
    "#     hues=J_skew,\n",
    "#     sizes=J_freq,\n",
    "# )\n",
    "\n",
    "graph_en_masse(\n",
    "    models,\n",
    "    out_dir=f'{base_dir}/Joint/t-SNE p25',\n",
    "    reduction='TSNE',\n",
    "    perplexity=25,\n",
    "    word_ids=J_ids,\n",
    "    words=J_words,\n",
    "    hues=J_skew,\n",
    "    sizes=J_freq,\n",
    ")\n",
    "\n",
    "graph_en_masse(\n",
    "    models,\n",
    "    out_dir=f'{base_dir}/Joint/t-SNE p50',\n",
    "    reduction='TSNE',\n",
    "    perplexity=50,\n",
    "    word_ids=J_ids,\n",
    "    words=J_words,\n",
    "    hues=J_skew,\n",
    "    sizes=J_freq,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph by Topic Denotation (for removing denotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "graph_en_masse(\n",
    "    models,\n",
    "    out_dir=f'{base_dir}/Highly GOP/PCA',\n",
    "    reduction='PCA',\n",
    "    perplexity=5,\n",
    "    word_ids=GOP_ids,\n",
    "    words=GOP_words,\n",
    "    hues=GOP_deno,\n",
    "    sizes=GOP_freq,\n",
    "    categorical=True\n",
    ")\n",
    "\n",
    "graph_en_masse(\n",
    "    models,\n",
    "    out_dir=f'{base_dir}/Highly GOP/t-SNE p5',\n",
    "    reduction='TSNE',\n",
    "    perplexity=5,\n",
    "    word_ids=GOP_ids,\n",
    "    words=GOP_words,\n",
    "    hues=GOP_deno,\n",
    "    sizes=GOP_freq,\n",
    "    categorical=True\n",
    ")\n",
    "\n",
    "graph_en_masse(\n",
    "    models,\n",
    "    out_dir=f'{base_dir}/Highly GOP/t-SNE p3',\n",
    "    reduction='TSNE',\n",
    "    perplexity=3,\n",
    "    word_ids=GOP_ids,\n",
    "    words=GOP_words,\n",
    "    hues=GOP_deno,\n",
    "    sizes=GOP_freq,\n",
    "    categorical=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_en_masse(\n",
    "    models,\n",
    "    out_dir=f'{base_dir}/Highly Dem/PCA',\n",
    "    reduction='PCA',\n",
    "    perplexity=5,\n",
    "    word_ids=D_ids,\n",
    "    words=D_words,\n",
    "    hues=D_deno,\n",
    "    sizes=D_freq,\n",
    "    categorical=True\n",
    ")\n",
    "\n",
    "graph_en_masse(\n",
    "    models,\n",
    "    out_dir=f'{base_dir}/Highly Dem/t-SNE p5',\n",
    "    reduction='TSNE',\n",
    "    perplexity=5,\n",
    "    word_ids=D_ids,\n",
    "    words=D_words,\n",
    "    hues=D_deno,\n",
    "    sizes=D_freq,\n",
    "    categorical=True\n",
    ")\n",
    "\n",
    "graph_en_masse(\n",
    "    models,\n",
    "    out_dir=f'{base_dir}/Highly Dem/t-SNE p3',\n",
    "    reduction='TSNE',\n",
    "    perplexity=3,\n",
    "    word_ids=D_ids,\n",
    "    words=D_words,\n",
    "    hues=D_deno,\n",
    "    sizes=D_freq,\n",
    "    categorical=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph_en_masse(\n",
    "#     models,\n",
    "#     out_dir=f'{base_dir}/Joint/PCA',\n",
    "#     reduction='PCA',\n",
    "#     perplexity=5,\n",
    "#     word_ids=J_ids,\n",
    "#     words=J_words,\n",
    "#     hues=J_deno,\n",
    "#     sizes=J_freq,\n",
    "#     categorical=True\n",
    "# )\n",
    "\n",
    "graph_en_masse(\n",
    "    models,\n",
    "    out_dir=f'{base_dir}/Joint/t-SNE p5',\n",
    "    reduction='TSNE',\n",
    "    perplexity=5,\n",
    "    word_ids=J_ids,\n",
    "    words=J_words,\n",
    "    hues=J_deno,\n",
    "    sizes=J_freq,\n",
    "    categorical=True\n",
    ")\n",
    "\n",
    "graph_en_masse(\n",
    "    models,\n",
    "    out_dir=f'{base_dir}/Joint/t-SNE p3',\n",
    "    reduction='TSNE',\n",
    "    perplexity=3,\n",
    "    word_ids=J_ids,\n",
    "    words=J_words,\n",
    "    hues=J_deno,\n",
    "    sizes=J_freq,\n",
    "    categorical=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Recomposers\n",
    "Want to show that...\n",
    "\n",
    "For deno vectors, topic cluster better than pretrained\n",
    "\n",
    "For cono vectors, skew cluster better than pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_recomposer(path):\n",
    "    stuff = torch.load(path, map_location=DEVICE)['model']\n",
    "    D_embed = stuff.deno_decomposer.embedding.weight.detach().numpy()\n",
    "    C_embed = stuff.cono_decomposer.embedding.weight.detach().numpy()\n",
    "    return D_embed, C_embed\n",
    "\n",
    "def load_recomposers_en_masse(in_dir, endswith):\n",
    "    D_models = {\n",
    "        'pretrained superset': load('../../results/pretrained/init.pt'),\n",
    "        'pretrained': load('../../results/pretrained bill mentions/init.pt')}\n",
    "    C_models = {\n",
    "        'pretrained superset': load('../../results/pretrained/init.pt'),\n",
    "        'pretrained': load('../../results/pretrained bill mentions/init.pt')}\n",
    "    for dirpath, _, filenames in os.walk(in_dir):\n",
    "        for file in filenames:\n",
    "            if file.endswith(endswith):\n",
    "                path = os.path.join(dirpath, file)\n",
    "                name = path.lstrip(in_dir).replace('/', ' ')\n",
    "                D_embed, C_embed = load_recomposer(path)\n",
    "                # Brittle Hack\n",
    "                name = name.split()\n",
    "                D_name = ' '.join(name[0:2] + name[4:])\n",
    "                R_name = ' '.join(name[2:])\n",
    "                D_models[D_name] = D_embed\n",
    "                C_models[R_name] = C_embed\n",
    "                print(name)\n",
    "    return D_models, C_models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/webson/Research/congressional_adversary/congressional_env/lib/python3.7/site-packages/torch/serialization.py:593: SourceChangeWarning: source code of class 'decomposer.Decomposer' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dd0.9', 'Dg-3.5', 'Cd-2.4', 'Cg2.2', 'R1.5', 'epoch30.pt']\n",
      "['Dd3.3', 'Dg-3.4', 'Cd-2.4', 'Cg2.7', 'R0.9', 'epoch30.pt']\n",
      "['Dd0.8', 'Dg-4.7', 'Cd-0.7', 'Cg3.0', 'R3.5', 'epoch30.pt']\n",
      "['Dd0.6', 'Dg-2.5', 'Cd-4.8', 'Cg4.5', 'R1.3', 'epoch30.pt']\n",
      "['Dd4.8', 'Dg-1.1', 'Cd-0.3', 'Cg4.5', 'R3.0', 'epoch30.pt']\n",
      "['Dd3.0', 'Dg-4.1', 'Cd-4.7', 'Cg4.7', 'R4.8', 'epoch30.pt']\n",
      "['Dd4.6', 'Dg-4.6', 'Cd-4.0', 'Cg0.2', 'R1.6', 'epoch30.pt']\n",
      "['Dd1.9', 'Dg-0.2', 'Cd-1.3', 'Cg3.0', 'R0.8', 'epoch30.pt']\n",
      "['Dd3.1', 'Dg-4.3', 'Cd-3.5', 'Cg1.8', 'R2.3', 'epoch30.pt']\n",
      "['Dd4.0', 'Dg-3.5', 'Cd-4.5', 'Cg3.4', 'R2.2', 'epoch30.pt']\n",
      "['Dd3.9', 'Dg-4.0', 'Cd-2.4', 'Cg3.0', 'R0.2', 'epoch30.pt']\n",
      "['Dd0.1', 'Dg-0.2', 'Cd-0.8', 'Cg1.1', 'R0.9', 'epoch30.pt']\n"
     ]
    }
   ],
   "source": [
    "base_dir = '../../results/recomposer/superset pretrained'\n",
    "D_sub, C_sub = load_recomposers_en_masse(base_dir, endswith='epoch30.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dd0.9', 'Dg-3.5', 'Cd-2.4', 'Cg2.2', 'R1.5', 'epoch30.pt']\n",
      "['Dd3.3', 'Dg-3.4', 'Cd-2.4', 'Cg2.7', 'R0.9', 'epoch30.pt']\n",
      "['Dd0.8', 'Dg-4.7', 'Cd-0.7', 'Cg3.0', 'R3.5', 'epoch30.pt']\n",
      "['Dd0.6', 'Dg-2.5', 'Cd-4.8', 'Cg4.5', 'R1.3', 'epoch30.pt']\n",
      "['Dd4.8', 'Dg-1.1', 'Cd-0.3', 'Cg4.5', 'R3.0', 'epoch30.pt']\n",
      "['Dd3.0', 'Dg-4.1', 'Cd-4.7', 'Cg4.7', 'R4.8', 'epoch30.pt']\n",
      "['Dd4.6', 'Dg-4.6', 'Cd-4.0', 'Cg0.2', 'R1.6', 'epoch30.pt']\n",
      "['Dd1.9', 'Dg-0.2', 'Cd-1.3', 'Cg3.0', 'R0.8', 'epoch30.pt']\n",
      "['Dd3.1', 'Dg-4.3', 'Cd-3.5', 'Cg1.8', 'R2.3', 'epoch30.pt']\n",
      "['Dd4.0', 'Dg-3.5', 'Cd-4.5', 'Cg3.4', 'R2.2', 'epoch30.pt']\n",
      "['Dd3.9', 'Dg-4.0', 'Cd-2.4', 'Cg3.0', 'R0.2', 'epoch30.pt']\n",
      "['Dd0.1', 'Dg-0.2', 'Cd-0.8', 'Cg1.1', 'R0.9', 'epoch30.pt']\n"
     ]
    }
   ],
   "source": [
    "base_dir = '../../results/recomposer/superset pretrained'\n",
    "D_super, C_super = load_recomposers_en_masse(base_dir, endswith='epoch30.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Evaluating Denotation\n",
    "models = D_models\n",
    "\n",
    "graph_en_masse(\n",
    "    models,\n",
    "    out_dir=f'{base_dir}/Joint/topic/PCA',\n",
    "    reduction='PCA',\n",
    "    word_ids=J_ids,\n",
    "    words=J_words,\n",
    "    hues=J_deno,\n",
    "    sizes=J_freq,\n",
    "    categorical=True\n",
    ")\n",
    "\n",
    "# graph_en_masse(\n",
    "#     models, out_dir=f'{base_dir}/Joint/topic/t-SNE p5',\n",
    "#     reduction='TSNE', perplexity=5,\n",
    "#     word_ids=J_ids, words=J_words, hues=J_deno, sizes=J_freq,\n",
    "#     categorical=True\n",
    "# )\n",
    "\n",
    "# graph_en_masse(\n",
    "#     models,\n",
    "#     out_dir=f'{base_dir}/Joint/topic/t-SNE p3',\n",
    "#     reduction='TSNE', perplexity=3,\n",
    "#     word_ids=J_ids, words=J_words, hues=J_deno, sizes=J_freq,\n",
    "#     categorical=True\n",
    "# )\n",
    "\n",
    "# graph_en_masse(\n",
    "#     models,\n",
    "#     out_dir=f'{base_dir}/Joint/topic/t-SNE p10',\n",
    "#     reduction='TSNE', perplexity=10,\n",
    "#     word_ids=J_ids, words=J_words, hues=J_deno, sizes=J_freq,\n",
    "#     categorical=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating Connotation\n",
    "models = C_models\n",
    "\n",
    "graph_en_masse(\n",
    "    models,\n",
    "    out_dir=f'{base_dir}/Joint/topic/PCA',\n",
    "    reduction='PCA',\n",
    "    word_ids=J_ids,\n",
    "    words=J_words,\n",
    "    hues=J_skew,\n",
    "    sizes=J_freq,\n",
    ")\n",
    "\n",
    "\n",
    "# graph_en_masse(\n",
    "#     models,\n",
    "#     out_dir=f'{base_dir}/Joint/party/t-SNE p25',\n",
    "#     reduction='TSNE', perplexity=25,\n",
    "#     word_ids=J_ids, words=J_words, hues=J_skew, sizes=J_freq,\n",
    "# )\n",
    "\n",
    "# graph_en_masse(\n",
    "#     models,\n",
    "#     out_dir=f'{base_dir}/Joint/party/t-SNE p50',\n",
    "#     reduction='TSNE', perplexity=50,\n",
    "#     word_ids=J_ids, words=J_words, hues=J_skew, sizes=J_freq,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering + Homogeneity V-Measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import homogeneity_completeness_v_measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed = models['pretrained superset'][J_ids]\n",
    "# Cono_Space = KMeans(n_clusters=2).fit(embed)\n",
    "# pred_labels = Cono_Space.predict(embed)\n",
    "# homogeneity, completeness, v_measure = np.around(homogeneity_completeness_v_measure(\n",
    "#     J_cono, pred_labels), 4)\n",
    "# print(homogeneity, completeness, v_measure, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import editdistance\n",
    "\n",
    "PE.deno_to_id = {val: key for key, val in PE.id_to_deno.items()}\n",
    "\n",
    "def discretize_cono(skew):\n",
    "    if skew < 0.5:\n",
    "        return 0 \n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def NN_cluster_ids(embed, query_ids, categorical, top_k=5):\n",
    "    query_ids = torch.tensor(query_ids, device=DEVICE)\n",
    "    embed = torch.tensor(embed, device=DEVICE)\n",
    "    embed = nn.Embedding.from_pretrained(embed, freeze=True)\n",
    "\n",
    "    query_embed = embed(query_ids)\n",
    "    top_neighbor_ids = [\n",
    "        nn.functional.cosine_similarity(\n",
    "            q.view(1, -1), embed.weight).argsort(descending=True)\n",
    "        for q in query_embed]\n",
    "\n",
    "    cluster_labels = []\n",
    "    true_labels = []\n",
    "    for query_index, sorted_target_indices in enumerate(top_neighbor_ids):\n",
    "        query_id = query_ids[query_index].item()\n",
    "        query_word = PE.id_to_word[query_id]\n",
    "        num_neighbors = 0\n",
    "#         if categorical:\n",
    "#             query_label = PE.deno_to_id[GD[query_word]['majority_deno']]\n",
    "#         else:\n",
    "#             query_label = discretize_cono(GD[query_word]['R_ratio'])\n",
    "        query_label = query_index\n",
    "            \n",
    "        for sort_rank, target_id in enumerate(sorted_target_indices):\n",
    "            target_id = target_id.item()\n",
    "            if num_neighbors == top_k:\n",
    "                break\n",
    "            if query_id == target_id:\n",
    "                continue\n",
    "            # target_id = target_ids[target_index]  # target is always all embed\n",
    "            target_word = PE.id_to_word[target_id]\n",
    "            if editdistance.eval(query_word, target_word) < 3:\n",
    "                continue\n",
    "            num_neighbors += 1\n",
    "\n",
    "            if categorical:\n",
    "                neighbor_label = PE.deno_to_id[GD[target_word]['majority_deno']]\n",
    "            else:\n",
    "                neighbor_label = discretize_cono(GD[target_word]['R_ratio'])            \n",
    "            cluster_labels.append(query_label)\n",
    "            true_labels.append(neighbor_label)\n",
    "    return cluster_labels, true_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretrained superset\t0.7042\t0.4406\t0.5421\n",
      "pretrained\t0.3441\t0.2111\t0.2617\n",
      "Dd0.9 Dg-3.5 R1.5 epoch30.pt\t0.3407\t0.2114\t0.2609\n",
      "Dd3.3 Dg-3.4 R0.9 epoch30.pt\t0.3319\t0.203\t0.2519\n",
      "Dd0.8 Dg-4.7 R3.5 epoch30.pt\t0.3408\t0.208\t0.2583\n",
      "Dd0.6 Dg-2.5 R1.3 epoch30.pt\t0.3384\t0.2087\t0.2581\n",
      "Dd4.8 Dg-1.1 R3.0 epoch30.pt\t0.3418\t0.2141\t0.2633\n",
      "Dd3.0 Dg-4.1 R4.8 epoch30.pt\t0.3382\t0.2108\t0.2598\n",
      "Dd4.6 Dg-4.6 R1.6 epoch30.pt\t0.3408\t0.2094\t0.2594\n",
      "Dd1.9 Dg-0.2 R0.8 epoch30.pt\t0.3395\t0.2042\t0.255\n",
      "Dd3.1 Dg-4.3 R2.3 epoch30.pt\t0.3396\t0.2078\t0.2578\n",
      "Dd4.0 Dg-3.5 R2.2 epoch30.pt\t0.3362\t0.2063\t0.2557\n",
      "Dd3.9 Dg-4.0 R0.2 epoch30.pt\t0.3353\t0.2048\t0.2543\n",
      "Dd0.1 Dg-0.2 R0.9 epoch30.pt\t0.3465\t0.216\t0.2661\n"
     ]
    }
   ],
   "source": [
    "# Deno space, eval deno, higher is better\n",
    "for model_name, model in D_super.items():\n",
    "    cluster_labels, true_labels = NN_cluster_ids(\n",
    "        model, J_ids, categorical=True, top_k=10)    \n",
    "    homogeneity, completeness, v_measure = np.around(\n",
    "        homogeneity_completeness_v_measure(true_labels, cluster_labels), 4)\n",
    "    print(model_name, homogeneity, completeness, v_measure, sep='\\t')\n",
    "#     print(pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretrained superset\t0.576\t0.565\t0.5704\n",
      "pretrained\t0.1763\t0.1721\t0.1742\n",
      "Dd0.9 Dg-3.5 R1.5 epoch30.pt\t0.1851\t0.1764\t0.1807\n",
      "Dd3.3 Dg-3.4 R0.9 epoch30.pt\t0.1713\t0.1635\t0.1673\n",
      "Dd0.8 Dg-4.7 R3.5 epoch30.pt\t0.1759\t0.1692\t0.1725\n",
      "Dd0.6 Dg-2.5 R1.3 epoch30.pt\t0.182\t0.1758\t0.1789\n",
      "Dd4.8 Dg-1.1 R3.0 epoch30.pt\t0.1758\t0.1726\t0.1742\n",
      "Dd3.0 Dg-4.1 R4.8 epoch30.pt\t0.1884\t0.1848\t0.1866\n",
      "Dd4.6 Dg-4.6 R1.6 epoch30.pt\t0.1774\t0.1722\t0.1747\n",
      "Dd1.9 Dg-0.2 R0.8 epoch30.pt\t0.1739\t0.1641\t0.1688\n",
      "Dd3.1 Dg-4.3 R2.3 epoch30.pt\t0.1831\t0.174\t0.1785\n",
      "Dd4.0 Dg-3.5 R2.2 epoch30.pt\t0.1832\t0.1761\t0.1796\n",
      "Dd3.9 Dg-4.0 R0.2 epoch30.pt\t0.1663\t0.1604\t0.1633\n",
      "Dd0.1 Dg-0.2 R0.9 epoch30.pt\t0.186\t0.1818\t0.1839\n"
     ]
    }
   ],
   "source": [
    "# Deno space, eval cono, lower is better\n",
    "for model_name, model in D_super.items():\n",
    "    cluster_labels, true_labels = NN_cluster_ids(\n",
    "        model, J_ids, categorical=True, top_k=5)    \n",
    "    homogeneity, completeness, v_measure = np.around(\n",
    "        homogeneity_completeness_v_measure(true_labels, cluster_labels), 4)\n",
    "    print(model_name, homogeneity, completeness, v_measure, sep='\\t')\n",
    "#     print(pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretrained superset\t0.3306\t0.3225\t0.3265\n",
      "pretrained\t0.0025\t0.0023\t0.0024\n",
      "Cd-2.4 Cg2.2 R1.5 epoch30.pt\t0.0\t0.0\t0.0\n",
      "Cd-2.4 Cg2.7 R0.9 epoch30.pt\t0.001\t0.0009\t0.0009\n",
      "Cd-0.7 Cg3.0 R3.5 epoch30.pt\t0.0003\t0.0003\t0.0003\n",
      "Cd-4.8 Cg4.5 R1.3 epoch30.pt\t0.0002\t0.0002\t0.0002\n",
      "Cd-0.3 Cg4.5 R3.0 epoch30.pt\t0.001\t0.0009\t0.001\n",
      "Cd-4.7 Cg4.7 R4.8 epoch30.pt\t0.0001\t0.0001\t0.0001\n",
      "Cd-4.0 Cg0.2 R1.6 epoch30.pt\t0.0002\t0.0002\t0.0002\n",
      "Cd-1.3 Cg3.0 R0.8 epoch30.pt\t0.0013\t0.0012\t0.0012\n",
      "Cd-3.5 Cg1.8 R2.3 epoch30.pt\t0.0017\t0.0016\t0.0017\n",
      "Cd-4.5 Cg3.4 R2.2 epoch30.pt\t0.0\t0.0\t0.0\n",
      "Cd-2.4 Cg3.0 R0.2 epoch30.pt\t0.0002\t0.0002\t0.0002\n",
      "Cd-0.8 Cg1.1 R0.9 epoch30.pt\t0.0022\t0.002\t0.0021\n"
     ]
    }
   ],
   "source": [
    "# Cono space, eval cono, higher is better\n",
    "for model_name, model in C_super.items():\n",
    "    cluster_labels, true_labels = NN_cluster_ids(\n",
    "        model, J_ids, categorical=False, top_k=5)    \n",
    "    homogeneity, completeness, v_measure = np.around(\n",
    "        homogeneity_completeness_v_measure(true_labels, cluster_labels), 4)\n",
    "    print(model_name, homogeneity, completeness, v_measure, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretrained superset\t0.576\t0.565\t0.5704\n",
      "pretrained\t0.1763\t0.1721\t0.1742\n",
      "Cd-2.4 Cg2.2 R1.5 epoch30.pt\t0.1718\t0.1692\t0.1705\n",
      "Cd-2.4 Cg2.7 R0.9 epoch30.pt\t0.1785\t0.1735\t0.1759\n",
      "Cd-0.7 Cg3.0 R3.5 epoch30.pt\t0.1899\t0.1796\t0.1846\n",
      "Cd-4.8 Cg4.5 R1.3 epoch30.pt\t0.1709\t0.1676\t0.1692\n",
      "Cd-0.3 Cg4.5 R3.0 epoch30.pt\t0.1726\t0.1701\t0.1713\n",
      "Cd-4.7 Cg4.7 R4.8 epoch30.pt\t0.1766\t0.1702\t0.1733\n",
      "Cd-4.0 Cg0.2 R1.6 epoch30.pt\t0.1729\t0.1664\t0.1696\n",
      "Cd-1.3 Cg3.0 R0.8 epoch30.pt\t0.1692\t0.1693\t0.1692\n",
      "Cd-3.5 Cg1.8 R2.3 epoch30.pt\t0.1754\t0.173\t0.1742\n",
      "Cd-4.5 Cg3.4 R2.2 epoch30.pt\t0.182\t0.1795\t0.1807\n",
      "Cd-2.4 Cg3.0 R0.2 epoch30.pt\t0.1827\t0.1799\t0.1813\n",
      "Cd-0.8 Cg1.1 R0.9 epoch30.pt\t0.1751\t0.1713\t0.1732\n"
     ]
    }
   ],
   "source": [
    "# Cono space, eval deno, lower is better\n",
    "for model_name, model in C_super.items():\n",
    "    cluster_labels, true_labels = NN_cluster_ids(\n",
    "        model, J_ids, categorical=True, top_k=5)    \n",
    "    homogeneity, completeness, v_measure = np.around(\n",
    "        homogeneity_completeness_v_measure(true_labels, cluster_labels), 4)\n",
    "    print(model_name, homogeneity, completeness, v_measure, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# homogeneity, completeness, v_measure = np.around(\n",
    "#     homogeneity_completeness_v_measure(true_labels, cluster_labels), 4)\n",
    "# print(homogeneity, completeness, v_measure, sep='\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
